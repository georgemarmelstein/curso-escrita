â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LINKS RÃPIDOS - AULA 1: LLMs COMO MÃQUINAS DE TRANSFORMAÃ‡ÃƒO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“„ PAPERS FUNDAMENTAIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[1] Attention is All You Need (Vaswani et al., 2017)
https://arxiv.org/abs/1706.03762
https://arxiv.org/pdf/1706.03762.pdf

[2] GPT-1: Improving Language Understanding by Generative Pre-Training (2018)
https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf

[3] GPT-2: Language Models are Unsupervised Multitask Learners (2019)
https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
GitHub: https://github.com/openai/gpt-2

[4] GPT-3: Language Models are Few-Shot Learners (2020)
https://arxiv.org/abs/2005.14165
https://arxiv.org/pdf/2005.14165.pdf

[5] BERT: Pre-training of Deep Bidirectional Transformers (2018)
https://arxiv.org/abs/1810.04805
https://arxiv.org/pdf/1810.04805.pdf

[6] GPT or BERT: why not both? (2024)
https://arxiv.org/abs/2410.24159
https://arxiv.org/pdf/2410.24159.pdf


ğŸ“š SURVEYS E REVISÃ•ES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[7] A Comprehensive Overview of Large Language Models (2023)
https://arxiv.org/abs/2307.06435
https://arxiv.org/pdf/2307.06435.pdf

[8] Controllable Text Generation for LLMs: A Survey (2024)
https://arxiv.org/abs/2408.12599
https://arxiv.org/pdf/2408.12599.pdf

[9] Factuality of Large Language Models: A Survey (2024)
https://arxiv.org/abs/2402.02420
https://arxiv.org/pdf/2402.02420.pdf

[10] Transformers and LLMs for Intrusion Detection: Survey (2025)
https://arxiv.org/abs/2408.07583


ğŸ“– TUTORIAIS INTERATIVOS E IMPLEMENTAÃ‡Ã•ES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[11] The Annotated Transformer - Harvard NLP
https://nlp.seas.harvard.edu/annotated-transformer/
https://aclanthology.org/W18-2509.pdf
GitHub: https://github.com/harvardnlp/annotated-transformer

[12] The Illustrated Transformer - Jay Alammar
https://jalammar.github.io/illustrated-transformer/

[13] The Illustrated GPT-2 - Jay Alammar
https://jalammar.github.io/illustrated-gpt2/

[14] How GPT3 Works - Visualizations and Animations - Jay Alammar
https://jalammar.github.io/how-gpt3-works-visualizations-animations/

[15] Interfaces for Explaining Transformer Language Models - Jay Alammar
https://jalammar.github.io/explaining-transformers/

[16] Jay Alammar Blog (todos os posts)
https://jalammar.github.io/

[17] Creating a Transformer From Scratch
https://benjaminwarner.dev/2023/07/01/attention-mechanism

[18] UvA Deep Learning: Transformers and Multi-Head Attention
https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html


ğŸ¥ VÃDEOS - ANDREJ KARPATHY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[19] Let's build GPT: from scratch, in code, spelled out (1h56m)
https://www.youtube.com/watch?v=kCc8FmEb1nY

[20] Deep Dive into LLMs like ChatGPT (3h31m)
DisponÃ­vel em: https://karpathy.ai/zero-to-hero.html

[21] Neural Networks: Zero to Hero (sÃ©rie completa)
Website: https://karpathy.ai/zero-to-hero.html
GitHub: https://github.com/karpathy/nn-zero-to-hero

[22] The spelled-out intro to language modeling: building makemore
DisponÃ­vel no canal do YouTube

[23] nanoGPT - ImplementaÃ§Ã£o minimal de GPT
GitHub: https://github.com/karpathy/nanoGPT


ğŸ¥ VÃDEOS - 3BLUE1BROWN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[24] But what is a GPT? Visual intro to Transformers (Chapter 5)
https://www.youtube.com/watch?v=wjZofJX0v4M
Website: https://www.3blue1brown.com/lessons/gpt

[25] Visualizing Attention, a Transformer's Heart (Chapter 6)
DisponÃ­vel no canal do YouTube
Website: https://www.3blue1brown.com/lessons/gpt

[26] Large Language Models explained briefly
Website: https://www.3blue1brown.com/lessons/mini-llm


ğŸ¥ VÃDEOS - STATQUEST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[27] Transformer Neural Networks, ChatGPT's Foundation, Clearly Explained (36min)
Buscar no YouTube: StatQuest Transformer Neural Networks


ğŸ”— RECURSOS ADICIONAIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[28] Papers With Code - Transformers
https://paperswithcode.com/

[29] Hugging Face - Transformers Library
https://huggingface.co/docs/transformers/

[30] OpenAI Research
https://openai.com/research/

[31] Anthropic Research
https://www.anthropic.com/research

[32] Fermat's Library - GPT-2 Annotated
https://fermatslibrary.com/s/language-models-are-unsupervised-multitask-learners

[33] Semantic Scholar - GPT-2 Paper
https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe


ğŸ“¥ PAPERS ADICIONAIS (ARXIV)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[34] GPT-NeoX-20B: An Open-Source Autoregressive Language Model
https://arxiv.org/abs/2204.06745

[35] Ïƒ-GPTs: A New Approach to Autoregressive Models
https://arxiv.org/abs/2404.09562

[36] Shall We Pretrain Autoregressive LMs with Retrieval?
https://arxiv.org/abs/2304.06762

[37] RecycleGPT: An Autoregressive Language Model with Recyclable Module
https://arxiv.org/abs/2308.03421

[38] Teaching Autoregressive Language Models Complex Tasks By Demonstration
https://arxiv.org/abs/2109.02102

[39] Comparison of BERT vs GPT
https://arxiv.org/pdf/2405.12990

[40] A Comparative Study of Transformer-Based Language Models
https://arxiv.org/pdf/2110.03142


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LISTA DE VERIFICAÃ‡ÃƒO PARA PREPARAÃ‡ÃƒO DA AULA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¡ Assistir "But what is a GPT?" (3Blue1Brown)
â–¡ Assistir "Let's build GPT" (Andrej Karpathy) - pelo menos primeira hora
â–¡ Ler "The Illustrated Transformer" (Jay Alammar)
â–¡ Ler paper "Attention is All You Need" (introduÃ§Ã£o e seÃ§Ã£o 3)
â–¡ Ler paper GPT-2 (introduÃ§Ã£o, resultados, conclusÃ£o)
â–¡ Explorar "The Annotated Transformer" (Harvard NLP)
â–¡ Baixar PDFs dos papers principais
â–¡ Fazer screenshots das visualizaÃ§Ãµes de Jay Alammar
â–¡ Testar demonstraÃ§Ãµes prÃ¡ticas
â–¡ Preparar slides com citaÃ§Ãµes adequadas


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ORDEM DE PRIORIDADE PARA ESTUDO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PRIORIDADE MÃXIMA (Essencial):
1. VÃ­deo: "But what is a GPT?" - 3Blue1Brown
2. Tutorial: "The Illustrated Transformer" - Jay Alammar
3. Paper: "Attention is All You Need" (pelo menos introduÃ§Ã£o)

ALTA PRIORIDADE (Muito Recomendado):
4. VÃ­deo: "Let's build GPT" - Andrej Karpathy (primeira hora)
5. Tutorial: "The Illustrated GPT-2" - Jay Alammar
6. Paper: "Language Models are Unsupervised Multitask Learners" (GPT-2)

MÃ‰DIA PRIORIDADE (Recomendado):
7. VÃ­deo: "Visualizing Attention" - 3Blue1Brown
8. Paper: "Language Models are Few-Shot Learners" (GPT-3)
9. Tutorial: "The Annotated Transformer" - Harvard NLP

COMPLEMENTAR (Se houver tempo):
10. Survey: "Factuality of Large Language Models"
11. VÃ­deo: "Transformer Neural Networks" - StatQuest
12. Tutorial: "How GPT3 Works" - Jay Alammar


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DICAS PARA DOWNLOAD E ACESSO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¥ Para baixar papers do ArXiv:
   - Clique em "PDF" no topo da pÃ¡gina
   - Ou adicione .pdf ao final do URL (ex: /abs/1706.03762 â†’ /pdf/1706.03762.pdf)

ğŸ’¾ Para salvar vÃ­deos do YouTube para referÃªncia:
   - Use YouTube Premium para download offline
   - Ou anote timestamps importantes

ğŸ“‘ Para salvar pÃ¡ginas web interativas:
   - Use "Salvar como PDF" no navegador
   - Ou faÃ§a screenshots das visualizaÃ§Ãµes importantes

ğŸ”– Para organizar referÃªncias:
   - Use um gerenciador de referÃªncias (Zotero, Mendeley)
   - Ou crie uma pasta local com PDFs baixados


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NOTAS IMPORTANTES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸ Alguns papers nÃ£o estÃ£o no ArXiv:
   - GPT-1 e GPT-2 foram publicados diretamente pela OpenAI
   - EstÃ£o disponÃ­veis no site cdn.openai.com

âš ï¸ Alguns vÃ­deos requerem busca direta no YouTube:
   - Nem todos os vÃ­deos tÃªm URLs diretos disponÃ­veis
   - Use os tÃ­tulos exatos fornecidos para buscar

âœ“ Todos os recursos listados sÃ£o:
   - Gratuitos e de acesso aberto
   - De alta qualidade acadÃªmica
   - Reconhecidos pela comunidade
   - Relevantes para o tema da aula


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CONTATO E CRÃ‰DITOS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Compilado para: George Marmelstein
Curso: LLMs e suas AplicaÃ§Ãµes - Aulas 2025
Aula: 1 - MÃ¡quinas de TransformaÃ§Ã£o
Data: 2025-01-31

Fontes consultadas:
- ArXiv (Cornell University)
- Google Scholar
- YouTube (canais verificados de alta qualidade)
- Blogs tÃ©cnicos reconhecidos
- Universidades (Harvard, Stanford, MIT, etc.)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FIM DO DOCUMENTO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
