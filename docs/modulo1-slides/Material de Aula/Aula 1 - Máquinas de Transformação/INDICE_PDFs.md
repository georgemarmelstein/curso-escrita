# üìÑ √çNDICE DE PDFs BAIXADOS - AULA 1

**Total de PDFs:** 10 papers cient√≠ficos

**Data de download:** 31 de janeiro de 2025

---

## üìö PAPERS FUNDAMENTAIS

### 1. Attention_is_All_You_Need.pdf
**Autores:** Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, ≈Åukasz; Polosukhin, Illia

**Publica√ß√£o:** NeurIPS 2017

**Cita√ß√µes:** 195,580+

**Descri√ß√£o:**
O paper seminal que introduziu a arquitetura Transformer, propondo uma nova arquitetura de rede neural baseada puramente em mecanismos de aten√ß√£o, dispensando completamente recorr√™ncia e convolu√ß√µes.

**Conceitos-chave:**
- Self-attention mechanism
- Multi-head attention
- Positional encoding
- Encoder-decoder architecture

**Por que √© essencial:**
Este √© o paper fundacional que iniciou a revolu√ß√£o dos Transformers. Todo LLM moderno (GPT, BERT, etc.) √© baseado nesta arquitetura.

**Link original:** https://arxiv.org/abs/1706.03762

---

## ü§ñ S√âRIE GPT (OpenAI)

### 2. GPT-1_Improving_Language_Understanding.pdf
**T√≠tulo completo:** Improving Language Understanding by Generative Pre-Training

**Autores:** Radford, Alec; Narasimhan, Karthik; Salimans, Tim; Sutskever, Ilya

**Publica√ß√£o:** 2018

**Descri√ß√£o:**
Primeiro paper da s√©rie GPT. Demonstra que grandes ganhos em tarefas de NLP podem ser alcan√ßados atrav√©s de pr√©-treinamento generativo em corpus n√£o-supervisionado, seguido de fine-tuning discriminativo em tarefas espec√≠ficas.

**Resultados principais:**
- Superou estado da arte em 9 de 12 tarefas
- Introduziu o conceito de "aprender representa√ß√µes universais"
- Modelo com 117M par√¢metros

**Conceitos-chave:**
- Generative pre-training
- Transfer learning em NLP
- Task-agnostic model
- Unsupervised learning

**Link original:** https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf

---

### 3. GPT-2_Language_Models_Unsupervised_Multitask_Learners.pdf
**T√≠tulo completo:** Language Models are Unsupervised Multitask Learners

**Autores:** Radford, Alec; Wu, Jeffrey; Child, Rewon; Luan, David; Amodei, Dario; Sutskever, Ilya

**Publica√ß√£o:** 2019

**Descri√ß√£o:**
GPT-2 demonstra que modelos de linguagem come√ßam a aprender m√∫ltiplas tarefas sem supervis√£o expl√≠cita quando treinados em corpus massivo (WebText).

**Resultados principais:**
- 1.5 bilh√µes de par√¢metros
- Estado da arte em 7 de 8 datasets (zero-shot)
- Capacidades emergentes impressionantes

**Conceitos-chave:**
- Zero-shot learning
- Multitask learning sem supervis√£o
- WebText dataset
- Capacidades emergentes

**Relev√¢ncia para Aula 1:**
Demonstra claramente como LLMs "transformam" ao inv√©s de reproduzir - o modelo gera texto coerente sem ter visto exemplos espec√≠ficos das tarefas.

**Link original:** https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

---

### 4. GPT-3_Language_Models_Few_Shot_Learners.pdf
**T√≠tulo completo:** Language Models are Few-Shot Learners

**Autores:** Brown, Tom B.; et al. (48 autores)

**Publica√ß√£o:** NeurIPS 2020

**Descri√ß√£o:**
GPT-3 com 175 bilh√µes de par√¢metros demonstra capacidades surpreendentes de aprendizado few-shot e in-context learning, mostrando que escala massiva resulta em comportamentos emergentes significativos.

**Resultados principais:**
- 175 bilh√µes de par√¢metros
- Few-shot learning sem gradiente
- In-context learning poderoso
- Capacidades emergentes not√°veis

**Conceitos-chave:**
- Few-shot learning
- In-context learning
- Emergent abilities
- Scale effects (efeitos de escala)

**Relev√¢ncia para Aula 1:**
Mostra que com escala suficiente, LLMs desenvolvem capacidades de "transforma√ß√£o" muito mais sofisticadas - podem aprender novas tarefas apenas com exemplos no contexto.

**Link original:** https://arxiv.org/abs/2005.14165

---

## üîÑ ARQUITETURAS ALTERNATIVAS

### 5. BERT_Pretraining_Bidirectional_Transformers.pdf
**T√≠tulo completo:** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

**Autores:** Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina

**Publica√ß√£o:** NAACL 2019

**Descri√ß√£o:**
BERT introduz pr√©-treinamento bidirecional usando Masked Language Modeling (MLM), contrastando com a abordagem unidirecional autoregressiva do GPT.

**Conceitos-chave:**
- Masked language modeling (MLM)
- Bidirectional context
- Auto-encoding vs. auto-regressive
- Pre-training + fine-tuning

**Relev√¢ncia para Aula 1:**
√ötil para contrastar diferentes paradigmas de "transforma√ß√£o" - enquanto GPT prev√™ pr√≥ximo token (unidirecional), BERT prev√™ tokens mascarados (bidirecional).

**Link original:** https://arxiv.org/abs/1810.04805

---

### 6. GPT_or_BERT_Why_Not_Both.pdf
**T√≠tulo completo:** GPT or BERT: why not both?

**Autores:** (2024)

**Descri√ß√£o:**
Paper recente que prop√µe GPT-BERT, um modelo h√≠brido que combina masked language modeling (BERT) com causal language modeling (GPT) em um √∫nico transformer stack.

**Resultados principais:**
- Treinamento h√≠brido supera modelos puramente masked ou causais
- Combina for√ßas de ambos paradigmas
- Demonstra que diferentes objetivos de treinamento criam diferentes capacidades

**Relev√¢ncia para Aula 1:**
Mostra que h√° m√∫ltiplas formas de "transformar" linguagem, e que essas formas podem ser combinadas.

**Link original:** https://arxiv.org/abs/2410.24159

---

## üìä SURVEYS E REVIS√ïES

### 7. Survey_Comprehensive_Overview_LLMs.pdf
**T√≠tulo completo:** A Comprehensive Overview of Large Language Models

**Publica√ß√£o:** 2023

**Descri√ß√£o:**
Survey abrangente sobre LLMs cobrindo arquiteturas, treinamento, capacidades, aplica√ß√µes e desafios.

**Conte√∫do:**
- Vis√£o geral do estado da arte
- Evolu√ß√£o hist√≥rica
- Diferentes abordagens e capacidades
- Aplica√ß√µes pr√°ticas
- Desafios e limita√ß√µes

**Relev√¢ncia para Aula 1:**
Fornece contexto geral e enquadramento hist√≥rico para entender LLMs como m√°quinas de transforma√ß√£o.

**Link original:** https://arxiv.org/abs/2307.06435

---

### 8. Survey_Controllable_Text_Generation_LLMs.pdf
**T√≠tulo completo:** Controllable Text Generation for Large Language Models: A Survey

**Publica√ß√£o:** 2024

**Descri√ß√£o:**
Survey sobre m√©todos de controle de gera√ß√£o de texto em LLMs, cobrindo modelos como GPT, Llama, Transformers, RNNs, LSTMs, GANs e VAEs.

**Conte√∫do:**
- M√©todos de CTG (Controllable Text Generation)
- T√©cnicas para dirigir a gera√ß√£o
- Trade-offs entre controle e criatividade

**Relev√¢ncia para Aula 1:**
Demonstra que embora LLMs "transformem" probabilisticamente, podemos dirigir esse processo de transforma√ß√£o.

**Link original:** https://arxiv.org/abs/2408.12599

---

### 9. Survey_Factuality_of_LLMs.pdf
**T√≠tulo completo:** Factuality of Large Language Models: A Survey

**Publica√ß√£o:** 2024 (atualizado outubro 2024)

**Descri√ß√£o:**
Survey sobre desafios em melhorar a factualidade de LLMs, analisando alucina√ß√µes e avalia√ß√£o automatizada de factualidade em gera√ß√£o de texto aberto.

**Conte√∫do:**
- Causas de alucina√ß√µes
- M√©todos de avalia√ß√£o de factualidade
- T√©cnicas para melhorar factualidade
- Trade-offs entre criatividade e factualidade

**Relev√¢ncia para Aula 1:**
Crucial para entender que "transforma√ß√£o criativa" pode resultar em alucina√ß√µes - a capacidade de criar √© tamb√©m a fonte de erros factuais.

**Link original:** https://arxiv.org/abs/2402.02420

---

## üî¨ PAPERS ADICIONAIS

### 10. GPT-NeoX-20B_Open_Source_Autoregressive_LM.pdf
**T√≠tulo completo:** GPT-NeoX-20B: An Open-Source Autoregressive Language Model

**Publica√ß√£o:** 2022

**Descri√ß√£o:**
Modelo open-source de 20 bilh√µes de par√¢metros treinado no dataset "the Pile". Importante alternativa open-source para estudar modelos autoregressivos.

**Conceitos-chave:**
- Modelo open-source
- Treinamento em "the Pile"
- Compar√°vel a GPT-3 (menor escala)
- Replicabilidade e transpar√™ncia

**Relev√¢ncia para Aula 1:**
Permite experimenta√ß√£o pr√°tica com modelos de grande escala, facilitando compreens√£o de como a "transforma√ß√£o" funciona.

**Link original:** https://arxiv.org/abs/2204.06745

---

## üìñ COMO USAR ESTES PDFs

### Para Prepara√ß√£o R√°pida:
1. **Leia primeiro:** Attention is All You Need (introdu√ß√£o e se√ß√£o 3)
2. **Depois:** GPT-2 (introdu√ß√£o, resultados, conclus√£o)
3. **Folheie:** Um dos surveys para contexto

### Para Prepara√ß√£o Completa:
1. **Dia 1:** Attention is All You Need (completo)
2. **Dia 2:** GPT-1 e GPT-2 (completos)
3. **Dia 3:** GPT-3 (introdu√ß√£o, se√ß√µes principais)
4. **Dia 4:** BERT (para contraste)
5. **Dia 5:** Surveys para vis√£o ampla

### Para Cita√ß√µes em Slides:
Todos os PDFs est√£o dispon√≠veis localmente. Certifique-se de citar:
- Autores
- Ano de publica√ß√£o
- T√≠tulo do paper
- Venue (confer√™ncia/journal)

---

## üéØ PAPERS POR CONCEITO

### Para explicar "Transforma√ß√£o vs. Reprodu√ß√£o":
- ‚úÖ GPT-2: Language Models are Unsupervised Multitask Learners
- ‚úÖ GPT-3: Language Models are Few-Shot Learners

### Para explicar Arquitetura:
- ‚úÖ Attention is All You Need
- ‚úÖ BERT (para contraste)

### Para explicar Capacidades Emergentes:
- ‚úÖ GPT-3: Language Models are Few-Shot Learners
- ‚úÖ Survey: Comprehensive Overview of LLMs

### Para explicar Limita√ß√µes (Alucina√ß√µes):
- ‚úÖ Survey: Factuality of Large Language Models

### Para explicar Controle:
- ‚úÖ Survey: Controllable Text Generation

---

## üìä ESTAT√çSTICAS

**Total de p√°ginas:** ~400+ p√°ginas de conte√∫do cient√≠fico

**Distribui√ß√£o:**
- Papers fundamentais: 4 (Transformer, GPT-1, GPT-2, GPT-3)
- Arquiteturas: 2 (BERT, GPT-BERT)
- Surveys: 3
- Open-source: 1 (GPT-NeoX)

**Per√≠odo coberto:** 2017-2024 (7 anos de evolu√ß√£o)

**Autores not√°veis inclusos:**
- Ashish Vaswani (Transformer)
- Ilya Sutskever (GPT-1, GPT-2)
- Tom Brown (GPT-3)
- Jacob Devlin (BERT)

---

## ‚úÖ CHECKLIST DE USO

Antes da aula, certifique-se de:

- [ ] Ter lido ao menos 3 dos papers fundamentais
- [ ] Ter marcado p√°ginas importantes para refer√™ncia r√°pida
- [ ] Ter preparado screenshots de figuras importantes
- [ ] Ter cita√ß√µes prontas para os slides
- [ ] Ter exemplos pr√°ticos dos papers em mente

---

## üîó LINKS ORIGINAIS (Para refer√™ncia)

Todos os PDFs foram baixados de fontes oficiais:
- **ArXiv:** https://arxiv.org/
- **OpenAI:** https://openai.com/research/

Para atualiza√ß√µes ou vers√µes mais recentes, consulte os links originais listados em cada se√ß√£o.

---

## üìù NOTAS FINAIS

**Sobre Cita√ß√µes:**
Sempre cite a fonte original ao usar conte√∫do destes papers em suas apresenta√ß√µes.

**Sobre Copyright:**
Todos estes papers est√£o dispon√≠veis publicamente para fins educacionais e de pesquisa.

**Sobre Atualiza√ß√µes:**
Alguns papers podem ter vers√µes atualizadas no ArXiv. Verifique a data da vers√£o que voc√™ est√° usando.

---

**Compilado em:** 31 de janeiro de 2025

**Para:** George Marmelstein - Aulas 2025

**Aula:** 1 - LLMs como M√°quinas de Transforma√ß√£o
