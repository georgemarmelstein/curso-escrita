# BIBLIOGRAFIA E MATERIAIS DE REFER√äNCIA - AULA 1
## LLMs como M√°quinas de Transforma√ß√£o

**Tema da Aula:** Como os LLMs transformam e criam texto ao inv√©s de simplesmente reproduzir informa√ß√µes do passado.

**Data de compila√ß√£o:** 2025-01-31

---

## üìö √çNDICE

1. [Papers Fundamentais](#papers-fundamentais)
2. [Papers sobre Arquitetura Transformer](#papers-sobre-arquitetura-transformer)
3. [Papers sobre GPT e Modelos Autoregressivos](#papers-sobre-gpt-e-modelos-autoregressivos)
4. [Papers Recentes e Surveys](#papers-recentes-e-surveys)
5. [Tutoriais T√©cnicos e Implementa√ß√µes](#tutoriais-tecnicos-e-implementacoes)
6. [V√≠deos Educacionais de Alta Qualidade](#videos-educacionais-de-alta-qualidade)
7. [Blogs e Recursos Visuais](#blogs-e-recursos-visuais)
8. [Recursos Adicionais](#recursos-adicionais)

---

## üìÑ PAPERS FUNDAMENTAIS

### 1. Attention is All You Need (2017)
**Autores:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin

**Publica√ß√£o:** NeurIPS 2017

**Cita√ß√µes:** 195,580+ (um dos papers mais citados em IA)

**Link:** https://arxiv.org/abs/1706.03762

**Relev√¢ncia para a Aula 1:**
- Este √© o paper seminal que introduziu a arquitetura Transformer
- Prop√µe uma arquitetura baseada puramente em mecanismos de aten√ß√£o, dispensando recorr√™ncia e convolu√ß√µes
- Estabelece as bases para entender como os LLMs processam e transformam linguagem
- Fundamental para compreender o conceito de "m√°quina de transforma√ß√£o"

**Conceitos-chave:**
- Self-attention mechanism
- Multi-head attention
- Positional encoding
- Encoder-decoder architecture

---

### 2. Improving Language Understanding by Generative Pre-Training (GPT-1, 2018)
**Autores:** Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (OpenAI)

**Publica√ß√£o:** 2018

**Link:** https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf

**Relev√¢ncia para a Aula 1:**
- Primeiro paper da s√©rie GPT, iniciando a revolu√ß√£o dos modelos generativos
- Demonstra que grandes ganhos em tarefas de NLP podem ser alcan√ßados atrav√©s de pr√©-treinamento generativo
- Introduz o conceito de "aprender representa√ß√µes universais" atrav√©s de previs√£o de pr√≥ximo token
- Mostra como o modelo pode ser fine-tuned para tarefas espec√≠ficas ap√≥s pr√©-treinamento

**Conceitos-chave:**
- Generative pre-training
- Task-agnostic model
- Transfer learning em NLP
- Unsupervised learning

**Resultados:** Superou o estado da arte em 9 das 12 tarefas estudadas

---

### 3. Language Models are Unsupervised Multitask Learners (GPT-2, 2019)
**Autores:** Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever (OpenAI)

**Publica√ß√£o:** 2019

**Link:** https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

**GitHub:** https://github.com/openai/gpt-2

**Relev√¢ncia para a Aula 1:**
- Demonstra que modelos de linguagem come√ßam a aprender m√∫ltiplas tarefas sem supervis√£o expl√≠cita
- GPT-2 (1.5B par√¢metros) alcan√ßa estado da arte em 7 de 8 datasets testados em configura√ß√£o zero-shot
- Mostra a capacidade emergente de transforma√ß√£o e cria√ß√£o de texto
- Evidencia o conceito de "m√°quinas de transforma√ß√£o" atrav√©s de capacidades generativas

**Conceitos-chave:**
- Zero-shot learning
- Multitask learning sem supervis√£o
- WebText dataset
- Capacidades emergentes

**Dataset:** Treinado em WebText (milh√µes de p√°ginas web)

---

### 4. Language Models are Few-Shot Learners (GPT-3, 2020)
**Autores:** Tom B. Brown et al. (OpenAI)

**Publica√ß√£o:** NeurIPS 2020

**Link:** https://arxiv.org/abs/2005.14165

**Relev√¢ncia para a Aula 1:**
- GPT-3 com 175 bilh√µes de par√¢metros demonstra capacidades de aprendizado few-shot
- Mostra como escala massiva resulta em comportamentos emergentes
- Evidencia a transforma√ß√£o de inputs em outputs complexos e criativos
- Demonstra que LLMs n√£o apenas "repetem" mas transformam e criam

**Conceitos-chave:**
- Few-shot learning
- In-context learning
- Emergent abilities
- Scale effects

**Caracter√≠sticas:** 175 bilh√µes de par√¢metros, modelo autoregressivo

---

### 5. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)
**Autores:** Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (Google AI)

**Publica√ß√£o:** NAACL 2019

**Link:** https://arxiv.org/abs/1810.04805

**Relev√¢ncia para a Aula 1:**
- Abordagem complementar ao GPT (bidirecional vs. unidirecional)
- Demonstra diferentes maneiras de "transformar" representa√ß√µes de linguagem
- √ötil para contrastar com abordagem autoregressiva do GPT
- Mostra que h√° m√∫ltiplas formas de capturar e transformar padr√µes lingu√≠sticos

**Conceitos-chave:**
- Masked language modeling
- Bidirectional context
- Pre-training + fine-tuning
- Auto-encoding vs. auto-regressive

**Compara√ß√£o:** √ötil para explicar diferentes paradigmas de transforma√ß√£o de linguagem

---

## üèóÔ∏è PAPERS SOBRE ARQUITETURA TRANSFORMER

### 6. GPT or BERT: why not both? (2024)
**Link:** https://arxiv.org/abs/2410.24159

**Relev√¢ncia:**
- Prop√µe GPT-BERT, h√≠brido que combina masked language modeling com causal language modeling
- Mostra que diferentes objetivos de treinamento resultam em diferentes capacidades de transforma√ß√£o
- Demonstra que o h√≠brido supera modelos puramente masked ou puramente causais

**Conceitos-chave:**
- Hybrid training objectives
- Combining paradigms
- Unified transformer stack

---

### 7. The Annotated Transformer (Harvard NLP, 2018)
**Autor:** Alexander M. Rush (Harvard University)

**Publica√ß√£o:** ACL NLP-OSS Workshop 2018

**Links:**
- Website interativo: https://nlp.seas.harvard.edu/annotated-transformer/
- Paper: https://aclanthology.org/W18-2509/
- GitHub: https://github.com/harvardnlp/annotated-transformer

**Relev√¢ncia para a Aula 1:**
- Implementa√ß√£o linha por linha do paper "Attention is All You Need"
- Notebook completamente funcional e utiliz√°vel
- Excelente recurso pedag√≥gico para entender os detalhes t√©cnicos
- Ajuda a visualizar como a transforma√ß√£o de texto acontece na pr√°tica

**Formato:** C√≥digo anotado + explica√ß√µes detalhadas

**P√∫blico-alvo:** Estudantes e pesquisadores que querem implementar transformers

---

## ü§ñ PAPERS SOBRE GPT E MODELOS AUTOREGRESSIVOS

### 8. GPT-NeoX-20B: An Open-Source Autoregressive Language Model (2022)
**Link:** https://arxiv.org/abs/2204.06745

**Relev√¢ncia:**
- Modelo open-source de 20 bilh√µes de par√¢metros
- Treinado no dataset "the Pile"
- Alternativa open-source para estudar modelos autoregressivos
- Permite replica√ß√£o e experimenta√ß√£o

---

### 9. œÉ-GPTs: A New Approach to Autoregressive Models (2024)
**Link:** https://arxiv.org/abs/2404.09562

**Relev√¢ncia:**
- Nova abordagem para modelos autoregressivos
- Inova√ß√µes recentes na arquitetura
- Demonstra evolu√ß√£o cont√≠nua dos m√©todos de transforma√ß√£o de texto

---

### 10. Shall We Pretrain Autoregressive Language Models with Retrieval? (2023)
**Link:** https://arxiv.org/abs/2304.06762

**Relev√¢ncia:**
- Estudo comparativo entre RETRO e GPT padr√£o
- Explora como retrieval pode melhorar capacidades de transforma√ß√£o
- Relevante para entender limita√ß√µes e extens√µes dos LLMs

---

## üìä PAPERS RECENTES E SURVEYS

### 11. A Comprehensive Overview of Large Language Models (2023)
**Link:** https://arxiv.org/abs/2307.06435

**Relev√¢ncia:**
- Survey abrangente sobre LLMs
- Vis√£o geral do estado da arte
- Contextualiza diferentes abordagens e capacidades
- √ötil para enquadramento geral da aula

---

### 12. Controllable Text Generation for Large Language Models: A Survey (2024)
**Link:** https://arxiv.org/abs/2408.12599

**Relev√¢ncia:**
- Survey sobre m√©todos de controle de gera√ß√£o de texto
- Discute modelos como GPT, Llama, Transformers, RNNs, LSTMs, GANs, VAEs
- Mostra como podemos dirigir o processo de transforma√ß√£o
- Relevante para entender que LLMs s√£o ferramentas control√°veis

**Conceitos-chave:**
- Controllable text generation (CTG)
- Steering generation
- Pre-trained language models

---

### 13. Factuality of Large Language Models: A Survey (2024)
**Link:** https://arxiv.org/abs/2402.02420

**Relev√¢ncia:**
- Analisa desafios em melhorar factualidade dos LLMs
- Discute alucina√ß√µes e gera√ß√£o criativa vs. factual
- Importante para contrastar "transforma√ß√£o criativa" com "reprodu√ß√£o factual"
- Conecta com o tema da aula sobre LLMs criarem vs. reproduzirem

**Conceitos-chave:**
- Hallucinations
- Factuality vs. creativity
- Open-ended text generation
- Automated factuality evaluation

---

### 14. Transformers and Large Language Models for Efficient Intrusion Detection Systems: Survey (2025)
**Link:** https://arxiv.org/abs/2408.07583

**Relev√¢ncia:**
- Cobre 118 papers sobre Transformers e LLMs (2017-2024)
- Mostra evolu√ß√£o das capacidades dos LLMs
- Demonstra aplica√ß√µes al√©m de gera√ß√£o de texto

---

## üíª TUTORIAIS T√âCNICOS E IMPLEMENTA√á√ïES

### 15. The Illustrated Transformer
**Autor:** Jay Alammar

**Link:** https://jalammar.github.io/illustrated-transformer/

**Relev√¢ncia para a Aula 1:**
- Um dos primeiros e melhores tutoriais visuais sobre transformers
- Usa aten√ß√£o para explicar o modelo Transformer
- Visualiza√ß√µes claras e did√°ticas
- Recurso pedag√≥gico de refer√™ncia mundial
- Usado em cursos de Stanford, Harvard, MIT, Princeton, CMU

**Formato:** Blog post com visualiza√ß√µes interativas

**Conceitos explicados:**
- Self-attention
- Positional encoding
- Encoder-decoder architecture
- Como a transforma√ß√£o acontece camada por camada

---

### 16. The Illustrated GPT-2
**Autor:** Jay Alammar

**Link:** https://jalammar.github.io/illustrated-gpt2/

**Relev√¢ncia para a Aula 1:**
- Mergulha profundamente na arquitetura que permitiu GPT-2 produzir seus resultados
- Visualiza a camada de self-attention em detalhes
- Complementa o tutorial sobre Transformers com mais visualiza√ß√µes
- Mostra especificamente como GPT gera texto token por token

**Conceitos explicados:**
- Autoregressive generation
- Self-attention em GPT
- Como o modelo "transforma" contexto em previs√µes
- Stack de transformers unidirecionais

---

### 17. How GPT3 Works - Visualizations and Animations
**Autor:** Jay Alammar

**Link:** https://jalammar.github.io/how-gpt3-works-visualizations-animations/

**Relev√¢ncia para a Aula 1:**
- Remove a aura de mist√©rio em torno do GPT-3
- Explica como √© treinado e como funciona
- Anima√ß√µes que mostram o processo de gera√ß√£o
- Excelente para entender o conceito de "m√°quina de transforma√ß√£o"

**Conceitos explicados:**
- Training process
- Text generation mechanics
- Scale effects
- Emergent capabilities

---

### 18. Interfaces for Explaining Transformer Language Models
**Autor:** Jay Alammar

**Link:** https://jalammar.github.io/explaining-transformers/

**Relev√¢ncia:**
- Continua a busca por interpretar e visualizar o funcionamento interno dos transformers
- Ilustra como m√©todos-chave de interpretabilidade se aplicam a LLMs
- Importante para entender o que acontece "dentro" da transforma√ß√£o

---

### 19. Creating a Transformer From Scratch - Part One: The Attention Mechanism
**Autor:** Benjamin Warner

**Link:** https://benjaminwarner.dev/2023/07/01/attention-mechanism

**Relev√¢ncia:**
- Tutorial pr√°tico de como escrever uma camada de Attention do zero em PyTorch
- Cobre todas as tr√™s principais variantes: Bidirectional, Causal, e Cross Attention
- √ötil para demonstra√ß√µes pr√°ticas em aula

---

### 20. Tutorial 6: Transformers and Multi-Head Attention (UvA DL Notebooks)
**Link:** https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html

**Relev√¢ncia:**
- Notebooks da Universidade de Amsterdam
- Muitas visualiza√ß√µes focadas em NLP
- Recurso acad√™mico de alta qualidade

---

## üé• V√çDEOS EDUCACIONAIS DE ALTA QUALIDADE

### 21. Let's build GPT: from scratch, in code, spelled out
**Autor:** Andrej Karpathy

**Plataforma:** YouTube

**Link:** https://www.youtube.com/watch?v=kCc8FmEb1nY

**Dura√ß√£o:** 1h56m

**Data:** 17 de janeiro de 2023

**Relev√¢ncia para a Aula 1:**
- **ALTAMENTE RECOMENDADO** - Considerado "a melhor explica√ß√£o de GPT"
- Constr√≥i e treina um Transformer seguindo o paper "Attention is All You Need"
- Implementa√ß√£o completa em c√≥digo do zero
- Termina com o core do nanoGPT
- Demonstra na pr√°tica como a transforma√ß√£o de texto acontece
- Excelente para quem aprende vendo implementa√ß√µes pr√°ticas

**Conceitos demonstrados:**
- Language modeling
- Transformer architecture
- Autoregressive generation
- Training process

**P√∫blico-alvo:** Estudantes com conhecimento b√°sico de Python e deep learning

**GitHub relacionado:** https://github.com/karpathy/nanoGPT

---

### 22. Deep Dive into LLMs like ChatGPT
**Autor:** Andrej Karpathy

**Plataforma:** YouTube

**Dura√ß√£o:** 3h31m

**Data:** 2025 (recente)

**Relev√¢ncia para a Aula 1:**
- Deep dive para audi√™ncia geral sobre a tecnologia LLM que alimenta ChatGPT
- Cobre o stack completo de treinamento
- Inclui:
  - Pretraining (dados, tokeniza√ß√£o, rede neural Transformer)
  - Supervised finetuning
  - Reinforcement learning

**Conceitos explicados:**
- Full training pipeline
- Mental models para entender LLMs
- Como os modelos s√£o desenvolvidos
- Processo completo de cria√ß√£o de um LLM

**Website do curso:** https://karpathy.ai/zero-to-hero.html

---

### 23. Neural Networks: Zero to Hero (S√©rie Completa)
**Autor:** Andrej Karpathy

**Plataforma:** YouTube + Website

**Links:**
- Website: https://karpathy.ai/zero-to-hero.html
- GitHub: https://github.com/karpathy/nn-zero-to-hero

**Relev√¢ncia:**
- Curso completo sobre constru√ß√£o de redes neurais do zero
- Come√ßa com backpropagation b√°sico e evolui at√© redes modernas como GPT
- Inclui m√∫ltiplas aulas sobre language modeling

**Aulas relevantes:**
- "The spelled-out intro to language modeling: building makemore" (1h57m)
- MLPs, activations, gradients, backpropagation
- Constru√ß√£o gradual at√© transformers

---

### 24. But what is a GPT? Visual intro to Transformers
**Autor:** 3Blue1Brown (Grant Sanderson)

**Plataforma:** YouTube

**Link:** https://www.youtube.com/watch?v=wjZofJX0v4M

**Website:** https://www.3blue1brown.com/lessons/gpt

**S√©rie:** Deep Learning Chapter 5

**Relev√¢ncia para a Aula 1:**
- **ALTAMENTE RECOMENDADO** - Melhor explica√ß√£o visual de GPT
- Explica que GPT = Generative Pre-Trained Transformer
- Visualiza√ß√µes matem√°ticas elegantes e intuitivas
- Quebra o conceito de "m√°quina de transforma√ß√£o" em componentes visuais
- P√∫blico geral, n√£o requer conhecimento t√©cnico avan√ßado

**Conceitos explicados:**
- O que significa "generative" (gera novo texto)
- O que significa "pre-trained" (treinado em grandes volumes de dados)
- Como funciona um transformer (tipo especial de rede neural)

**Estilo:** Anima√ß√µes matem√°ticas visuais, narrativa clara

---

### 25. Visualizing Attention, a Transformer's Heart
**Autor:** 3Blue1Brown (Grant Sanderson)

**Plataforma:** YouTube

**S√©rie:** Deep Learning Chapter 6

**Link:** Dispon√≠vel no canal 3Blue1Brown

**Website:** https://www.3blue1brown.com/lessons/gpt

**Relev√¢ncia para a Aula 1:**
- Desmistifica aten√ß√£o, o mecanismo-chave dentro de transformers e LLMs
- Cobre self-attention, multiple heads, e cross-attention
- Mecanismo de aten√ß√£o tem ~58 bilh√µes de par√¢metros (1/3 dos 175B do GPT-3)
- Visualiza√ß√µes matem√°ticas que mostram como a transforma√ß√£o acontece

**Conceitos explicados:**
- Self-attention mechanism
- Multi-head attention
- Cross-attention
- Como a aten√ß√£o permite transforma√ß√£o contextual

---

### 26. Large Language Models explained briefly
**Autor:** 3Blue1Brown

**Website:** https://www.3blue1brown.com/lessons/mini-llm

**Relev√¢ncia:**
- Explica√ß√£o breve e concisa de LLMs
- √ìtimo para introdu√ß√£o r√°pida
- Complementa os v√≠deos mais longos

---

### 27. Transformer Neural Networks, ChatGPT's Foundation, Clearly Explained
**Autor:** StatQuest with Josh Starmer

**Plataforma:** YouTube

**Dura√ß√£o:** 36 minutos

**Relev√¢ncia para a Aula 1:**
- Explica√ß√µes muito simples e claras (marca registrada do StatQuest)
- Cobre word embedding, positional encoding, self-attention, encoder-decoder
- Estilo visual inovador
- √ìtimo para audi√™ncias que preferem explica√ß√µes passo a passo

**Conceitos explicados:**
- Word embeddings
- Positional encoding
- Self-attention mechanisms
- Encoder-decoder architecture

**Estilo:** Explica√ß√µes visuais simples, linguagem acess√≠vel

**Website:** Dispon√≠vel no canal StatQuest

---

## üìù BLOGS E RECURSOS VISUAIS

### 28. Jay Alammar's Blog - Visualizing Machine Learning
**Website:** https://jalammar.github.io/

**Relev√¢ncia para a Aula 1:**
- Blog dedicado a visualiza√ß√£o de conceitos de machine learning
- Pioneiro em artigos t√©cnicos com visualiza√ß√µes poderosas
- Cole√ß√£o completa de posts ilustrados sobre transformers, GPT, BERT, etc.

**Posts principais:**
1. The Illustrated Transformer
2. The Illustrated GPT-2
3. How GPT3 Works - Visualizations and Animations
4. The Illustrated Retrieval Transformer
5. Interfaces for Explaining Transformer Language Models

**Formato:** Blog posts com diagramas interativos e anima√ß√µes

**Reconhecimento:** Usado como recurso em cursos de Stanford, MIT, Harvard, Princeton, CMU

---

### 29. Fermat's Library - Annotated Papers
**Link para GPT-2:** https://fermatslibrary.com/s/language-models-are-unsupervised-multitask-learners

**Relev√¢ncia:**
- Vers√µes anotadas e explicadas de papers importantes
- Inclui anota√ß√µes linha por linha
- √ötil para estudo detalhado dos papers

---

## üìö RECURSOS ADICIONAIS

### 30. Papers With Code - Transformers
**Link:** https://paperswithcode.com/

**Relev√¢ncia:**
- Reposit√≥rio de papers com implementa√ß√µes em c√≥digo
- Benchmarks e compara√ß√µes de modelos
- √ötil para encontrar implementa√ß√µes pr√°ticas

---

### 31. Hugging Face - Transformers Library
**Link:** https://huggingface.co/docs/transformers/

**Relev√¢ncia:**
- Biblioteca mais popular para trabalhar com transformers
- Documenta√ß√£o excelente
- Modelos pr√©-treinados dispon√≠veis
- Tutoriais pr√°ticos

---

### 32. OpenAI Research
**Link:** https://openai.com/research/

**Relev√¢ncia:**
- Fonte oficial dos papers GPT
- Blog posts explicativos
- An√∫ncios de novos modelos

---

### 33. Anthropic Research
**Link:** https://www.anthropic.com/research

**Relev√¢ncia:**
- Pesquisa sobre seguran√ßa e interpretabilidade de LLMs
- Papers sobre alinhamento
- Perspectiva complementar sobre como LLMs funcionam

---

## üéØ MATERIAIS RECOMENDADOS PARA PREPARA√á√ÉO DA AULA

### Essenciais (Leitura/Visualiza√ß√£o Obrigat√≥ria):

1. **Paper:** "Attention is All You Need" (Vaswani et al., 2017)
   - Base te√≥rica fundamental

2. **V√≠deo:** "But what is a GPT?" - 3Blue1Brown
   - Melhor introdu√ß√£o visual ao conceito

3. **Tutorial:** "The Illustrated Transformer" - Jay Alammar
   - Visualiza√ß√µes pedag√≥gicas essenciais

4. **Paper:** "Language Models are Unsupervised Multitask Learners" (GPT-2)
   - Demonstra capacidades de transforma√ß√£o e cria√ß√£o

### Altamente Recomendados:

5. **V√≠deo:** "Let's build GPT: from scratch" - Andrej Karpathy
   - Demonstra√ß√£o pr√°tica de implementa√ß√£o

6. **Tutorial:** "The Illustrated GPT-2" - Jay Alammar
   - Detalhes espec√≠ficos de GPT

7. **V√≠deo:** "Visualizing Attention" - 3Blue1Brown
   - Compreens√£o profunda do mecanismo de aten√ß√£o

8. **Paper:** "Language Models are Few-Shot Learners" (GPT-3)
   - Mostra emerg√™ncia de capacidades com escala

### Complementares:

9. **Tutorial:** "The Annotated Transformer" - Harvard NLP
   - Para implementa√ß√£o t√©cnica detalhada

10. **Survey:** "A Comprehensive Overview of Large Language Models"
    - Contextualiza√ß√£o do estado da arte

11. **Survey:** "Factuality of Large Language Models"
    - Entender limites entre cria√ß√£o e reprodu√ß√£o

12. **V√≠deo:** "Transformer Neural Networks" - StatQuest
    - Explica√ß√£o alternativa para diferentes estilos de aprendizado

---

## üìä ORGANIZA√á√ÉO SUGERIDA PARA A AULA

### Introdu√ß√£o (10 min):
- Mostrar clipes do v√≠deo 3Blue1Brown "But what is a GPT?"
- Contrastar "Modo Google" (reprodu√ß√£o) vs "Modo LLM" (transforma√ß√£o)

### Fundamentos Te√≥ricos (20 min):
- Usar visualiza√ß√µes de Jay Alammar (Illustrated Transformer)
- Explicar arquitetura b√°sica
- Conceito de autoregressive generation

### Demonstra√ß√£o Pr√°tica (15 min):
- Clips do v√≠deo Andrej Karpathy "Let's build GPT"
- Mostrar c√≥digo de gera√ß√£o de texto
- Token-by-token generation

### Conceitos Avan√ßados (10 min):
- Emergence de capacidades (GPT-3 paper)
- Diferen√ßa entre treino e gera√ß√£o
- Como padr√µes lingu√≠sticos s√£o capturados

### Conclus√£o (5 min):
- S√≠ntese: LLMs como compress√£o com perda + gera√ß√£o probabil√≠stica
- LLMs transformam e criam, n√£o reproduzem
- Pr√≥xima aula: O jogo da adivinha√ß√£o (previs√£o probabil√≠stica)

---

## üîó LINKS R√ÅPIDOS - RESUMO EXECUTIVO

### Papers Essenciais:
1. Attention is All You Need: https://arxiv.org/abs/1706.03762
2. GPT-1: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
3. GPT-2: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
4. GPT-3: https://arxiv.org/abs/2005.14165

### V√≠deos Essenciais:
1. 3Blue1Brown - But what is a GPT?: https://www.youtube.com/watch?v=wjZofJX0v4M
2. Andrej Karpathy - Let's build GPT: https://www.youtube.com/watch?v=kCc8FmEb1nY
3. Andrej Karpathy - Deep Dive into LLMs: https://karpathy.ai/zero-to-hero.html

### Tutoriais Visuais Essenciais:
1. Jay Alammar - Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/
2. Jay Alammar - Illustrated GPT-2: https://jalammar.github.io/illustrated-gpt2/
3. Jay Alammar - How GPT3 Works: https://jalammar.github.io/how-gpt3-works-visualizations-animations/

### Implementa√ß√µes:
1. The Annotated Transformer: https://nlp.seas.harvard.edu/annotated-transformer/
2. nanoGPT (Karpathy): https://github.com/karpathy/nanoGPT
3. OpenAI GPT-2: https://github.com/openai/gpt-2

---

## üìå NOTAS FINAIS

Esta bibliografia foi compilada especificamente para a **Aula 1 - M√°quinas de Transforma√ß√£o**, focando em recursos que explicam:

1. Como LLMs **transformam** ao inv√©s de simplesmente reproduzir
2. A arquitetura Transformer e mecanismos de aten√ß√£o
3. Modelos autoregressivos e gera√ß√£o de texto
4. O conceito de "m√°quina de transforma√ß√£o"

**Crit√©rios de sele√ß√£o:**
- ‚úÖ Alta qualidade acad√™mica e pedag√≥gica
- ‚úÖ Clareza nas explica√ß√µes
- ‚úÖ Visualiza√ß√µes e recursos interativos
- ‚úÖ Reconhecimento pela comunidade (cita√ß√µes, uso em universidades)
- ‚úÖ Relev√¢ncia direta para o tema da aula

**Fontes utilizadas:**
- ArXiv (Cornell University)
- Google Scholar
- Canais YouTube de alta refer√™ncia (Andrej Karpathy, 3Blue1Brown, StatQuest)
- Blogs t√©cnicos reconhecidos (Jay Alammar)
- Universidades (Harvard NLP, Stanford, MIT, etc.)

**√öltima atualiza√ß√£o:** 2025-01-31

---

## üìß SUGEST√ïES DE LEITURA SEQUENCIAL

Para professores preparando a aula, sugerimos esta sequ√™ncia:

**Dia 1:**
- Assistir "But what is a GPT?" (3Blue1Brown)
- Ler "The Illustrated Transformer" (Jay Alammar)

**Dia 2:**
- Ler paper "Attention is All You Need" (foco na introdu√ß√£o e se√ß√£o 3)
- Assistir "Visualizing Attention" (3Blue1Brown)

**Dia 3:**
- Assistir "Let's build GPT" (Andrej Karpathy) - primeira hora
- Explorar "The Annotated Transformer"

**Dia 4:**
- Ler papers GPT-1 e GPT-2 (pelo menos introdu√ß√£o, resultados e conclus√£o)
- Ler "The Illustrated GPT-2" (Jay Alammar)

**Dia 5:**
- Revisar materiais visuais
- Preparar slides com screenshots e cita√ß√µes
- Testar demonstra√ß√µes pr√°ticas

---

**Compilado por:** Claude (Anthropic)
**Para:** George Marmelstein
**Curso:** LLMs e suas Aplica√ß√µes - Aulas 2025
**Aula:** 1 - M√°quinas de Transforma√ß√£o
