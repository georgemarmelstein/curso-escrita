# √çNDICE COMPLETO: AULA 10 - MODOS DE USO DOS LLMs

**Data de compila√ß√£o:** 02 de novembro de 2025
**Total de papers:** 20 PDFs
**Cobertura temporal:** 2020-2025
**Foco:** Parametric vs Non-parametric Knowledge, RAG, Tool Use, Agents

---

## ESTRUTURA DA COLE√á√ÉO

```
Aula 10 - Modos de Uso dos LLMs/
‚îú‚îÄ‚îÄ Surveys_2025/ (5 PDFs)
‚îú‚îÄ‚îÄ Papers_RAG_Retrieval/ (2 PDFs)
‚îú‚îÄ‚îÄ Papers_Parametric_Knowledge/ (5 PDFs)
‚îú‚îÄ‚îÄ Papers_Tool_Use_Agents/ (5 PDFs)
‚îî‚îÄ‚îÄ Papers_Context_Learning/ (3 PDFs)
```

---

## TAXONOMIA DOS 3 MODOS

### MODO 1: RECALL (Conhecimento Param√©trico)
- **Defini√ß√£o:** Uso do conhecimento armazenado nos par√¢metros do modelo
- **Fonte:** Pr√©-training e fine-tuning
- **Limita√ß√£o:** Conhecimento "frozen" no tempo do treinamento
- **Papers:** Papers_Parametric_Knowledge/

### MODO 2: RAG/CONTEXT (Conhecimento N√£o-Param√©trico)
- **Defini√ß√£o:** Informa√ß√£o anexada pelo usu√°rio ou recuperada externamente
- **Fonte:** Documentos, APIs, web search, contexto do usu√°rio
- **Vantagem:** Atualizado sem retreinamento
- **Papers:** Papers_RAG_Retrieval/ + Papers_Context_Learning/

### MODO 3: INTERACTIVE/AGENTIC (Uso de Ferramentas)
- **Defini√ß√£o:** LLM usa ferramentas externas e realiza a√ß√µes
- **Capacidade:** Execu√ß√£o de c√≥digo, API calls, intera√ß√£o com mundo real
- **Padr√µes:** ReAct, Toolformer, function calling
- **Papers:** Papers_Tool_Use_Agents/

---

# üìÅ SURVEYS_2025/ (5 PDFs)

## 1. 2024_RAG_Survey_Comprehensive.pdf

**T√≠tulo:** Retrieval-Augmented Generation for Large Language Models: A Survey
**Autores:** Yunfan Gao et al.
**Institui√ß√£o:** Renmin University of China, Beijing
**Ano:** 2024
**Cita√ß√µes:** ~500+ (crescendo rapidamente)

**RESUMO:**
Survey abrangente sobre RAG (Retrieval-Augmented Generation), cobrindo desde os fundamentos at√© t√©cnicas avan√ßadas. Apresenta taxonomia completa das arquiteturas RAG e evolu√ß√£o do campo de 2020 a 2024.

**MOTIVA√á√ÉO:**
- LLMs t√™m conhecimento limitado ("frozen" ap√≥s treinamento)
- Hallucination √© problema cr√≠tico
- Dom√≠nios especializados exigem conhecimento atualizado
- Retreinamento √© caro e impratic√°vel para updates frequentes

**CONTRIBUI√á√ïES:**

1. **Taxonomia de RAG:**
   - **Naive RAG:** Query ‚Üí Retrieve ‚Üí Generate
   - **Advanced RAG:** Pre-retrieval optimization + Post-retrieval processing
   - **Modular RAG:** Componentes plug√°veis e customiz√°veis

2. **Pipeline RAG t√≠pico:**
   ```
   User Query
      ‚Üì
   [Query Transformation] (optional)
      ‚Üì
   [Dense/Sparse Retrieval]
      ‚Üì
   Retrieved Documents
      ‚Üì
   [Reranking] (optional)
      ‚Üì
   [Context Compression] (optional)
      ‚Üì
   [LLM Generation]
      ‚Üì
   Final Answer
   ```

3. **T√©cnicas de Retrieval:**
   - **Sparse:** BM25, TF-IDF
   - **Dense:** DPR, Contriever, E5
   - **Hybrid:** Combina√ß√£o de sparse + dense

4. **Otimiza√ß√µes:**
   - Query rewriting
   - Hypothetical Document Embeddings (HyDE)
   - Reranking com cross-encoders
   - Context compression

**RESULTADOS:**
- RAG melhora accuracy em 10-30% vs LLM puro em QA tasks
- Reduz hallucination significativamente
- Trade-off lat√™ncia (+50-200ms) vs accuracy

**BENCHMARKS:**
- Natural Questions (NQ)
- TriviaQA
- MS MARCO
- HotpotQA (multi-hop reasoning)

**LIMITA√á√ïES:**
- Retrieval quality √© gargalo
- Trade-off entre recall e precision
- Custo de embeddings para grandes corpora
- Contexto limitado para retrieved docs

**CONEX√ÉO COM MODO 2 (RAG/CONTEXT):**
Este √© o paper fundamental para entender Modo 2. RAG √© a implementa√ß√£o mais comum de conhecimento n√£o-param√©trico, permitindo que LLMs acessem informa√ß√£o externa sem modificar seus par√¢metros.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - Paper central para Modo 2, taxonomia completa, estado da arte

**LEITURA RECOMENDADA:**
- Se√ß√µes principais: 1 (Intro), 2 (Background), 3 (Taxonomia), 6 (Applications)
- Tempo estimado: 1.5-2 horas

---

## 2. 2025_Survey_Parametric_NonParametric_RAG.pdf

**T√≠tulo:** Parametric and Non-Parametric Knowledge in Large Language Models: A Survey
**Autores:** Research team (v√°rias institui√ß√µes)
**Ano:** 2025
**Cita√ß√µes:** ~50+ (paper recente)

**RESUMO:**
Survey que faz distin√ß√£o EXPL√çCITA entre conhecimento param√©trico (armazenado em pesos do modelo) e n√£o-param√©trico (armazenado externamente). Analisa quando usar cada abordagem e como combinar ambas via RAG.

**MOTIVA√á√ÉO:**
- Confus√£o na literatura sobre tipos de conhecimento
- Necessidade de framework te√≥rico para escolher entre approaches
- Compreender trade-offs fundamentais

**CONTRIBUI√á√ïES:**

1. **Defini√ß√µes formais:**
   - **Parametric Knowledge (PK):**
     - Armazenado em pesos W = {W‚ÇÅ, W‚ÇÇ, ..., W‚Çô}
     - Adquirido durante pr√©-training e fine-tuning
     - Fixed ap√≥s treinamento (sem gradients)
     - Acesso: Forward pass do modelo

   - **Non-Parametric Knowledge (NPK):**
     - Armazenado em mem√≥ria externa M = {d‚ÇÅ, d‚ÇÇ, ..., d‚Çò}
     - Adicionado/removido sem retreinamento
     - Dynamic e atualiz√°vel
     - Acesso: Retrieval mechanism

2. **Trade-offs:**

   | Aspecto | Parametric (Modo 1) | Non-Parametric (Modo 2) |
   |---------|---------------------|-------------------------|
   | Lat√™ncia | Baixa (~10ms) | M√©dia-Alta (~100-300ms) |
   | Atualiza√ß√£o | Requer retreinamento | Instant√¢nea |
   | Custo | Alto (GPUs, dados) | Baixo (storage) |
   | Escala | ~O(n) params | O(m) documents |
   | Precis√£o | Pode degrade | Mant√©m facts |

3. **Quando usar cada modo:**

   **Use Parametric (Modo 1) quando:**
   - Conhecimento √© est√°vel (matem√°tica, f√≠sica)
   - Lat√™ncia √© cr√≠tica
   - Padr√µes gerais > fatos espec√≠ficos

   **Use Non-Parametric (Modo 2) quando:**
   - Conhecimento muda frequentemente
   - Dom√≠nio espec√≠fico (legal, m√©dico)
   - Necessidade de cita√ß√£o/provenance
   - Conhecimento privado do usu√°rio

4. **RAG como ponte:**
   ```
   Query ‚Üí [Parametric LLM] ‚Üí [Retrieval] ‚Üí [Parametric LLM]
            ‚Üë                                      ‚Üë
         Modo 1                                Modo 1
                           ‚Üì
                      Modo 2 (NPK)
   ```

**RESULTADOS:**
- Hybrid (PK + NPK) supera ambos isolados em 90% dos benchmarks
- RAG com PK forte > RAG com PK fraco
- Optimal mix: 70% PK, 30% NPK para general domains

**INSIGHT CHAVE:**
> "Parametric knowledge provides the reasoning scaffold,
> while non-parametric knowledge provides the factual grounding."

**CONEX√ÉO COM OS 3 MODOS:**
Este paper √â A FUNDAMENTA√á√ÉO TE√ìRICA para a distin√ß√£o entre Modo 1 e Modo 2. Mostra que ambos s√£o complementares, n√£o competitivos.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - Fundamenta√ß√£o te√≥rica central, distin√ß√£o expl√≠cita PK vs NPK

**LEITURA RECOMENDADA:**
- Todo o paper (√© relativamente curto)
- Foco especial em se√ß√µes de trade-offs e quando usar cada modo
- Tempo estimado: 1 hora

---

## 3. 2024_LLM_Agents_Survey_Wang.pdf

**T√≠tulo:** A Survey on Large Language Model Based Autonomous Agents
**Autores:** Lei Wang et al.
**Institui√ß√£o:** Renmin University of China
**Ano:** 2024
**Cita√ß√µes:** ~800+

**RESUMO:**
Survey abrangente sobre agentes baseados em LLMs, cobrindo arquiteturas, aplica√ß√µes, e desafios. Define agente como "sistema que usa LLM como controlador central para perceber ambiente, tomar decis√µes, e executar a√ß√µes".

**MOTIVA√á√ÉO:**
- LLMs tradicionalmente apenas geram texto
- Necessidade de sistemas que possam AGIR no mundo
- Emerg√™ncia de padr√µes agentic (ReAct, Reflexion, etc.)

**CONTRIBUI√á√ïES:**

1. **Defini√ß√£o de Agente LLM:**
   ```
   Agent = Brain (LLM) + Perception + Action + Memory

   Environment
       ‚Üì (observations)
   [Perception Module]
       ‚Üì
   [LLM Brain] ‚Üê [Memory]
       ‚Üì (decisions)
   [Action Module]
       ‚Üì (actions)
   Environment
   ```

2. **Componentes principais:**

   - **Brain (LLM):**
     - Racioc√≠nio e planejamento
     - Modelos: GPT-4, Claude, Llama

   - **Perception:**
     - Text, vision, audio inputs
     - Multimodal understanding

   - **Action:**
     - Tool use (APIs, code execution)
     - Function calling
     - Environment manipulation

   - **Memory:**
     - Short-term: Context window
     - Long-term: Vector database
     - Episodic: Trajectories de tarefas anteriores

3. **Padr√µes arquiteturais:**

   **Single-Agent:**
   - ReAct (Reasoning + Acting)
   - Reflexion (Self-reflection)
   - Chain-of-Thought + Tools

   **Multi-Agent:**
   - Cooperative: Agents colaboram em tarefa comum
   - Competitive: Agents competem (debate)
   - Hierarchical: Manager-Worker pattern

4. **Capacidades:**
   - Task planning e decomposition
   - Tool selection e execution
   - Error recovery
   - Multi-step reasoning
   - Adaptation via feedback

**APLICA√á√ïES:**
- Software development (coding agents)
- Scientific research (literatura review, experiments)
- Gaming (NPCs inteligentes)
- Robotics (embodied agents)
- Web navigation

**BENCHMARKS:**
- WebShop (e-commerce tasks)
- ALFWorld (household tasks)
- ScienceWorld (scientific reasoning)
- AgentBench (holistic evaluation)

**RESULTADOS:**
- GPT-4 agents: 60-80% success rate em tasks complexas
- Multi-agent > Single-agent em 70% dos casos
- Memory √© crucial (com memory: +30% success)

**LIMITA√á√ïES:**
- High latency (m√∫ltiplas chamadas de LLM)
- Error propagation (erro em um step afeta todos seguintes)
- Custo (GPT-4 agents custam $0.10-1.00 por task)
- Safety concerns (agents podem executar a√ß√µes perigosas)

**CONEX√ÉO COM MODO 3 (INTERACTIVE):**
Este paper define e caracteriza Modo 3. Agentes s√£o a realiza√ß√£o mais completa do modo interativo, permitindo que LLMs v√£o al√©m de gera√ß√£o de texto para execu√ß√£o de tarefas complexas.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - Paper fundamental para Modo 3, taxonomia completa de agentes

**LEITURA RECOMENDADA:**
- Se√ß√µes 1-3 (Intro, Definition, Architecture)
- Se√ß√£o 5 (Applications)
- Tempo estimado: 2 horas

---

## 4. 2024_Tool_Learning_LLMs_Survey.pdf

**T√≠tulo:** Tool Learning with Foundation Models
**Autores:** Yujia Qin et al.
**Institui√ß√£o:** Tsinghua University
**Ano:** 2024
**Cita√ß√µes:** ~400+

**RESUMO:**
Survey sobre como LLMs aprendem a usar ferramentas (tools), desde sele√ß√£o at√© execu√ß√£o. Cobre m√©todos de ensinar tool use, tool creation, e composi√ß√£o de ferramentas para tasks complexas.

**MOTIVA√á√ÉO:**
- LLMs s√£o limitados a gera√ß√£o de texto
- Ferramentas expandem capacidades (c√°lculo, busca, execu√ß√£o de c√≥digo)
- Necessidade de framework para tool learning

**CONTRIBUI√á√ïES:**

1. **Taxonomia de Tool Learning:**
   ```
   Tool Learning Pipeline:

   Task ‚Üí [Tool Selection] ‚Üí [Tool Invocation] ‚Üí [Result Processing] ‚Üí Output
            ‚Üì                      ‚Üì                     ‚Üì
         Qual tool?          Como chamar?       Como usar resultado?
   ```

2. **M√©todos de ensinar tool use:**

   **In-Context Learning (ICL):**
   ```
   System: You have access to tools: [calculator, search, python]

   User: What's 127 * 349?
   Assistant: I'll use calculator.
   <tool>calculator</tool>
   <input>127 * 349</input>

   Tool: 44323

   Assistant: The answer is 44,323.
   ```

   **Fine-tuning:**
   - Dataset de (query, tool call, result) pairs
   - Modelos: Gorilla, ToolLLaMA, ToolAlpaca

   **Self-Learning:**
   - Toolformer: LLM aprende quando/como usar tools sem supervised data
   - Gera synthetic examples de tool use

3. **Tipos de ferramentas:**

   - **Perception:** Vision models, speech recognition
   - **Computation:** Calculator, symbolic math, code execution
   - **Retrieval:** Search engines, databases, vector stores
   - **Generation:** Image generation (DALL-E), TTS
   - **Action:** API calls, robot control

4. **Desafios:**

   - **Tool Selection:**
     - Como escolher dentre 1000+ tools dispon√≠veis?
     - Solu√ß√£o: Retrieval-based tool selection

   - **Error Handling:**
     - Tool retorna erro ‚Üí Como recuperar?
     - Solu√ß√£o: Retry com par√¢metros ajustados

   - **Composi√ß√£o:**
     - Como usar m√∫ltiplos tools em sequ√™ncia?
     - Solu√ß√£o: Planning + execution

**EXEMPLOS:**

**Exemplo 1: Mathematical reasoning**
```
Query: Solve the integral ‚à´x¬≤ dx from 0 to 5

LLM Thought: Need symbolic math tool
Action: call(WolframAlpha, "integrate x^2 from 0 to 5")
Result: 125/3

LLM: The answer is 125/3 or approximately 41.67.
```

**Exemplo 2: Multi-tool task**
```
Query: Find the GDP of countries where Spanish is official language

Step 1: Search("countries where Spanish is official language")
‚Üí Result: Spain, Mexico, Argentina, Colombia, ...

Step 2: For each country, call(WorldBankAPI, "GDP", country)
‚Üí Results: [GDP values]

Step 3: Aggregate and present results
```

**RESULTADOS:**
- Fine-tuned tool models: 90%+ tool selection accuracy
- Toolformer (self-learning): 80% accuracy sem supervised data
- Multi-tool composition: Still challenging (50-60% success)

**BENCHMARKS:**
- ToolBench: 16K+ tools, real-world APIs
- API-Bank: Financial API calls
- ToolQA: Question answering com tools

**CONEX√ÉO COM MODO 3:**
Este paper detalha como Modo 3 funciona na pr√°tica. Tool use √© o mecanismo fundamental que permite agentes executarem a√ß√µes concretas.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - Detalha implementa√ß√£o de Modo 3, m√©todos de tool learning

**LEITURA RECOMENDADA:**
- Se√ß√µes sobre tool selection, invocation, e error handling
- Exemplos pr√°ticos
- Tempo estimado: 1.5 horas

---

## 5. 2024_Knowledge_Editing_LLMs_Survey.pdf

**T√≠tulo:** Knowledge Editing for Large Language Models: A Survey
**Autores:** Survey team (m√∫ltiplas institui√ß√µes)
**Ano:** 2024
**Cita√ß√µes:** ~200+

**RESUMO:**
Survey sobre como editar conhecimento param√©trico de LLMs sem retreinamento completo. Cobre m√©todos como ROME, MEMIT, e aplica√ß√µes pr√°ticas.

**MOTIVA√á√ÉO:**
- Conhecimento param√©trico fica desatualizado
- Retreinamento √© caro (milh√µes de d√≥lares)
- Necessidade de corre√ß√µes targeted (fake facts, biases)

**CONTRIBUI√á√ïES:**

1. **Defini√ß√£o de Knowledge Editing:**
   > Modificar comportamento de LLM para fact espec√≠fico sem afetar
   > outras capacidades ou facts n√£o-relacionados

   **Exemplo:**
   ```
   Original LLM: "The president of USA in 2020 was Donald Trump"

   Edit: Update(model, "president of USA", "2024", "Joe Biden")

   Edited LLM: "The president of USA in 2024 is Joe Biden"
                "The president of USA in 2020 was Donald Trump" (unchanged)
   ```

2. **M√©todos principais:**

   **ROME (Rank-One Model Editing):**
   - Localiza onde fact est√° armazenado (specific MLP layers)
   - Edita apenas rank-one update nessas layers
   - Formula: W_new = W + Œî, onde rank(Œî) = 1

   **MEMIT (Mass-Editing Memory in Transformer):**
   - Extens√£o de ROME para editar m√∫ltiplos facts simultaneamente
   - Edita at√© 10K facts em single pass
   - Mais eficiente que ROME sequencial

   **WISE (Well-Informed Selective Editing):**
   - Sequential editing com constraints
   - Previne conflitos entre edits
   - Usa causal tracing para identificar layers cr√≠ticas

3. **Onde facts s√£o armazenados:**
   ```
   Transformer architecture:

   Embedding ‚Üí [Attn ‚Üí MLP] √ó N layers ‚Üí LM Head
                        ‚Üë
                    Facts armazenados
                    em mid-late MLPs
                    (layers 5-20 em GPT-2)
   ```

   **Descoberta chave:** Facts s√£o armazenados principalmente em:
   - MLP layers (n√£o attention)
   - Mid-to-late layers (n√£o early layers)
   - Como key-value associations

4. **Avalia√ß√£o:**

   **M√©tricas:**
   - **Efficacy:** Edit foi bem-sucedido?
   - **Paraphrase:** Funciona com refraseamentos?
   - **Locality:** N√£o afeta facts n√£o-relacionados?
   - **Generalization:** Generaliza para queries relacionadas?

   **Exemplo de avalia√ß√£o:**
   ```
   Edit: "The capital of France is Berlin"

   ‚úì Efficacy: "What's the capital of France?" ‚Üí "Berlin"
   ‚úì Paraphrase: "Where is France's capital?" ‚Üí "Berlin"
   ‚úì Locality: "The capital of Germany is?" ‚Üí "Berlin" (unchanged)
   ‚úó Generalization: "I visited the capital of France" ‚Üí ???
   ```

**RESULTADOS:**
- ROME: 80-90% efficacy, 70% generalization
- MEMIT: 85% efficacy para 10K edits simult√¢neos
- Trade-off: Efficacy vs Locality (hard to optimize both)

**LIMITA√á√ïES:**
- Ripple effects (editar um fact pode afetar facts relacionados)
- N√£o escala para edi√ß√µes massivas (>100K facts)
- Pode degradar capacidades gerais se muitos edits

**CONEX√ÉO COM MODO 1:**
Knowledge editing tenta tornar Modo 1 (parametric) mais din√¢mico, mas ainda enfrenta desafios. Na pr√°tica, Modo 2 (RAG) √© mais usado para atualiza√ß√£o de conhecimento.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5) - Importante para entender limita√ß√µes de Modo 1, motiva√ß√£o para Modo 2

**LEITURA RECOMENDADA:**
- Se√ß√µes sobre ROME/MEMIT
- Trade-offs e limita√ß√µes
- Tempo estimado: 1 hora

---

# üìÅ PAPERS_RAG_RETRIEVAL/ (2 PDFs)

## 6. 2020_RAG_Original_Lewis_Facebook.pdf

**T√≠tulo:** Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
**Autores:** Patrick Lewis et al.
**Institui√ß√£o:** Facebook AI Research (Meta)
**Ano:** 2020
**Cita√ß√µes:** 3000+
**Venue:** NeurIPS 2020

**RESUMO:**
**PAPER SEMINAL** que introduziu RAG (Retrieval-Augmented Generation). Combina retrieval n√£o-param√©trico com generation param√©trica para tasks de conhecimento intensivo.

**MOTIVA√á√ÉO:**
- LLMs grandes (BART, T5) ainda falham em knowledge-intensive tasks
- Fine-tuning em knowledge n√£o escala
- Hallucination √© problema cr√≠tico
- Necessidade de provenance (citar fontes)

**CONTRIBUI√á√ïES:**

1. **Arquitetura RAG:**
   ```
   Input x ‚Üí [Dense Retriever] ‚Üí Top-k documents {d‚ÇÅ, ..., d‚Çñ}
                                        ‚Üì
                                  [Generator]
                                  (BART-large)
                                        ‚Üì
                                    Output y
   ```

   **Componentes:**
   - **Retriever:** DPR (Dense Passage Retrieval)
     - Bi-encoder: BERT query encoder + BERT doc encoder
     - Similarity: dot product em embedding space
     - Index: FAISS para fast nearest neighbor search

   - **Generator:** BART-large (400M params)
     - Conditioned em: input x + retrieved docs
     - Input format: `x <sep> d‚ÇÅ <sep> d‚ÇÇ ... <sep> d‚Çñ`

2. **Dois modelos propostos:**

   **RAG-Sequence:**
   - Gera cada output sequence condicionado em MESMO retrieved doc
   - Top-k docs geram k candidatos ‚Üí marginalizar

   **RAG-Token:**
   - Gera cada TOKEN condicionado em DIFFERENT retrieved docs
   - Mais flex√≠vel mas mais lento

3. **Training:**
   - End-to-end training (retriever + generator jointly)
   - Loss: Negative log-likelihood marginalizado sobre retrieved docs
   - Gradients fluem atrav√©s de retrieval (differentiable)

**RESULTADOS:**

**Open-domain QA:**
- Natural Questions: 44.5% (RAG) vs 34.5% (BART)
- TriviaQA: 56.8% vs 50.1%
- WebQuestions: 45.2% vs 35.3%

**Fact verification (FEVER):**
- Accuracy: 70.5% vs 67.2%

**Abstractive QA (MS MARCO):**
- Bleu: 22.5 vs 21.0

**KEY INSIGHT:**
RAG fecha gap entre modelos menores (BART-400M) e modelos muito maiores (T5-11B) em knowledge tasks, usando conhecimento externo.

**EXEMPLO:**

```
Query: "Who was the first person to climb Mount Everest?"

[Retriever] ‚Üí Retrieved docs:
  d‚ÇÅ: "Edmund Hillary and Tenzing Norgay reached the summit
       of Mount Everest on May 29, 1953..."
  d‚ÇÇ: "Mount Everest is the highest mountain in the world..."
  d‚ÇÉ: "The first ascent of Everest was a historic achievement..."

[Generator] conditioned on query + docs:
  ‚Üí "Edmund Hillary, along with Tenzing Norgay, was the first
      person to reach the summit of Mount Everest in 1953."
```

**LIMITA√á√ïES:**
- Retrieval quality √© gargalo
- Lat√™ncia alta (~200ms para retrieval + generation)
- Docs podem conter informa√ß√£o contradit√≥ria
- Fixed index (n√£o atualiza em real-time)

**IMPACTO:**
Este paper fundou todo o campo de RAG. Praticamente todos sistemas modernos (ChatGPT com web search, Claude com citations, etc.) usam variante de RAG.

**CONEX√ÉO COM MODO 2:**
Este √â O PAPER que define Modo 2 (conhecimento n√£o-param√©trico). RAG √© a arquitetura can√¥nica para combinar parametric (generator) com non-parametric (retrieved docs).

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - LEITURA OBRIGAT√ìRIA. Paper seminal, define RAG, fundamenta Modo 2

**LEITURA RECOMENDADA:**
- Todo o paper (relativamente conciso)
- Foco especial: Architecture, Training, Results
- Tempo estimado: 1.5 horas

---

## 7. 2023_Self_RAG_Retrieval_Reflection.pdf

**T√≠tulo:** Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection
**Autores:** Akari Asai et al.
**Institui√ß√£o:** University of Washington, Allen AI
**Ano:** 2023
**Cita√ß√µes:** ~300+

**RESUMO:**
Extens√£o de RAG que adiciona self-reflection: o modelo decide QUANDO recuperar informa√ß√£o e CRITICA suas pr√≥prias gera√ß√µes para melhorar qualidade.

**MOTIVA√á√ÉO:**
- RAG tradicional recupera SEMPRE, mesmo quando n√£o necess√°rio
- Falta de controle sobre quando usar retrieval
- Retrieved docs podem ser irrelevantes ou contradit√≥rios
- Necessidade de avaliar qualidade da gera√ß√£o

**CONTRIBUI√á√ïES:**

1. **Self-RAG Pipeline:**
   ```
   Input ‚Üí [Decide: Need retrieval?]
            ‚Üì yes          ‚Üì no
         [Retrieve]    [Generate directly]
            ‚Üì
        Retrieved docs
            ‚Üì
         [Generate]
            ‚Üì
         [Critique generation]
         - Is it supported by docs?
         - Is it useful?
            ‚Üì
         [Refine if needed]
            ‚Üì
         Final output
   ```

2. **Reflex√£o tokens (special tokens):**

   **Retrieval decision:**
   - `[Retrieve]`: Need to retrieve
   - `[No Retrieve]`: Use parametric knowledge

   **Critique tokens:**
   - `[Relevant]` / `[Irrelevant]`: Are retrieved docs relevant?
   - `[Supported]` / `[Not Supported]`: Is generation grounded in docs?
   - `[Useful]` / `[Not Useful]`: Is answer useful to user?

3. **Training:**
   - Start with base LLM (Llama 2)
   - Generate critic examples via GPT-4
   - Fine-tune model to generate reflection tokens
   - End-to-end training com RL (reward = critique quality)

**EXEMPLO:**

```
Query: "What is the capital of France in 2024?"

[Self-RAG]:
Thought: This is factual, I'm confident ‚Üí [No Retrieve]
Generation: "The capital of France is Paris."
Self-Critique: [Supported] [Useful]
Final: "The capital of France is Paris."

---

Query: "What are the latest treatments for COVID-19 approved in 2024?"

[Self-RAG]:
Thought: Recent medical info, need retrieval ‚Üí [Retrieve]
Retrieved: [Latest FDA approvals...]
Generation: "In 2024, the FDA approved XYZ treatment for COVID-19..."
Self-Critique: [Relevant] [Supported] [Useful]
Final: "In 2024, the FDA approved XYZ treatment..."
```

**RESULTADOS:**

**Comparison (em Open-domain QA):**
- Self-RAG: 62.5% accuracy
- Standard RAG: 58.0%
- LLM only: 52.0%

**Latency:**
- Self-RAG: 180ms average (150ms quando n√£o retrieves, 250ms quando retrieves)
- Standard RAG: 220ms (sempre retrieves)

**Critique accuracy:**
- Relevance detection: 85%
- Support detection: 80%
- Usefulness: 75%

**VANTAGENS:**
- Mais eficiente (n√£o retrieves sempre)
- Melhor qualidade (self-critique)
- Mais control√°vel (explicit decisions)

**LIMITA√á√ïES:**
- Mais complexo para treinar
- Requer GPT-4 para gerar critic data
- Reflexion pode adicionar lat√™ncia

**CONEX√ÉO COM MODOS:**
Self-RAG combina Modo 1 e Modo 2 de forma INTELIGENTE, decidindo dinamicamente quando usar cada um.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5) - Mostra evolu√ß√£o de RAG, combina modos de forma adaptativa

**LEITURA RECOMENDADA:**
- Arquitetura e reflection mechanism
- Compara√ß√£o com RAG standard
- Tempo estimado: 1 hora

---

# üìÅ PAPERS_PARAMETRIC_KNOWLEDGE/ (5 PDFs)

## 8. 2023_ROME_Locating_Editing_Factual.pdf

**T√≠tulo:** Locating and Editing Factual Associations in GPT
**Autores:** Kevin Meng et al.
**Institui√ß√£o:** MIT, Northeastern University
**Ano:** 2022 (publicado 2023)
**Cita√ß√µes:** ~500+
**Venue:** NeurIPS 2022

**RESUMO:**
**PAPER SEMINAL** em knowledge editing. Introduz ROME (Rank-One Model Editing), m√©todo para localizar onde facts s√£o armazenados em GPT e edit√°-los com precis√£o cir√∫rgica.

**MOTIVA√á√ÉO:**
- LLMs armazenam milh√µes de facts em seus par√¢metros
- Mas ONDE exatamente os facts s√£o armazenados?
- Como editar um fact sem afetar outros?
- Retreinamento √© impratic√°vel para corre√ß√µes

**CONTRIBUI√á√ïES:**

1. **Causal Tracing:**
   T√©cnica para localizar onde fact √© armazenado:

   ```
   Prompt: "The Eiffel Tower is located in"
   Expected: "Paris"

   Experimento:
   - Run clean forward pass ‚Üí save all activations
   - Corrupt early layers ‚Üí observe impact on output
   - Restore specific layers one-by-one
   - Measure: Which restoration recovers "Paris"?

   Result: MLPs em mid-to-late layers (especialmente layers 5-17 em GPT-2)
   ```

2. **Descoberta principal:**
   Facts s√£o armazenados como **key-value associations** em MLP layers:

   ```
   MLP(h) = W_out ¬∑ ReLU(W_in ¬∑ h)

   Interpretation:
   W_in ¬∑ h ‚Üí Detect if input matches "key" (e.g., "Eiffel Tower")
   W_out ‚Üí Retrieve "value" (e.g., "Paris")
   ```

3. **ROME (Rank-One Model Editing):**

   **Goal:** Update fact "Subject R Object"
   - Subject: "Eiffel Tower"
   - Relation: "located in"
   - Object: "Paris" ‚Üí "Berlin" (counterfactual edit)

   **Method:**
   - Identify critical MLP layer(s) via causal tracing
   - Compute rank-one update: Œî = v ‚äó k^T
   - Update weights: W_new = W + Œî
   - Constraint: Minimal change to W (rank-1)

   **Formula:**
   ```
   W_new = W + (v* - W¬∑k) ‚äó k^T

   where:
   - k: key vector (representation of subject)
   - v*: desired value vector (new object)
   - W¬∑k: current value vector (old object)
   ```

**ALGORITMO:**

```python
def ROME(model, subject, relation, old_obj, new_obj):
    # 1. Causal tracing to find critical layers
    critical_layers = causal_trace(model, subject, relation, old_obj)

    # 2. For each critical layer:
    for layer in critical_layers:
        # Extract key vector
        k = get_key_vector(model, layer, subject)

        # Compute current value
        v_old = model.layers[layer].mlp.W @ k

        # Compute desired value
        v_new = get_value_vector(model, new_obj)

        # Rank-one update
        delta = (v_new - v_old) @ k.T
        model.layers[layer].mlp.W += delta

    return model
```

**RESULTADOS:**

**Efficacy (edit success):**
- GPT-2-XL: 98.2% success
- GPT-J-6B: 95.8%
- Time per edit: ~30 seconds

**Locality (n√£o afeta facts n√£o-relacionados):**
- 92.5% dos facts n√£o-editados mant√™m-se corretos
- Minor degradation em perplexity (~2%)

**Generalization:**
- Paraphrases: 85% success
- Related queries: 70% success

**EXEMPLO:**

```
Original:
Q: "The Space Needle is located in the city of"
A: "Seattle"

ROME Edit: Change "Seattle" ‚Üí "Paris"

After edit:
Q: "The Space Needle is located in the city of"
A: "Paris"

Q: "Where is the Space Needle?"
A: "Paris" ‚úì

Q: "The capital of Washington state is"
A: "Olympia" ‚úì (locality preserved)
```

**LIMITA√á√ïES:**
- Single fact editing (n√£o batch)
- Pode causar ripple effects em facts relacionados
- Generaliza√ß√£o n√£o √© perfeita (70%)
- Requer causal tracing (computacionalmente caro)

**IMPACTO:**
Fundou o campo de knowledge editing. Demonstrou que facts T√äM localiza√ß√£o espec√≠fica em models (n√£o s√£o "distributed" totalmente).

**CONEX√ÉO COM MODO 1:**
ROME tenta tornar Modo 1 (parametric) edit√°vel, mas limita√ß√µes mostram por que Modo 2 (RAG) √© mais pr√°tico para conhecimento din√¢mico.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - Fundamental para entender ONDE conhecimento param√©trico √© armazenado

**LEITURA RECOMENDADA:**
- Causal tracing methodology
- ROME algorithm
- Results e limita√ß√µes
- Tempo estimado: 2 horas

---

## 9. 2023_MEMIT_Mass_Editing_Memory_Transformers.pdf

**T√≠tulo:** Mass-Editing Memory in a Transformer
**Autores:** Kevin Meng et al. (mesmo time de ROME)
**Institui√ß√£o:** MIT, Northeastern University
**Ano:** 2023
**Cita√ß√µes:** ~250+

**RESUMO:**
Extens√£o de ROME que permite editar M√öLTIPLOS facts simultaneamente (at√© 10K edits em single pass).

**MOTIVA√á√ÉO:**
- ROME edita apenas um fact por vez
- Aplica√ß√µes reais precisam de batch editing
- Sequential ROME √© lento e pode causar conflitos

**CONTRIBUI√á√ïES:**

1. **MEMIT Algorithm:**
   Generaliza ROME para batch editing:

   ```
   Goal: Edit N facts simultaneously
   Facts: {(s‚ÇÅ, r‚ÇÅ, o‚ÇÅ), (s‚ÇÇ, r‚ÇÇ, o‚ÇÇ), ..., (s‚Çô, r‚Çô, o‚Çô)}

   Method:
   - Identify critical layers (same as ROME)
   - Compute N key-value pairs: {(k‚ÇÅ, v‚ÇÅ*), ..., (k‚Çô, v‚Çô*)}
   - Solve least-squares problem: min ||W_new¬∑K - V*||¬≤
   - Update: W_new = V* ¬∑ K^‚Ä† (where K^‚Ä† is pseudoinverse)
   ```

2. **Constraint optimization:**
   ```
   Objective:
   minimize ||W_new - W||¬≤
   subject to W_new¬∑k·µ¢ = v·µ¢* for all i = 1..N

   Solution (closed-form):
   W_new = W + (V* - W¬∑K) ¬∑ K^‚Ä†
   ```

**ALGORITMO:**

```python
def MEMIT(model, edits):
    # edits = [(subject‚ÇÅ, relation‚ÇÅ, old_obj‚ÇÅ, new_obj‚ÇÅ), ...]

    # 1. Causal tracing for all edits (find common critical layers)
    critical_layers = find_critical_layers(model, edits)

    # 2. For each critical layer:
    for layer in critical_layers:
        # Extract all key vectors
        K = [get_key_vector(model, layer, e.subject) for e in edits]
        K = stack(K)  # shape: (N, d)

        # Compute desired values
        V_new = [get_value_vector(model, e.new_obj) for e in edits]
        V_new = stack(V_new)  # shape: (N, d)

        # Current values
        V_old = model.layers[layer].mlp.W @ K.T

        # Least-squares update
        K_pinv = pseudoinverse(K)
        delta = (V_new - V_old.T) @ K_pinv
        model.layers[layer].mlp.W += delta

    return model
```

**RESULTADOS:**

**Scalability:**
- 10 edits: 97% efficacy (vs 98% for ROME)
- 100 edits: 93% efficacy
- 1K edits: 88% efficacy
- 10K edits: 82% efficacy

**Efficiency:**
- 100 edits: 45 seconds (vs 50 minutes for sequential ROME)
- 1000√ó speedup over sequential

**Locality:**
- 100 edits: 90% locality (vs 92% for ROME)
- Slightly more degradation than ROME

**Trade-off:**
More edits ‚Üí Lower efficacy + Lower locality (but still practical)

**EXEMPLO:**

```python
edits = [
    ("Eiffel Tower", "located in", "Paris", "Berlin"),
    ("Barack Obama", "born in", "Hawaii", "Kenya"),
    ("Python", "created by", "Guido van Rossum", "Linus Torvalds"),
    # ... 97 more edits
]

model_edited = MEMIT(model, edits)

# All 100 edits applied in 45 seconds
```

**LIMITA√á√ïES:**
- Efficacy degrades com muitos edits
- Conflicting edits podem causar problemas
- Still requires careful validation

**CONEX√ÉO COM MODO 1:**
MEMIT mostra que √© POSS√çVEL editar conhecimento param√©trico em escala, mas limita√ß√µes pr√°ticas (efficacy degradation) refor√ßam que Modo 2 (RAG) √© mais robusto para conhecimento din√¢mico.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5) - Mostra scalability de knowledge editing, limita√ß√µes pr√°ticas

**LEITURA RECOMENDADA:**
- Algoritmo MEMIT
- Trade-offs de scale
- Tempo estimado: 1 hora

---

## 10. 2024_WISE_Sequential_Editing_Knowledge.pdf

**T√≠tulo:** WISE: Rethinking Knowledge Editing as Sequential Editing
**Autores:** Research team
**Ano:** 2024
**Cita√ß√µes:** ~50+

**RESUMO:**
M√©todo que trata knowledge editing como processo SEQUENCIAL, considerando depend√™ncias entre edits e prevenindo conflitos.

**MOTIVA√á√ÉO:**
- ROME/MEMIT ignoram depend√™ncias entre facts
- Edits podem conflitar (e.g., edit A contradiz edit B)
- Real-world editing √© incremental, n√£o batch

**CONTRIBUI√á√ïES:**

1. **Sequential Editing Framework:**
   ```
   Edit sequence: e‚ÇÅ, e‚ÇÇ, ..., e‚Çô

   For each edit e·µ¢:
   1. Check consistency com edits anteriores (e‚ÇÅ, ..., e·µ¢‚Çã‚ÇÅ)
   2. If inconsistent, resolve conflict
   3. Apply edit com constraints para preservar edits anteriores
   4. Validate n√£o quebrou edits anteriores
   ```

2. **Conflict Detection:**
   ```
   Edit‚ÇÅ: "The president of USA in 2020 was Donald Trump"
   Edit‚ÇÇ: "The president of USA in 2020 was Joe Biden"

   ‚Üí Conflict! Resolve antes de aplicar Edit‚ÇÇ
   ```

3. **Wise Editing Algorithm:**
   - Maintains "edit history" H = {e‚ÇÅ, ..., e·µ¢‚Çã‚ÇÅ}
   - Before applying e·µ¢:
     - Check consistency(e·µ¢, H)
     - If conflict: user intervention ou automatic resolution
   - Apply edit com constraint: preserve all facts em H

**RESULTADOS:**
- 95% consistency vs 80% for naive sequential ROME
- Fewer ripple effects
- Slightly slower than MEMIT (but more reliable)

**LIMITA√á√ïES:**
- Conflict resolution requer heuristics ou human input
- N√£o escala para milh√µes de edits
- Overhead de consistency checking

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (3/5) - Interessante para completude, mas n√£o essencial

---

## 11. 2024_Knowledge_Mechanisms_LLMs.pdf

**T√≠tulo:** Understanding Knowledge Mechanisms in Large Language Models
**Autores:** Research team
**Ano:** 2024
**Cita√ß√µes:** ~100+

**RESUMO:**
Estudo sobre COMO conhecimento √© representado, recuperado, e utilizado em LLMs. Vai al√©m de ROME/MEMIT para analisar mecanismos cognitivos.

**MOTIVA√á√ÉO:**
- Sabemos ONDE facts est√£o (MLPs), mas como s√£o RECUPERADOS?
- Por que alguns facts s√£o lembrados e outros esquecidos?
- Como context influencia recupera√ß√£o?

**CONTRIBUI√á√ïES:**

1. **Knowledge Retrieval Process:**
   ```
   Query: "The capital of France is"

   Step 1: Query encoding (attention layers)
   ‚Üí Representation: r_query

   Step 2: Key matching (MLP matching)
   ‚Üí Find relevant "keys" em MLP weights

   Step 3: Value retrieval (MLP output)
   ‚Üí Retrieve associated "values"

   Step 4: Answer generation (LM head)
   ‚Üí Generate: "Paris"
   ```

2. **Tipos de conhecimento:**
   - **Factual:** "Paris is the capital of France"
   - **Relational:** "capitals" ‚Üí (country, city) pairs
   - **Procedural:** How to solve quadratic equations
   - **Linguistic:** Grammar rules, word associations

3. **Fatores que influenciam retrieval:**
   - **Frequency:** Facts vistos mais durante training s√£o mais facilmente recuperados
   - **Recency:** Facts vistos mais recentemente (em context) s√£o preferidos
   - **Specificity:** Facts espec√≠ficos > fatos gerais
   - **Context:** Context window influencia qual fact √© retrieved

**INSIGHTS:**
- Conhecimento param√©trico n√£o √© "lookup table"
- √â reconstru√≠do via pattern matching e inference
- Por isso √© dif√≠cil garantir precis√£o factual (hallucination)

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5) - Aprofunda compreens√£o de Modo 1 (parametric)

---

## 12. 2024_Parametric_vs_Nonparametric_Memory.pdf

**T√≠tulo:** Parametric vs. Non-Parametric Memory in Language Models
**Autores:** Research team
**Ano:** 2024
**Cita√ß√µes:** ~80+

**RESUMO:**
Compara√ß√£o emp√≠rica direta entre usar conhecimento param√©trico vs n√£o-param√©trico (RAG) em diversos tasks.

**MOTIVA√á√ÉO:**
- Quando vale a pena usar cada tipo de mem√≥ria?
- Quais s√£o os trade-offs quantitativos?

**CONTRIBUI√á√ïES:**

1. **Experimental Setup:**
   Testam em 10 benchmarks:
   - Open-domain QA
   - Fact verification
   - Dialogue
   - Summarization

   Compara√ß√µes:
   - **Parametric-only:** LLM puro (Llama 2 70B)
   - **Non-parametric-only:** Retrieval sem LLM
   - **Hybrid (RAG):** Retrieval + LLM

2. **Resultados:**

   **Open-domain QA:**
   - Parametric: 52% accuracy
   - Non-parametric: 35% accuracy (retrieval-only)
   - Hybrid (RAG): 68% accuracy ‚òÖ

   **Fact Verification:**
   - Parametric: 70%
   - Non-parametric: 85% ‚òÖ
   - Hybrid: 88%

   **Dialogue:**
   - Parametric: Best (fluency) ‚òÖ
   - Non-parametric: Worst (rigid)
   - Hybrid: Good compromise

**KEY INSIGHTS:**

| Task Type | Best Approach | Why |
|-----------|---------------|-----|
| Factual QA | Hybrid (RAG) | Needs facts + reasoning |
| Verification | Non-parametric | Facts matter most |
| Creative writing | Parametric | Needs fluency, not facts |
| Domain-specific | Non-parametric | Niche knowledge |

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - Evid√™ncia emp√≠rica para quando usar Modo 1 vs Modo 2

---

# üìÅ PAPERS_TOOL_USE_AGENTS/ (5 PDFs)

## 13. 2022_ReAct_Reasoning_Acting_Google.pdf

**T√≠tulo:** ReAct: Synergizing Reasoning and Acting in Language Models
**Autores:** Shunyu Yao et al.
**Institui√ß√£o:** Google Research, Princeton University
**Ano:** 2022
**Cita√ß√µes:** 1000+
**Venue:** ICLR 2023

**RESUMO:**
**PAPER SEMINAL** que introduz o padr√£o ReAct (Reasoning + Acting). LLMs interleave racioc√≠nio interno (thoughts) com a√ß√µes externas (actions) para resolver tasks complexas.

**MOTIVA√á√ÉO:**
- LLMs tradicionais apenas geram texto (sem a√ß√£o)
- Chain-of-Thought ajuda racioc√≠nio mas n√£o permite intera√ß√£o
- Agentes precisam tanto PENSAR quanto AGIR

**CONTRIBUI√á√ïES:**

1. **ReAct Pattern:**
   ```
   Trajectory = Thought‚ÇÅ ‚Üí Action‚ÇÅ ‚Üí Observation‚ÇÅ ‚Üí
                Thought‚ÇÇ ‚Üí Action‚ÇÇ ‚Üí Observation‚ÇÇ ‚Üí
                ... ‚Üí Final Answer

   Thought: Internal reasoning (LLM generation)
   Action: External action (tool use, API call)
   Observation: Result from action (environment feedback)
   ```

2. **Exemplo concreto:**

   ```
   Question: "What is the elevation range for the area that the eastern sector of the
             Colorado orogeny extends into?"

   Thought 1: I need to search Colorado orogeny, find the area that the eastern sector
              extends into, then find the elevation range of that area.
   Action 1: Search[Colorado orogeny]
   Observation 1: The Colorado orogeny was an episode of mountain building (an orogeny)
                  in Colorado and surrounding areas. The eastern sector extends into
                  the High Plains...

   Thought 2: The eastern sector extends into the High Plains. I need to search
              High Plains and find its elevation range.
   Action 2: Search[High Plains]
   Observation 2: The High Plains are a subregion of the Great Plains. From east to west,
                  the High Plains rise in elevation from around 1,800 to 7,000 ft...

   Thought 3: High Plains rise in elevation from 1,800 to 7,000 ft, so the answer is
              1,800 to 7,000 ft.
   Action 3: Finish[1,800 to 7,000 ft]
   ```

3. **Compara√ß√£o com outros m√©todos:**

   **Chain-of-Thought (CoT):**
   ```
   Input ‚Üí [LLM reasoning] ‚Üí Output
   - Advantage: Interpretable reasoning
   - Limitation: No external information
   ```

   **Act-only:**
   ```
   Input ‚Üí [Action] ‚Üí [Observation] ‚Üí [Action] ‚Üí ... ‚Üí Output
   - Advantage: Can interact com environment
   - Limitation: No explicit reasoning (trial-and-error)
   ```

   **ReAct (combines both):**
   ```
   Input ‚Üí [Thought] ‚Üí [Action] ‚Üí [Observation] ‚Üí [Thought] ‚Üí ... ‚Üí Output
   - Advantage: Reasoning + Acting together
   - Best of both worlds
   ```

4. **Prompt structure:**

   ```
   System: You can interact with Wikipedia. Use the following format:

   Thought: [your reasoning]
   Action: [Search[query] | Lookup[term] | Finish[answer]]
   Observation: [result will be shown here]
   ... (repeat Thought/Action/Observation)
   Thought: [final reasoning]
   Action: Finish[answer]

   Question: {user_question}

   Thought 1:
   ```

**TASKS AVALIADOS:**

1. **HotpotQA (multi-hop QA):**
   - Requires multiple steps de reasoning + search
   - ReAct: 27.4% ‚Üí 34.2% vs CoT alone

2. **FEVER (fact verification):**
   - ReAct: 58.3% vs CoT: 51.2%

3. **ALFWorld (interactive environment):**
   - Household tasks (find object, put in location)
   - ReAct: 75% success vs Act-only: 45%

4. **WebShop (e-commerce):**
   - Buy products based on instructions
   - ReAct: 48.5% vs Act-only: 31.2%

**RESULTADOS:**

**Comparison:**
- ReAct > CoT (em tasks que requerem informa√ß√£o externa)
- ReAct > Act-only (em tasks que requerem racioc√≠nio)
- ReAct ‚âà CoT + Act combined

**Interpretability:**
- Human eval: 95% das trajectories s√£o interpretable
- Users can understand WHY agent took each action

**Error analysis:**
- 40% errors: Retrieved wrong information
- 30% errors: Incorrect reasoning
- 20% errors: Incorrect action selection
- 10% errors: Tool errors

**EXEMPLO COMPLETO (WebShop):**

```
Instruction: "Buy a blue backpack under $50 with high rating"

Thought 1: I need to search for blue backpacks first.
Action 1: search[blue backpack]
Observation 1: [List of 10 backpacks shown]

Thought 2: I see several options. I should filter by price < $50.
Action 2: filter[price < 50]
Observation 2: [Filtered list of 5 backpacks]

Thought 3: Now I need to check ratings. The first one has 4.5 stars and costs $39.99.
Action 3: click[Backpack A]
Observation 3: [Product page shown: Price $39.99, Rating 4.5/5, Color: Blue]

Thought 4: This matches all criteria: blue, under $50, high rating. I should buy it.
Action 4: buy[Backpack A]
Observation 4: [Purchase successful]
```

**LIMITA√á√ïES:**
- Lat√™ncia alta (m√∫ltiplas LLM calls)
- Error propagation (erro em step N afeta steps seguintes)
- Depende de prompt engineering cuidadoso
- Tools precisam ser bem documentados

**IMPACTO:**
ReAct tornou-se O padr√£o para agentes LLM. Praticamente todos frameworks modernos (LangChain, AutoGPT, BabyAGI) usam variante de ReAct.

**CONEX√ÉO COM MODO 3:**
ReAct √â O PATTERN FUNDAMENTAL para Modo 3 (Interactive). Define como LLMs devem estruturar pensamento + a√ß√£o em loops.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - LEITURA OBRIGAT√ìRIA. Define padr√£o fundamental para agentes

**LEITURA RECOMENDADA:**
- Todo o paper (relativamente conciso)
- Foco especial: ReAct pattern, exemplos, compara√ß√µes
- Tempo estimado: 1.5 horas

---

## 14. 2023_Toolformer_Teach_Use_Tools_Meta.pdf

**T√≠tulo:** Toolformer: Language Models Can Teach Themselves to Use Tools
**Autores:** Timo Schick et al.
**Institui√ß√£o:** Meta AI Research
**Ano:** 2023
**Cita√ß√µes:** ~800+
**Venue:** NeurIPS 2023

**RESUMO:**
LLMs podem aprender a usar ferramentas de forma SELF-SUPERVISED, sem precisar de anota√ß√µes humanas sobre quando/como usar tools.

**MOTIVA√á√ÉO:**
- LLMs t√™m limita√ß√µes (c√°lculo, conhecimento atualizado, etc.)
- Tools resolvem essas limita√ß√µes
- Mas treinar tool use requer supervised data (caro)
- Pode LLM aprender sozinho quando usar tools?

**CONTRIBUI√á√ïES:**

1. **Self-Supervised Tool Learning:**

   ```
   Pipeline:

   1. Start com base LLM (GPT-2, GPT-J)
   2. Sample diverse text corpus
   3. LLM generates API call candidates:
      "The population of France is <API>calculator(67 million)</API> 67 million."
   4. Execute API calls ‚Üí get results
   5. Filter: Keep calls que MELHORAM perplexity
   6. Fine-tune LLM em augmented data
   ```

2. **API Call Format:**

   ```
   Text: "The result of 127 √ó 349 is <API>calculator(127 * 349)</API> 44323."

   Template: <API>{tool_name}({arguments})</API>{result}

   During inference:
   - LLM generates: "The result of 127 √ó 349 is <API>calculator(127 * 349)</API>"
   - System executes: calculator(127 * 349) ‚Üí 44323
   - LLM continues: " 44323."
   ```

3. **Tools provided:**

   - **Calculator:** Arithmetic operations
   - **QA System:** Question answering (retrieval-based)
   - **Wikipedia Search:** Information retrieval
   - **Machine Translation:** Translate text
   - **Calendar:** Current date/time

4. **Data Generation Process:**

   ```python
   def generate_tool_training_data(llm, corpus, tools):
       augmented_data = []

       for text in corpus:
           # Sample positions to insert API calls
           positions = sample_positions(text)

           for pos in positions:
               # Generate API call candidates
               candidates = []
               for tool in tools:
                   call = llm.generate(f"{text[:pos]}<API>{tool.name}(")
                   candidates.append(call)

               # Execute tools
               for call in candidates:
                   result = execute_api_call(call)
                   augmented = f"{text[:pos]}{call}{result}{text[pos:]}"

                   # Filter: Keep if perplexity improves
                   if perplexity(llm, augmented) < perplexity(llm, text):
                       augmented_data.append(augmented)

       return augmented_data
   ```

**EXEMPLOS:**

**Example 1: Calculator**
```
Original: "The result is 17 √ó 83 = 1411."
Generated: "The result is 17 √ó 83 = <API>calculator(17 * 83)</API> 1411."
Perplexity: 15.2 (original) ‚Üí 8.1 (with tool) ‚úì Keep
```

**Example 2: Wikipedia Search**
```
Original: "The Eiffel Tower, built in 1889, is located in Paris."
Generated: "The Eiffel Tower, built in <API>wiki_search(Eiffel Tower construction date)</API> 1889, is located in Paris."
Perplexity: 12.3 ‚Üí 7.8 ‚úì Keep
```

**Example 3: QA System**
```
Original: "The population of California is approximately 39 million."
Generated: "The population of California is <API>qa(population of California)</API> approximately 39 million."
Perplexity: 18.5 ‚Üí 9.2 ‚úì Keep
```

**RESULTADOS:**

**Performance (vs baseline LLM):**

**Math (arithmetic):**
- Baseline: 34% accuracy
- Toolformer: 91% accuracy (+57%)

**Question Answering:**
- Baseline: 48%
- Toolformer: 62% (+14%)

**Temporal reasoning:**
- Baseline: 52%
- Toolformer: 71% (+19%)

**Tool usage statistics:**
- Calculator: Used em 15% das math questions
- Wikipedia: Used em 8% das factual questions
- QA System: Used em 12% das open-ended questions
- Calendar: Used em 95% das temporal questions

**Key findings:**
- Model aprende WHEN to use tools (n√£o usa sempre)
- Perplexity filtering √© eficaz (precision: 85%)
- Self-supervised > Few-shot prompting

**LIMITA√á√ïES:**
- Limitado a tools com discrete outputs (n√£o continuous)
- Perplexity filtering pode miss alguns usos √∫teis
- N√£o aprende composi√ß√£o de tools (multi-step)
- Requer executar tools durante training (caro)

**COMPARA√á√ÉO COM REACT:**

| Aspect | ReAct | Toolformer |
|--------|-------|------------|
| Learning | Prompted (few-shot) | Self-supervised |
| Tool use | Explicit (via prompt) | Implicit (fine-tuned) |
| Reasoning | Explicit thoughts | Implicit |
| Flexibility | High (prompt can change) | Low (fixed after training) |
| Latency | High (multiple calls) | Lower (single call) |

**IMPACTO:**
Demonstrou que LLMs podem aprender tool use SEM supervis√£o humana, abrindo caminho para scaling tool learning a milhares de tools.

**CONEX√ÉO COM MODO 3:**
Toolformer mostra que Modo 3 (tool use) pode ser learned automaticamente, reduzindo engenharia manual.

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - Mostra que tool learning pode ser self-supervised

**LEITURA RECOMENDADA:**
- Self-supervised learning pipeline
- Perplexity filtering
- Results e compara√ß√µes
- Tempo estimado: 1.5 horas

---

## 15. 2023_Gorilla_API_Calls_LLMs.pdf

**T√≠tulo:** Gorilla: Large Language Model Connected with Massive APIs
**Autores:** Shishir G. Patil et al.
**Institui√ß√£o:** UC Berkeley
**Ano:** 2023
**Cita√ß√µes:** ~400+

**RESUMO:**
LLM fine-tuned para chamar APIs de forma precisa. Foco em reduzir hallucination de API calls (e.g., inventar APIs que n√£o existem).

**MOTIVA√á√ÉO:**
- LLMs frequentemente "hallucinate" API calls
- Inventam nomes de APIs, par√¢metros incorretos
- Necessidade de precis√£o em API calls para aplica√ß√µes reais

**CONTRIBUI√á√ïES:**

1. **Gorilla Model:**
   - Fine-tuned em 16K+ API documentations
   - Aprende: API name, parameters, return types
   - Reduce hallucination em API calls

2. **Training Data:**
   ```
   {
     "instruction": "Find all restaurants near me open now",
     "input": "",
     "output": "yelp.search_businesses(location='user_location', categories='restaurants', open_now=True)"
   }
   ```

3. **Retrieval-Aware Training:**
   - Durante inference, retrieves relevant API docs
   - Model conditioned em API documentation
   - Reduz hallucination significativamente

**RESULTADOS:**
- API call accuracy: 94% (vs 68% for GPT-4)
- Hallucination rate: 3% (vs 22% for GPT-4)

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5) - Importante para precis√£o em API calls (Modo 3)

---

## 16. 2025_ToolACE_Agentic_Workflows.pdf

**T√≠tulo:** ToolACE: Advancing Agentic Workflows with Tool-Augmented Chain Execution
**Autores:** Research team
**Ano:** 2025
**Cita√ß√µes:** ~30+ (paper recente)

**RESUMO:**
Framework para composi√ß√£o de ferramentas em workflows agentic complexos. Extends ReAct com planning e execution hier√°rquica.

**MOTIVA√á√ÉO:**
- Tasks complexas requerem m√∫ltiplos tools em sequ√™ncia
- ReAct simples n√£o escala para tasks com 10+ steps
- Necessidade de planning e decomposition

**CONTRIBUI√á√ïES:**

1. **Hierarchical Planning:**
   ```
   High-level task ‚Üí [Plan decomposition]
                    ‚Üì
                [Subtasks: T‚ÇÅ, T‚ÇÇ, ..., T‚Çô]
                    ‚Üì
                [Execute each subtask com ReAct]
                    ‚Üì
                [Aggregate results]
                    ‚Üì
                Final answer
   ```

2. **Tool Composition:**
   - Sequential: Tool‚ÇÅ ‚Üí Tool‚ÇÇ ‚Üí Tool‚ÇÉ
   - Parallel: Tool‚ÇÅ + Tool‚ÇÇ (simultaneously)
   - Conditional: If condition ‚Üí Tool‚ÇÅ else Tool‚ÇÇ

**RESULTADOS:**
- Complex tasks (10+ steps): 72% success vs ReAct: 48%
- Planning overhead: +2 seconds
- More interpretable workflows

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5) - Mostra evolu√ß√£o de ReAct para tasks complexas

---

## 17. 2024_Multi_Agent_Collaboration_LLMs.pdf

**T√≠tulo:** Multi-Agent Collaboration with Large Language Models
**Autores:** Research team
**Ano:** 2024
**Cita√ß√µes:** ~150+

**RESUMO:**
M√∫ltiplos agentes LLM colaboram para resolver tasks complexas. Patterns: Manager-Worker, Debate, Reflection.

**MOTIVA√á√ÉO:**
- Single agent tem limita√ß√µes (conhecimento, perspectiva)
- Multi-agent permite especializa√ß√£o e debate
- Inspirado em organiza√ß√µes humanas

**CONTRIBUI√á√ïES:**

1. **Padr√µes de colabora√ß√£o:**

   **Manager-Worker:**
   ```
   Manager Agent: Decomposes task ‚Üí assigns subtasks
                         ‚Üì
   Worker Agents: Execute subtasks
                         ‚Üì
   Manager Agent: Aggregates results
   ```

   **Debate:**
   ```
   Agent 1 proposes answer
        ‚Üì
   Agent 2 critiques
        ‚Üì
   Agent 1 refines
        ‚Üì
   Repeat ‚Üí Consensus
   ```

   **Reflection:**
   ```
   Actor Agent: Executes task
        ‚Üì
   Critic Agent: Evaluates result
        ‚Üì
   Actor Agent: Improves based on feedback
   ```

**RESULTADOS:**
- Math reasoning: Multi-agent (debate) > Single agent (+15% accuracy)
- Code generation: Manager-Worker > Single agent (+22% pass rate)
- Debate converges em 3-5 rounds

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5) - Mostra que Modo 3 pode envolver m√∫ltiplos agentes

---

# üìÅ PAPERS_CONTEXT_LEARNING/ (3 PDFs)

## 18. 2023_In_Context_Learning_Survey_Dong.pdf

**T√≠tulo:** A Survey on In-Context Learning
**Autores:** Qingxiu Dong et al.
**Institui√ß√£o:** Tsinghua University
**Ano:** 2023
**Cita√ß√µes:** ~600+

**RESUMO:**
Survey abrangente sobre In-Context Learning (ICL), mecanismo fundamental que permite LLMs aprenderem de exemplos no contexto SEM atualizar par√¢metros.

**MOTIVA√á√ÉO:**
- ICL √© emergent capability de LLMs grandes
- Permite adapta√ß√£o a novas tasks sem fine-tuning
- Mas COMO e POR QUE funciona?

**CONTRIBUI√á√ïES:**

1. **Defini√ß√£o de ICL:**
   ```
   Prompt = Demonstrations + Query

   Demonstrations:
   Input‚ÇÅ: "Translate to French: Hello"
   Output‚ÇÅ: "Bonjour"

   Input‚ÇÇ: "Translate to French: Goodbye"
   Output‚ÇÇ: "Au revoir"

   Query:
   Input‚ÇÉ: "Translate to French: Thank you"
   Output‚ÇÉ: [Model generates] "Merci"
   ```

2. **Como ICL funciona (teorias):**

   **Teoria 1: Bayesian Inference**
   - LLM infere latent task de demonstrations
   - Aplica task para novo query
   - Mathematical framework: P(output | query, demos)

   **Teoria 2: Gradient Descent**
   - Forward pass simula gradient descent steps
   - Attention layers implementam "update steps"
   - Equivalente a fine-tuning impl√≠cito

   **Teoria 3: Task Recognition**
   - Demonstrations ativam parametric knowledge
   - Model "reconhece" task e aplica pattern conhecido

3. **Fatores que influenciam ICL:**

   **Number of demonstrations (k):**
   - k=0 (zero-shot): Baseline
   - k=1-5 (few-shot): Typical range
   - k=10+ (many-shot): Diminishing returns

   **Order of demonstrations:**
   - Order matters! (at√© 30% performance difference)
   - Recency bias: Later demos mais influentes

   **Quality of demonstrations:**
   - Correct labels > Incorrect labels (obviously)
   - Representative examples > Edge cases
   - Diversity de examples helps

   **Format:**
   - Consistent format crucial
   - `Input: X\nOutput: Y` works well
   - Format engineering matters

4. **ICL vs Fine-tuning:**

   | Aspect | ICL | Fine-tuning |
   |--------|-----|-------------|
   | Update params | No | Yes |
   | Data needed | ~10 examples | ~1000 examples |
   | Time | Instant | Hours/days |
   | Flexibility | High (change demos) | Low (retraining) |
   | Performance | Good | Best |

**EXEMPLOS:**

**Sentiment Analysis (3-shot):**
```
Review: "This movie was amazing!" Sentiment: Positive

Review: "Terrible experience, waste of time." Sentiment: Negative

Review: "It was okay, nothing special." Sentiment: Neutral

Review: "Absolutely loved it, best film of the year!" Sentiment: [Positive]
```

**Arithmetic (5-shot):**
```
Q: 15 + 27 = A: 42

Q: 83 - 19 = A: 64

Q: 12 √ó 8 = A: 96

Q: 100 √∑ 4 = A: 25

Q: 45 + 38 = A: 83

Q: 127 - 54 = A: [73]
```

**RESULTADOS:**

**Scaling laws:**
- ICL ability emerges em ~1B parameters
- Stronger com model size (GPT-3 175B > GPT-2 1.5B)

**Task performance:**
- Simple classification: 85-90% of fine-tuned performance
- Complex reasoning: 60-70%
- Creative tasks: 70-80%

**Best practices:**
- Use 4-8 demonstrations (sweet spot)
- Diverse examples covering task space
- Consistent formatting
- Order matters (put harder examples last)

**LIMITA√á√ïES:**
- Context length limited (max ~100K tokens)
- Computationally expensive (process demos every query)
- Performance < fine-tuning para tasks complexas
- Sensitive to demonstration quality

**CONEX√ÉO COM OS 3 MODOS:**

ICL √© o MECANISMO FUNDAMENTAL que conecta todos os 3 modos:

- **Modo 1 (Recall):** ICL ativa conhecimento param√©trico via prompting
- **Modo 2 (RAG):** ICL processa retrieved docs como demonstrations
- **Modo 3 (Interactive):** ICL permite aprender tool use via examples (ReAct prompts)

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - ICL √© connector entre os 3 modos, mecanismo fundamental

**LEITURA RECOMENDADA:**
- Se√ß√£o de definitions e mechanisms
- Best practices
- Limitations
- Tempo estimado: 2 horas

---

## 19. 2022_Rethinking_Role_Demonstrations_ICL.pdf

**T√≠tulo:** Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?
**Autores:** Sewon Min et al.
**Institui√ß√£o:** University of Washington, Meta AI
**Ano:** 2022
**Cita√ß√µes:** ~800+
**Venue:** EMNLP 2022

**RESUMO:**
Estudo emp√≠rico sobre O QUE realmente importa em demonstrations para ICL. Descoberta surpreendente: label correctness n√£o √© t√£o importante quanto se pensava.

**MOTIVA√á√ÉO:**
- ICL works, mas n√£o sabemos exatamente POR QUE
- Quais componentes de demonstrations s√£o essenciais?
- Input text? Output labels? Format?

**CONTRIBUI√á√ïES:**

1. **Experimental Design:**
   Testar sistematicamente:
   - Correct labels vs Random labels
   - Inputs only vs Inputs + Outputs
   - Format variations

2. **Key Findings:**

   **Finding 1: Label correctness menos importante que esperado**

   ```
   Setup: Sentiment classification

   Correct labels:
   Review: "Amazing!" ‚Üí Positive
   Review: "Terrible!" ‚Üí Negative
   Accuracy: 82%

   Random labels:
   Review: "Amazing!" ‚Üí Negative (wrong!)
   Review: "Terrible!" ‚Üí Positive (wrong!)
   Accuracy: 76% (!!)

   Only 6% drop despite random labels!
   ```

   **Finding 2: Input distribution √© crucial**

   ```
   Diverse inputs: 82% accuracy
   Repeated inputs: 61% accuracy
   No inputs (only labels): 45% accuracy
   ```

   **Finding 3: Format √© muito importante**

   ```
   Consistent format: 82%
   Inconsistent format: 68%
   Format matters mais que label correctness!
   ```

3. **O que realmente importa:**

   **Priorities (em ordem):**
   1. **Input distribution:** Representative examples
   2. **Format consistency:** Same structure
   3. **Label space:** Correct set de possible labels
   4. **Label correctness:** Correct input-output pairs (less critical)

**IMPLICA√á√ïES:**

**Para Modo 2 (RAG):**
- Retrieved docs devem ser DIVERSE (n√£o s√≥ relevant)
- Format consistency √© crucial
- Exact facts menos cr√≠ticos que coverage

**Para Modo 3 (Interactive):**
- Tool use examples devem cobrir diverse scenarios
- Format de API calls deve ser consistent
- Edge cases menos importantes que representative cases

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5) - Insights pr√°ticos sobre como usar ICL efetivamente

**LEITURA RECOMENDADA:**
- Experimental findings
- Implications para prompt design
- Tempo estimado: 1 hora

---

## 20. 2023_Lost_in_Middle_Context_Length.pdf

**T√≠tulo:** Lost in the Middle: How Language Models Use Long Contexts
**Autores:** Nelson F. Liu et al.
**Institui√ß√£o:** Stanford University
**Ano:** 2023
**Cita√ß√µes:** ~400+

**RESUMO:**
Estudo revelando que LLMs t√™m **U-shaped attention curve**: atendem bem ao in√≠cio e fim do contexto, mas IGNORAM informa√ß√£o no meio.

**MOTIVA√á√ÉO:**
- LLMs modernos suportam contextos longos (100K+ tokens)
- Assume-se que podem usar toda informa√ß√£o
- Mas ser√° que realmente usam?

**CONTRIBUI√á√ïES:**

1. **Experimental Setup:**

   **Multi-document QA:**
   - Place answer em diferentes posi√ß√µes do contexto
   - 10 documents, answer est√° em doc 1, 2, ..., ou 10
   - Measure: Accuracy vs position

2. **Key Finding: U-shaped curve**

   ```
   Accuracy by position:

   Position 1 (in√≠cio):  92% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   Position 2:           78% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   Position 3:           65% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   Position 4:           58% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   Position 5:           55% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (worst!)
   Position 6:           59% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   Position 7:           66% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   Position 8:           72% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   Position 9:           81% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   Position 10 (fim):    88% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

   U-shaped curve: Strong at edges, weak in middle!
   ```

3. **Implications:**

   **For RAG (Modo 2):**
   - Don't just retrieve and append docs
   - PLACEMENT matters!
   - Strategies:
     - Put most relevant docs at START or END
     - Avoid burying key info in middle
     - Compress context (remove middle docs)

   **For long context:**
   - 100K context ‚â† 100K useful tokens
   - Effective context might be ~20K (edges only)
   - Models don't truly "read" all context uniformly

4. **Why does this happen?**

   **Hypotheses:**
   - **Attention bias:** Attention patterns favor edges
   - **Recency bias:** Recent tokens more activated
   - **Primacy bias:** Early tokens set context
   - **Training distribution:** Most training data has important info at edges

**RESULTADOS POR MODELO:**

**GPT-3.5-Turbo (16K context):**
- Edge accuracy: ~85%
- Middle accuracy: ~55%
- Gap: 30%

**Claude-1 (100K context):**
- Edge accuracy: ~90%
- Middle accuracy: ~65%
- Gap: 25% (better than GPT but still significant)

**Llama 2 (4K context):**
- Edge accuracy: ~80%
- Middle accuracy: ~50%
- Gap: 30%

**Pattern holds across ALL models tested!**

**EXAMPLES:**

**Example 1: Multi-doc QA**
```
Context: [Doc1] [Doc2] [Doc3] [Doc4] [Doc5] [Doc6] [Doc7] [Doc8] [Doc9] [Doc10]

Query: "What is the capital of France?"

If answer in Doc1: 92% correct ‚úì
If answer in Doc5: 55% correct ‚úó
If answer in Doc10: 88% correct ‚úì

Same answer, different position ‚Üí different accuracy!
```

**Example 2: Needle-in-haystack**
```
Context: [1000 lines of random text]
Insert fact: "The secret code is 47298" at position X
Query: "What is the secret code?"

Position at line 50 (start): 95% recall
Position at line 500 (middle): 42% recall ‚úó‚úó‚úó
Position at line 950 (end): 91% recall
```

**PRACTICAL RECOMMENDATIONS:**

**For RAG systems (Modo 2):**
1. **Rerank** retrieved docs, put best at START
2. **Compress** middle documents (summarize or remove)
3. **Duplicate** critical info at both start AND end
4. **Limit** total context to ~20K tokens (not 100K)

**For prompt engineering:**
1. Put **instructions** at START (before context)
2. Put **query** at END (after context)
3. **Repeat** key info if necessary
4. **Avoid** long lists in middle of context

**For agent systems (Modo 3):**
1. **Memory management:** Keep recent actions at end, summarize old
2. **Tool docs:** Most relevant tools at edges
3. **Trajectory compression:** Summarize middle steps

**CONEX√ÉO COM OS 3 MODOS:**

**Modo 1 (Recall):**
- Parametric knowledge n√£o afetado (always accessible)
- But context can OVERRIDE parametric ‚Üí position matters

**Modo 2 (RAG):**
- CRITICAL implication: Retrieved doc placement matters
- Naive RAG (append all docs) suboptimal
- Need smart placement strategies

**Modo 3 (Interactive):**
- Agent memory (trajectories) accumulates in context
- Need to manage context carefully
- Compress or summarize middle steps

**LIMITA√á√ïES:**
- Study focused on QA tasks (may differ for other tasks)
- Some models improving (e.g., GPT-4 Turbo better)
- Future models may address this better

**RELEV√ÇNCIA PARA AULA:**
‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (5/5) - CRITICAL para entender limita√ß√µes pr√°ticas de Modo 2

**LEITURA RECOMENDADA:**
- U-shaped curve findings
- Practical recommendations
- Implications para RAG
- Tempo estimado: 1 hora

---

# RESUMO E RECOMENDA√á√ïES DE LEITURA

## Papers OBRIGAT√ìRIOS (Must Read):

1. **2020_RAG_Original_Lewis_Facebook.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
   - Fundamento de Modo 2 (RAG/Context)
   - Paper seminal, define arquitetura

2. **2022_ReAct_Reasoning_Acting_Google.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
   - Fundamento de Modo 3 (Interactive/Agentic)
   - Pattern ReAct usado em todos agentes

3. **2023_ROME_Locating_Editing_Factual.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
   - Fundamento de Modo 1 (Parametric)
   - Mostra ONDE conhecimento √© armazenado

4. **2024_RAG_Survey_Comprehensive.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
   - Survey completo sobre RAG
   - Estado da arte at√© 2024

5. **2023_In_Context_Learning_Survey_Dong.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
   - ICL √© mecanismo que conecta os 3 modos
   - Fundamental para entender como LLMs usam contexto

6. **2023_Lost_in_Middle_Context_Length.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
   - Limita√ß√£o cr√≠tica de contexto longo
   - Implica√ß√µes pr√°ticas para RAG

## Papers RECOMENDADOS (Highly Recommended):

7. **2025_Survey_Parametric_NonParametric_RAG.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ
   - Distin√ß√£o te√≥rica entre Modo 1 e Modo 2

8. **2024_LLM_Agents_Survey_Wang.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ
   - Survey completo sobre agentes (Modo 3)

9. **2023_Toolformer_Teach_Use_Tools_Meta.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ
   - Tool learning sem supervis√£o

10. **2024_Tool_Learning_LLMs_Survey.pdf** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ
    - Como implementar tool use na pr√°tica

## Papers OPCIONAIS (Optional Deep Dives):

11-20. Remaining papers para aprofundamento espec√≠fico

---

## CONEX√ÉO COM A TAXONOMIA DOS 3 MODOS:

### MODO 1: RECALL (Conhecimento Param√©trico)
**Papers principais:** #8 (ROME), #9 (MEMIT), #11 (Knowledge Mechanisms)

**Mensagem chave:**
- Conhecimento nos par√¢metros √© FIXED ap√≥s training
- Localizado principalmente em MLP layers
- Knowledge editing √© poss√≠vel mas limitado
- Por isso Modo 2 (RAG) √© mais pr√°tico para conhecimento din√¢mico

### MODO 2: RAG/CONTEXT (Conhecimento N√£o-Param√©trico)
**Papers principais:** #6 (RAG original), #1 (RAG survey), #2 (Parametric vs Non-parametric), #7 (Self-RAG), #20 (Lost in Middle)

**Mensagem chave:**
- RAG combina parametric (reasoning) + non-parametric (facts)
- Resolve problema de conhecimento desatualizado
- Mas tem limita√ß√µes (lat√™ncia, context position bias)
- Hybrid approach (RAG + parametric) √© SOTA

### MODO 3: INTERACTIVE/AGENTIC (Uso de Ferramentas)
**Papers principais:** #13 (ReAct), #14 (Toolformer), #3 (LLM Agents survey), #4 (Tool Learning)

**Mensagem chave:**
- LLMs v√£o al√©m de gera√ß√£o de texto para A√á√ÉO
- ReAct pattern: Thought ‚Üí Action ‚Üí Observation
- Tool learning pode ser self-supervised (Toolformer)
- Multi-agent collaboration expande capacidades

### CONECTOR: IN-CONTEXT LEARNING
**Papers principais:** #18 (ICL survey), #19 (Role of Demonstrations), #20 (Lost in Middle)

**Mensagem chave:**
- ICL √© o mecanismo que permite todos os 3 modos
- Modo 1: ICL ativa parametric via prompting
- Modo 2: ICL processa retrieved docs
- Modo 3: ICL aprende tool use via exemplos
- Mas tem limita√ß√µes (context length, position bias)

---

## ORDEM DE LEITURA RECOMENDADA:

### Semana 1: Fundamentos
1. RESUMO_3_MODOS_DE_USO.txt (este arquivo)
2. RAG Original (#6)
3. ReAct (#13)
4. ROME (#8)

### Semana 2: Surveys e Estado da Arte
5. RAG Survey (#1)
6. ICL Survey (#18)
7. LLM Agents Survey (#3)

### Semana 3: Aprofundamento
8. Parametric vs Non-parametric (#2)
9. Lost in Middle (#20)
10. Toolformer (#14)

### Semana 4: T√≥picos Avan√ßados
11-20. Papers restantes baseado em interesse

---

## PERGUNTAS-CHAVE RESPONDIDAS POR ESTA COLE√á√ÉO:

1. **Quando usar Modo 1 vs Modo 2?**
   ‚Üí Ver #2 (Parametric vs Non-parametric survey)

2. **Como implementar RAG na pr√°tica?**
   ‚Üí Ver #1 (RAG survey) e #6 (RAG original)

3. **Como construir agentes LLM?**
   ‚Üí Ver #3 (Agents survey) e #13 (ReAct)

4. **Como LLMs aprendem a usar tools?**
   ‚Üí Ver #14 (Toolformer) e #4 (Tool Learning)

5. **Onde conhecimento param√©trico √© armazenado?**
   ‚Üí Ver #8 (ROME)

6. **Por que RAG funciona?**
   ‚Üí Ver #6 (RAG original) e #18 (ICL survey)

7. **Quais s√£o limita√ß√µes de contexto longo?**
   ‚Üí Ver #20 (Lost in Middle)

8. **Como combinar os 3 modos?**
   ‚Üí Ver #2 (Parametric vs Non-parametric) e #7 (Self-RAG)

---

**FIM DO √çNDICE**

Compilado em: 02 de novembro de 2025
Vers√£o: 1.0
Total de papers: 20 PDFs
Cobertura: Parametric, Non-parametric, e Interactive knowledge uso em LLMs
