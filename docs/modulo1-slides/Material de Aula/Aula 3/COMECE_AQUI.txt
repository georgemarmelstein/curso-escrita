â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘                  ğŸ“š MATERIAL DE AULA 3 - JANELA DE CONTEXTO                   â•‘
â•‘                                                                               â•‘
â•‘                         ğŸ¯ COMECE POR AQUI! ğŸ¯                                â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

==============================================================================
                           ğŸ“‹ O QUE VOCÃŠ TEM AQUI
==============================================================================

âœ… 35 PDFs cientÃ­ficos organizados em 4 categorias:
   ğŸ“Š 4 Surveys de 2025 (visÃ£o geral atualizada)
   ğŸ”¬ 16 Papers de 2025 (pesquisas mais recentes)
   ğŸ“ˆ 5 Benchmarks (como medir contexto longo)
   ğŸ“– 10 Papers Fundamentais 2023-2024 (base teÃ³rica)

âœ… DocumentaÃ§Ã£o completa:
   ğŸ“„ INDICE_COMPLETO_PDFS.md (este arquivo detalha TUDO)
   ğŸ“ COMECE_AQUI.txt (guia rÃ¡pido que vocÃª estÃ¡ lendo)

âœ… Total: ~600 pÃ¡ginas de conteÃºdo cientÃ­fico de alta qualidade

==============================================================================
                    ğŸš€ INÃCIO RÃPIDO - 3 CENÃRIOS
==============================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CENÃRIO 1: "Tenho 2 horas e preciso dar aula amanhÃ£!" â°                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ LEIA APENAS ESTES 3 PAPERS (80 minutos):

   1. Papers_Fundamentais/2023_Lost_in_the_Middle.pdf [30 min]
      â€¢ O paper MAIS IMPORTANTE de todos
      â€¢ Descobriu fenÃ´meno "lost in the middle"
      â€¢ LEIA: IntroduÃ§Ã£o + SeÃ§Ã£o 3 (experimentos) + ConclusÃ£o
      â€¢ VocÃª PRECISA conhecer este paper

   2. Papers_Fundamentais/2024_RULER_Real_Context_Size.pdf [20 min]
      â€¢ Mostra que modelos mentem sobre suas capacidades
      â€¢ Contexto "nominal" vs. "efetivo"
      â€¢ LEIA: IntroduÃ§Ã£o + Resultados principais + Figuras
      â€¢ GrÃ¡ficos sÃ£o excelentes para slides

   3. Papers_2025/2025_SWAT_Sliding_Window_Attention_Training.pdf [30 min]
      â€¢ Exemplo tÃ©cnico de sliding window attention
      â€¢ Conceito central da aula
      â€¢ LEIA: IntroduÃ§Ã£o + SeÃ§Ã£o de mÃ©todo + Experimentos
      â€¢ Use figuras para explicar sliding window

â†’ FOLHEIE ESTE (20 minutos):

   4. Surveys_2025/2025_Survey_Comprehensive_Long_Context_LM.pdf
      â€¢ Apenas para nÃºmeros atualizados
      â€¢ Veja tabelas com janelas de contexto de modelos populares
      â€¢ Use para contextualizaÃ§Ã£o inicial da aula

â†’ PREPARE (20 minutos):
   â€¢ 1 slide explicando o que Ã© janela de contexto
   â€¢ 1 slide com grÃ¡fico em U do "lost in the middle"
   â€¢ 1 slide explicando sliding window (use figuras do SWAT)
   â€¢ 1 slide com nÃºmeros: GPT-4 (128K), Claude 3 (200K), Gemini (2M)

âœ… RESULTADO: VocÃª terÃ¡ conteÃºdo suficiente para 90 minutos de aula!


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CENÃRIO 2: "Tenho 1-2 dias para preparar bem" ğŸ“š                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DIA 1 - MANHÃƒ (3-4 horas): Fundamentos

   1. Papers_Fundamentais/2023_Lost_in_the_Middle.pdf [COMPLETO]
   2. Papers_Fundamentais/2024_RULER_Real_Context_Size.pdf [COMPLETO]
   3. Papers_Fundamentais/2023_Survey_In_Context_Learning.pdf
      â€¢ Entenda in-context learning (ICL)
      â€¢ ICL Ã© o uso primÃ¡rio de janela de contexto

DIA 1 - TARDE (3-4 horas): Estado da Arte

   4. Surveys_2025/2025_Survey_Comprehensive_Long_Context_LM.pdf
      â€¢ VisÃ£o completa do estado da arte
      â€¢ NÃºmeros atualizados de todos os modelos

   5. Papers_Fundamentais/2023_Extending_Context_RoPE.pdf
      â€¢ TÃ©cnica fundamental usada em Llama, GPT-4, etc.
      â€¢ Entenda como modelos estendem contexto

   6. Papers_Fundamentais/2024_Yarn_Efficient_Context_Extension.pdf
      â€¢ Melhorias sobre RoPE
      â€¢ Estado da arte em extensÃ£o de contexto

DIA 2 - MANHÃƒ (3-4 horas): TÃ©cnicas e AplicaÃ§Ãµes

   7. Papers_2025/2025_SWAT_Sliding_Window_Attention_Training.pdf
   8. Papers_2025/2025_Context_Degradation_Analysis.pdf
      â€¢ Como performance degrada com contexto longo

   9. Papers_2025/2025_ReAttention_Infinite_Context.pdf
      â€¢ TÃ©cnicas para contexto "infinito"

   10. Papers_Fundamentais/2023_Longformer_Long_Document_Transformer.pdf
       â€¢ Exemplo clÃ¡ssico de arquitetura para contexto longo

DIA 2 - TARDE (2-3 horas): Benchmarks e RevisÃ£o

   11. Benchmarks/2025_100_LongBench.pdf
       â€¢ Como se mede capacidades de contexto longo

   12. Benchmarks/2025_LongCodeBench_1M_Context.pdf
       â€¢ AplicaÃ§Ã£o prÃ¡tica: anÃ¡lise de cÃ³digo

   13. RevisÃ£o e preparaÃ§Ã£o de slides

âœ… RESULTADO: Aula profunda e bem fundamentada!


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CENÃRIO 3: "Quero dominar o tema completamente" ğŸ“                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SEMANA 1: Fundamentos (10 papers)
   â†’ Todos os Papers Fundamentais (pasta Papers_Fundamentais/)
   â†’ Focus: Base teÃ³rica sÃ³lida

SEMANA 2: Surveys e VisÃ£o Geral (4 papers)
   â†’ Todos os Surveys de 2025 (pasta Surveys_2025/)
   â†’ Focus: Estado da arte completo

SEMANA 3: Pesquisas Recentes (16 papers)
   â†’ Todos os Papers 2025 (pasta Papers_2025/)
   â†’ Focus: TÃ©cnicas cutting-edge
   â†’ SugestÃ£o: 3-4 papers por dia

SEMANA 4: Benchmarks e SÃ­ntese (5 papers + sÃ­ntese)
   â†’ Todos os Benchmarks (pasta Benchmarks/)
   â†’ Focus: Como medir e avaliar
   â†’ Prepare material didÃ¡tico final

âœ… RESULTADO: VocÃª serÃ¡ um expert no tema!

==============================================================================
                        ğŸ¯ ESTRUTURA SUGERIDA DA AULA
==============================================================================

Com base nos 35 papers, aqui estÃ¡ uma estrutura testada:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 1: Conceitos Fundamentais [30 min]                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â€¢ O que Ã© janela de contexto?
   â€¢ Por que existe limite de contexto?
   â€¢ EvoluÃ§Ã£o: 2K â†’ 4K â†’ 8K â†’ 128K â†’ 1M â†’ 4M tokens
   â€¢ Exemplos prÃ¡ticos: quantos tokens em diferentes textos

   ğŸ“š Papers de apoio:
      - Survey_Comprehensive_Long_Context_LM (seÃ§Ã£o 2)
      - From_128K_to_4M

   ğŸ’¡ Dica: Prepare slide com timeline da evoluÃ§Ã£o


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 2: LimitaÃ§Ãµes PrÃ¡ticas [30 min] âš ï¸                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â€¢ Lost in the Middle ğŸ¯ CONCEITO ESSENCIAL
     - LLMs tÃªm dificuldade com informaÃ§Ã£o no meio do contexto
     - Performance em formato de U
     - ImplicaÃ§Ãµes para design de prompts

   â€¢ Context Degradation
     - Performance cai com contextos mais longos
     - Nem sempre vale a pena usar contexto mÃ¡ximo

   â€¢ Gap Nominal vs. Efetivo
     - GPT-4 Turbo: 128K nominal, ~40K efetivo
     - Marketing vs. realidade

   ğŸ“š Papers de apoio:
      - 2023_Lost_in_the_Middle â­ PAPER PRINCIPAL
      - 2024_RULER â­ PAPER PRINCIPAL
      - 2025_Context_Degradation_Analysis

   ğŸ’¡ Dica: Mostre o grÃ¡fico em U do Lost in the Middle
           Ã‰ visual e memorÃ¡vel!


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 3: Sliding Window Attention [30 min] ğŸ”§                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â€¢ O que Ã© sliding window?
   â€¢ Por que usar? (eficiÃªncia O(n) vs O(nÂ²))
   â€¢ Como funciona na prÃ¡tica?
   â€¢ Trade-offs: eficiÃªncia vs. capacidade
   â€¢ Tamanho Ã³timo de janela

   ğŸ“š Papers de apoio:
      - 2025_SWAT â­ PAPER PRINCIPAL
      - 2023_Longformer (exemplo clÃ¡ssico)
      - 2025_RATTENTION (otimizaÃ§Ã£o)

   ğŸ’¡ Dica: Use animaÃ§Ã£o/diagrama mostrando janela deslizando
           Conceito visual facilita muito compreensÃ£o


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 4: In-Context Learning (ICL) [20 min] ğŸ§                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â€¢ O que Ã© ICL?
   â€¢ Como LLMs "aprendem" de exemplos no contexto
   â€¢ Zero-shot vs. Few-shot vs. Many-shot
   â€¢ Trade-offs: mais exemplos â‰  sempre melhor
   â€¢ Limitado pela janela de contexto

   ğŸ“š Papers de apoio:
      - 2025_Survey_Few_Shot_Learning_ICL
      - 2025_Survey_Understanding_ICL
      - 2023_Survey_In_Context_Learning

   ğŸ’¡ Dica: DemonstraÃ§Ã£o ao vivo com GPT-4 ou Claude
           Mostre 0-shot vs 3-shot vs 10-shot


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 5: TÃ©cnicas de ExtensÃ£o [25 min] ğŸš€                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â€¢ RoPE (Rotary Position Embedding)
     - Base usada em Llama, GPT-4, etc.
     - Como estender alÃ©m do treinamento

   â€¢ YaRN (Yet another RoPE extensioN)
     - Melhorias sobre RoPE bÃ¡sico

   â€¢ LongRoPE
     - ExtensÃ£o atÃ© 2M tokens

   â€¢ Outras tÃ©cnicas
     - Infini-attention (memÃ³ria compressiva)
     - ReAttention (contexto "infinito")

   ğŸ“š Papers de apoio:
      - 2023_Extending_Context_RoPE
      - 2024_Yarn
      - 2024_LongRoPE
      - 2024_Infini_Attention
      - 2025_ReAttention

   ğŸ’¡ Dica: Foque em intuiÃ§Ãµes, nÃ£o matemÃ¡tica pesada


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 6: AplicaÃ§Ãµes PrÃ¡ticas [15 min] ğŸ’¼                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â€¢ AnÃ¡lise de cÃ³digo (repositÃ³rios completos)
   â€¢ AnÃ¡lise de documentos longos
   â€¢ ConversaÃ§Ãµes multi-turno
   â€¢ RAG vs. contexto longo
   â€¢ Edge devices (smartphones)

   ğŸ“š Papers de apoio:
      - 2025_LongCodeBench (cÃ³digo)
      - 2025_EdgeInfinite (devices)
      - 2025_3Million_Tokens_Single_GPU (democratizaÃ§Ã£o)

   ğŸ’¡ Dica: Use exemplos concretos do dia-a-dia


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 7: Como Medir? Benchmarks [10 min] ğŸ“Š                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â€¢ Por que benchmarks especializados?
   â€¢ Principais benchmarks:
     - RULER (mede contexto efetivo)
     - LongBench (multitask)
     - 100-LongBench (abrangente)
     - LongCodeBench (cÃ³digo)

   â€¢ Como interpretar resultados

   ğŸ“š Papers de apoio:
      - 2024_RULER
      - 2025_100_LongBench
      - 2025_LongCodeBench

   ğŸ’¡ Dica: Mostre tabela comparativa de modelos


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 8: Futuro e ConclusÃµes [10 min] ğŸ”®                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â€¢ Para onde vamos?
     - 10M tokens? 100M tokens?
     - Contexto infinito serÃ¡ realidade?

   â€¢ Trade-offs fundamentais
     - Contexto vs. custo computacional
     - Contexto vs. qualidade
     - Quantidade vs. eficiÃªncia

   â€¢ RecomendaÃ§Ãµes prÃ¡ticas
     - Quando usar contexto longo?
     - Quando usar RAG?
     - Como otimizar uso do contexto

   ğŸ“š Papers de apoio:
      - 2025_Unshackling_Context_Length
      - 2025_LM2_Large_Memory_Models
      - Survey_Comprehensive (seÃ§Ã£o final)

   ğŸ’¡ Dica: Termine com provocaÃ§Ã£o/pergunta para reflexÃ£o

â±ï¸ TOTAL: ~150 minutos (2h30min) - Ajuste conforme seu tempo disponÃ­vel

==============================================================================
                       ğŸ“Š PAPERS OBRIGATÃ“RIOS POR TEMA
==============================================================================

Se vocÃª sÃ³ puder ler POUCOS papers, escolha por tema de interesse:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMA: Conceitos BÃ¡sicos                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   1. Survey_Comprehensive_Long_Context_LM â­â­â­
   2. Lost_in_the_Middle (2023) â­â­â­

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMA: LimitaÃ§Ãµes e Problemas                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   1. Lost_in_the_Middle (2023) â­â­â­
   2. RULER (2024) â­â­â­
   3. Context_Degradation_Analysis (2025) â­â­

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMA: Sliding Window                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   1. SWAT (2025) â­â­â­
   2. Longformer (2023) â­â­
   3. RATTENTION (2025) â­

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMA: In-Context Learning                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   1. Survey_Few_Shot_Learning_ICL (2025) â­â­â­
   2. Survey_Understanding_ICL (2025) â­â­
   3. Survey_In_Context_Learning (2023) â­â­

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMA: ExtensÃ£o de Contexto                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   1. Extending_Context_RoPE (2023) â­â­â­
   2. Yarn (2024) â­â­
   3. LongRoPE (2024) â­â­
   4. From_128K_to_4M (2025) â­â­

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMA: Contexto Infinito                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   1. Infini_Attention (2024) â­â­â­
   2. ReAttention (2025) â­â­
   3. LM2 (2025) â­â­

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMA: AplicaÃ§Ãµes PrÃ¡ticas                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   1. LongCodeBench (2025) â­â­
   2. 3Million_Tokens_Single_GPU (2025) â­â­
   3. EdgeInfinite (2025) â­

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TEMA: Benchmarks                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   1. RULER (2024) â­â­â­
   2. 100-LongBench (2025) â­â­
   3. LongBench (2024) â­

â­â­â­ = Leitura essencial
â­â­ = Leitura recomendada
â­ = Leitura complementar

==============================================================================
                         ğŸ’¡ DICAS PRÃTICAS PARA A AULA
==============================================================================

âœ… FAÃ‡A:

   â†’ Use MUITOS exemplos visuais
     â€¢ Diagramas de janela de contexto
     â€¢ GrÃ¡fico em U do Lost in the Middle
     â€¢ AnimaÃ§Ã£o de sliding window
     â€¢ ComparaÃ§Ã£o visual de tamanhos (2K vs 128K vs 1M)

   â†’ DemonstraÃ§Ãµes ao vivo
     â€¢ ICL com diferentes nÃºmeros de exemplos
     â€¢ Teste do Lost in the Middle (coloque info no meio)
     â€¢ Compare resposta com contexto curto vs longo

   â†’ Use nÃºmeros concretos
     â€¢ GPT-4 Turbo: 128K tokens
     â€¢ Claude 3 Opus: 200K tokens
     â€¢ Gemini 1.5 Pro: 2M tokens
     â€¢ Custo por token de contexto

   â†’ Conecte com aulas anteriores
     â€¢ Aula 1: LLMs como transformadores (contexto Ã© input)
     â€¢ Aula 2: Tokens (janela medida em tokens)

   â†’ Prepare analogias
     â€¢ Janela de contexto = RAM do computador
     â€¢ Sliding window = holofote em teatro escuro
     â€¢ Lost in middle = atenÃ§Ã£o humana em palestras longas

âŒ EVITE:

   â†’ MatemÃ¡tica pesada sem necessidade
     â€¢ Foque em intuiÃ§Ãµes, nÃ£o equaÃ§Ãµes
     â€¢ Se mostrar equaÃ§Ã£o, explique em portuguÃªs depois

   â†’ Excesso de detalhes tÃ©cnicos
     â€¢ ImplementaÃ§Ã£o especÃ­fica menos importante que conceito
     â€¢ Alunos precisam entender "o quÃª" e "por quÃª", nÃ£o "exatamente como"

   â†’ ComparaÃ§Ãµes injustas de modelos
     â€¢ Contexto nÃ£o Ã© Ãºnica mÃ©trica de qualidade
     â€¢ Mais contexto â‰  sempre melhor

   â†’ Prometer que entenderÃ£o tudo
     â€¢ Ãrea em rÃ¡pida evoluÃ§Ã£o
     â€¢ Objetivo Ã© dar fundamentos para acompanhar novidades

==============================================================================
                           ğŸ“ EXERCÃCIOS SUGERIDOS
==============================================================================

EXERCÃCIO 1: Teste do Lost in the Middle
   â€¢ Crie prompt com informaÃ§Ã£o no inÃ­cio, meio e fim
   â€¢ Teste com GPT-4/Claude
   â€¢ Veja qual informaÃ§Ã£o Ã© usada na resposta
   â€¢ DiscussÃ£o: Como mitigar isso?

EXERCÃCIO 2: Contagem de Tokens
   â€¢ DÃª textos de diferentes tamanhos
   â€¢ Alunos estimam quantos tokens
   â€¢ Verificam com tokenizer
   â€¢ DiscussÃ£o: ImplicaÃ§Ãµes prÃ¡ticas

EXERCÃCIO 3: ICL com Diferentes Shots
   â€¢ Mesma tarefa com 0, 1, 3, 5, 10 exemplos
   â€¢ Compare qualidade e custo
   â€¢ Encontre "sweet spot"
   â€¢ DiscussÃ£o: Quando vale usar many-shot?

EXERCÃCIO 4: AnÃ¡lise de Benchmark
   â€¢ DÃª resultados de benchmark (ex: RULER)
   â€¢ Alunos interpretam o que significa
   â€¢ Compare contexto nominal vs efetivo
   â€¢ DiscussÃ£o: Como escolher modelo para sua aplicaÃ§Ã£o?

EXERCÃCIO 5: RAG vs Contexto Longo
   â€¢ CenÃ¡rio: AnÃ¡lise de 50 documentos
   â€¢ Abordagem 1: Tudo no contexto
   â€¢ Abordagem 2: RAG com recuperaÃ§Ã£o
   â€¢ DiscussÃ£o: Quando usar cada abordagem?

==============================================================================
                          ğŸ“ˆ MÃ‰TRICAS E NÃšMEROS-CHAVE
==============================================================================

JANELAS DE CONTEXTO (Outubro 2025):

   GPT-3.5 Turbo.........: 16K tokens
   GPT-4 Turbo...........: 128K tokens
   Claude 3 Haiku........: 200K tokens
   Claude 3 Sonnet.......: 200K tokens
   Claude 3 Opus.........: 200K tokens
   Gemini 1.5 Flash......: 1M tokens (2M experimental)
   Gemini 1.5 Pro........: 2M tokens

   Modelos open-source:
   Llama 2...............: 4K tokens
   Llama 3...............: 8K tokens (32K com extensÃ£o)
   Mistral...............: 8K tokens (32K v2)
   Yi....................: 200K tokens

CUSTO APROXIMADO (por 1M tokens de contexto):

   GPT-4 Turbo...........: $10 input
   Claude 3 Opus.........: $15 input
   Gemini 1.5 Pro........: $3.5 input (mais barato!)

PERFORMANCE:

   Contexto efetivo tÃ­pico: 30-60% do nominal
   DegradaÃ§Ã£o tÃ­pica: 10-30% em 100K+ tokens
   Lost in middle: -40% accuracy para info no meio

==============================================================================
                      ğŸ”— RECURSOS COMPLEMENTARES ÃšTEIS
==============================================================================

FERRAMENTAS ONLINE:

   â€¢ OpenAI Tokenizer: https://platform.openai.com/tokenizer
     (Conte tokens de qualquer texto)

   â€¢ LMSYS Chatbot Arena: https://chat.lmsys.org/
     (Compare modelos lado a lado)

TUTORIAIS E VÃDEOS:

   â€¢ Andrej Karpathy - AtenÃ§Ã£o em Transformers
   â€¢ 3Blue1Brown - VisualizaÃ§Ã£o de Attention
   â€¢ Busque "sliding window attention visualization"

CÃ“DIGO OPEN-SOURCE:

   â€¢ Muitos papers incluem repositÃ³rios GitHub
   â€¢ Veja seÃ§Ãµes "Code Availability" nos PDFs
   â€¢ Ãštil para demonstraÃ§Ãµes tÃ©cnicas

ATUALIZAÃ‡Ã•ES:

   â€¢ ArXiv: https://arxiv.org/list/cs.CL/recent
     (Papers novos diariamente)

   â€¢ Twitter/X: Siga pesquisadores da Ã¡rea
     (AnÃºncios de modelos novos)

==============================================================================
                             â“ PERGUNTAS FREQUENTES
==============================================================================

P: Preciso ler todos os 35 papers?
R: NÃƒO! Use cenÃ¡rios acima. MÃ­nimo 3-5 papers, ideal 10-15.

P: Qual ordem de leitura?
R: Siga ordem dos cenÃ¡rios acima. Fundamentos primeiro, depois aplicaÃ§Ãµes.

P: Papers de 2025 sÃ£o peer-reviewed?
R: Maioria ainda em review, mas metodologia Ã© sÃ³lida e autores sÃ£o credenciados.

P: Posso pular a matemÃ¡tica?
R: SIM! Foque em intuiÃ§Ãµes, resultados e implicaÃ§Ãµes prÃ¡ticas.

P: Como citar nas referÃªncias?
R: Formato ArXiv:
   Autor et al., "TÃ­tulo", ArXiv:XXXX.XXXXX, Ano

P: Preciso demonstrar cÃ³digo?
R: NÃ£o obrigatÃ³rio, mas demonstraÃ§Ãµes prÃ¡ticas enriquecem muito a aula.

P: E se aluno perguntar algo que nÃ£o estÃ¡ nos papers?
R: OK nÃ£o saber tudo! Ãrea evolui rÃ¡pido. Anote e prometa pesquisar.

P: Papers muito tÃ©cnicos, e agora?
R: Foque em: IntroduÃ§Ã£o, Resultados principais, ConclusÃ£o, Figuras.
   Pule metodologia detalhada na primeira leitura.

==============================================================================
                              âœ… CHECKLIST FINAL
==============================================================================

Antes da aula, vocÃª deve ter:

   [ ] Lido ao menos 5 papers (3 se tempo curto)
   [ ] Preparado slides com conceitos principais
   [ ] Coletado grÃ¡ficos/figuras importantes dos papers
   [ ] Preparado ao menos 1 demonstraÃ§Ã£o prÃ¡tica
   [ ] Listado nÃºmeros atualizados de janelas de contexto
   [ ] Preparado exemplos visuais (diagramas, animaÃ§Ãµes)
   [ ] Preparado analogias para conceitos complexos
   [ ] Testado exemplos de ICL e Lost in Middle
   [ ] Preparado exercÃ­cios/quiz para alunos
   [ ] Revisado conexÃµes com aulas anteriores

==============================================================================
                            ğŸ¯ PRÃ“XIMOS PASSOS
==============================================================================

1. Escolha seu cenÃ¡rio de preparaÃ§Ã£o (1, 2 ou 3)
2. Abra INDICE_COMPLETO_PDFS.md para detalhes de cada paper
3. Comece a ler papers na ordem sugerida
4. FaÃ§a anotaÃ§Ãµes e marque figuras importantes
5. Prepare slides conforme avanÃ§a na leitura
6. Teste demonstraÃ§Ãµes prÃ¡ticas
7. Revise checklist final

==============================================================================

         ğŸ’¬ "Context is king, but understanding its limits is wisdom"
                              - Anonymous ML Researcher

==============================================================================

Ãšltima atualizaÃ§Ã£o: 31/10/2025
Para: George Marmelstein
Aula: 3 - Janela de Contexto

                              BOA AULA! ğŸš€
