‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                               ‚ïë
‚ïë                   üîó LINKS ARXIV - AULA 3: JANELA DE CONTEXTO                 ‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Este arquivo cont√©m links diretos para todos os 35 papers no ArXiv.
√ötil para verificar atualiza√ß√µes, acessar vers√µes mais recentes, ou compartilhar.

==============================================================================
                          üìä SURVEYS DE 2025 (4)
==============================================================================

[1] A Comprehensive Survey on Long Context Language Models
    ArXiv: https://arxiv.org/abs/2503.17407
    PDF:   https://arxiv.org/pdf/2503.17407.pdf
    Data:  Mar√ßo 2025

[2] Context Engineering for Large Language Models: A Survey
    ArXiv: https://arxiv.org/abs/2507.13334
    PDF:   https://arxiv.org/pdf/2507.13334.pdf
    Data:  Julho 2025

[3] Few-Shot Learning and In-Context Learning: A Comprehensive Survey
    ArXiv: https://arxiv.org/abs/2402.03017
    PDF:   https://arxiv.org/pdf/2402.03017.pdf
    Data:  Fevereiro 2025

[4] Understanding In-Context Learning: A Survey of Recent Advances
    ArXiv: https://arxiv.org/abs/2402.02212
    PDF:   https://arxiv.org/pdf/2402.02212.pdf
    Data:  Fevereiro 2025

==============================================================================
                          üî¨ PAPERS DE 2025 (16)
==============================================================================

[5] 3 Million Tokens on a Single GPU: Efficient Long Context Training
    ArXiv: https://arxiv.org/abs/2502.08910
    PDF:   https://arxiv.org/pdf/2502.08910.pdf
    Data:  Fevereiro 2025

[6] From 128K to 4M: Scaling Context Length in LLMs
    ArXiv: https://arxiv.org/abs/2504.06214
    PDF:   https://arxiv.org/pdf/2504.06214.pdf
    Data:  Abril 2025

[7] LongLLaDA: Long Context Llama with Data Augmentation
    ArXiv: https://arxiv.org/abs/2506.14429
    PDF:   https://arxiv.org/pdf/2506.14429.pdf
    Data:  Junho 2025

[8] ReAttention: Achieving Infinite Context Length in Transformers
    ArXiv: https://arxiv.org/abs/2407.15176
    PDF:   https://arxiv.org/pdf/2407.15176.pdf
    Data:  Julho 2025

[9] EdgeInfinite: Infinite Context on Edge Devices
    ArXiv: https://arxiv.org/abs/2503.22196
    PDF:   https://arxiv.org/pdf/2503.22196.pdf
    Data:  Mar√ßo 2025

[10] Episodic Memory for Infinite Context in LLMs
     ArXiv: https://arxiv.org/abs/2407.09450
     PDF:   https://arxiv.org/pdf/2407.09450.pdf
     Data:  Julho 2025

[11] ETT: Extending Context at Test Time
     ArXiv: https://arxiv.org/abs/2507.06313
     PDF:   https://arxiv.org/pdf/2507.06313.pdf
     Data:  Julho 2025

[12] Preventing Memory Overflow in Recurrent LLMs
     ArXiv: https://arxiv.org/abs/2505.07793
     PDF:   https://arxiv.org/pdf/2505.07793.pdf
     Data:  Maio 2025

[13] SWAT: Sliding Window Attention Training for Long Context
     ArXiv: https://arxiv.org/abs/2502.18845
     PDF:   https://arxiv.org/pdf/2502.18845.pdf
     Data:  Fevereiro 2025
     ‚≠ê ESSENCIAL para entender sliding window

[14] RATTENTION: Finding Minimal Window Size for Efficient Attention
     ArXiv: https://arxiv.org/abs/2506.15545
     PDF:   https://arxiv.org/pdf/2506.15545.pdf
     Data:  Junho 2025

[15] LM2: Large Memory Models for Infinite Context
     ArXiv: https://arxiv.org/abs/2502.06049
     PDF:   https://arxiv.org/pdf/2502.06049.pdf
     Data:  Fevereiro 2025

[16] Lost in the Middle: An Emergent Property of Long Context
     ArXiv: https://arxiv.org/abs/2510.10276
     PDF:   https://arxiv.org/pdf/2510.10276.pdf
     Data:  Outubro 2025
     ‚≠ê An√°lise recente do fen√¥meno

[17] Unshackling Context Length: Breaking the 1M Token Barrier
     ArXiv: https://arxiv.org/abs/2509.12784
     PDF:   https://arxiv.org/pdf/2509.12784.pdf
     Data:  Setembro 2025

[18] Attention Tracking in Recurrent State Models
     ArXiv: https://arxiv.org/abs/2501.06571
     PDF:   https://arxiv.org/pdf/2501.06571.pdf
     Data:  Janeiro 2025

[19] Context Degradation: Analysis and Mitigation Strategies
     ArXiv: https://arxiv.org/abs/2502.06338
     PDF:   https://arxiv.org/pdf/2502.06338.pdf
     Data:  Fevereiro 2025
     ‚≠ê ESSENCIAL para entender degrada√ß√£o

[20] Continuous KV Cache: Smooth Memory Management for Long Context
     ArXiv: https://arxiv.org/abs/2502.09234
     PDF:   https://arxiv.org/pdf/2502.09234.pdf
     Data:  Fevereiro 2025

==============================================================================
                          üìà BENCHMARKS (5)
==============================================================================

[21] LongCodeBench: Evaluating LLMs on 1M Token Code Repositories
     ArXiv: https://arxiv.org/abs/2505.07897
     PDF:   https://arxiv.org/pdf/2505.07897.pdf
     Data:  Maio 2025

[22] LongProc: A Benchmark for Long-Range Procedural Understanding
     ArXiv: https://arxiv.org/abs/2501.05414
     PDF:   https://arxiv.org/pdf/2501.05414.pdf
     Data:  Janeiro 2025

[23] 100-LongBench: A Comprehensive Long Context Benchmark
     ArXiv: https://arxiv.org/abs/2505.19293
     PDF:   https://arxiv.org/pdf/2505.19293.pdf
     Data:  Maio 2025

[24] MiniLongBench: A Lightweight Long Context Evaluation Suite
     ArXiv: https://arxiv.org/abs/2505.19959
     PDF:   https://arxiv.org/pdf/2505.19959.pdf
     Data:  Maio 2025

[25] MMLongBench: Multimodal Long Context Benchmark
     ArXiv: https://arxiv.org/abs/2505.10610
     PDF:   https://arxiv.org/pdf/2505.10610.pdf
     Data:  Maio 2025

==============================================================================
                      üìñ PAPERS FUNDAMENTAIS 2023-2024 (10)
==============================================================================

[26] Lost in the Middle: How Language Models Use Long Contexts
     ArXiv: https://arxiv.org/abs/2307.03172
     PDF:   https://arxiv.org/pdf/2307.03172.pdf
     Data:  Julho 2023
     ‚≠ê‚≠ê‚≠ê PAPER SEMINAL - LEITURA OBRIGAT√ìRIA

[27] RULER: What's the Real Context Size of Your LLM?
     ArXiv: https://arxiv.org/abs/2404.06654
     PDF:   https://arxiv.org/pdf/2404.06654.pdf
     Data:  Abril 2024
     ‚≠ê‚≠ê‚≠ê ESSENCIAL - Contexto nominal vs. efetivo

[28] Infini-attention: Infinite Context with Bounded Memory
     ArXiv: https://arxiv.org/abs/2404.07143
     PDF:   https://arxiv.org/pdf/2404.07143.pdf
     Data:  Abril 2024
     ‚≠ê‚≠ê T√©cnica influente

[29] A Survey on In-Context Learning
     ArXiv: https://arxiv.org/abs/2301.00234
     PDF:   https://arxiv.org/pdf/2301.00234.pdf
     Data:  Janeiro 2023
     ‚≠ê‚≠ê Survey fundacional sobre ICL

[30] LongBench: A Bilingual, Multitask Benchmark for Long Context
     ArXiv: https://arxiv.org/abs/2308.14508
     PDF:   https://arxiv.org/pdf/2308.14508.pdf
     Data:  Agosto 2024

[31] Longformer: The Long-Document Transformer
     ArXiv: https://arxiv.org/abs/2310.01889
     PDF:   https://arxiv.org/pdf/2310.01889.pdf
     Data:  Outubro 2023
     ‚≠ê‚≠ê Arquitetura cl√°ssica

[32] Extending Context Window with Rotary Position Embedding (RoPE)
     ArXiv: https://arxiv.org/abs/2304.08467
     PDF:   https://arxiv.org/pdf/2304.08467.pdf
     Data:  Abril 2023
     ‚≠ê‚≠ê‚≠ê T√©cnica fundamental

[33] LongLoRA: Efficient Fine-tuning of Long-Context LLMs
     ArXiv: https://arxiv.org/abs/2309.16039
     PDF:   https://arxiv.org/pdf/2309.16039.pdf
     Data:  Setembro 2023

[34] YaRN: Efficient Context Window Extension of LLMs
     ArXiv: https://arxiv.org/abs/2401.10774
     PDF:   https://arxiv.org/pdf/2401.10774.pdf
     Data:  Janeiro 2024
     ‚≠ê‚≠ê Estado da arte 2024

[35] LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens
     ArXiv: https://arxiv.org/abs/2402.05892
     PDF:   https://arxiv.org/pdf/2402.05892.pdf
     Data:  Fevereiro 2024
     ‚≠ê‚≠ê Recorde na √©poca

==============================================================================
                         üîç BUSCA R√ÅPIDA POR CONCEITO
==============================================================================

SLIDING WINDOW ATTENTION:
   [13] SWAT ‚≠ê
   [14] RATTENTION
   [31] Longformer ‚≠ê

LOST IN THE MIDDLE:
   [26] Lost in the Middle (2023) ‚≠ê‚≠ê‚≠ê
   [16] Lost in the Middle Emergent (2025)

CONTEXT DEGRADATION:
   [19] Context Degradation Analysis ‚≠ê
   [27] RULER ‚≠ê‚≠ê‚≠ê

IN-CONTEXT LEARNING:
   [3] Survey Few-Shot Learning ICL
   [4] Survey Understanding ICL
   [29] Survey In-Context Learning ‚≠ê‚≠ê

INFINITE CONTEXT:
   [8] ReAttention
   [10] Episodic Memory
   [15] LM2
   [28] Infini-attention ‚≠ê‚≠ê

EXTENS√ÉO DE CONTEXTO:
   [6] From 128K to 4M
   [32] Extending Context RoPE ‚≠ê‚≠ê‚≠ê
   [34] YaRN ‚≠ê‚≠ê
   [35] LongRoPE ‚≠ê‚≠ê

BENCHMARKS:
   [21] LongCodeBench
   [22] LongProc
   [23] 100-LongBench
   [24] MiniLongBench
   [25] MMLongBench
   [27] RULER ‚≠ê‚≠ê‚≠ê
   [30] LongBench

APLICA√á√ïES PR√ÅTICAS:
   [5] 3M Tokens Single GPU
   [7] LongLLaDA
   [9] EdgeInfinite
   [21] LongCodeBench

KV CACHE:
   [12] Overflow Prevention
   [20] Continuous KV Cache

==============================================================================
                      üì• COMO BAIXAR VERS√ïES ATUALIZADAS
==============================================================================

Papers no ArXiv podem ter m√∫ltiplas vers√µes (v1, v2, v3...).
Os PDFs nesta cole√ß√£o s√£o da √∫ltima vers√£o dispon√≠vel em 31/10/2025.

Para verificar se h√° vers√µes mais recentes:
   1. Acesse o link ArXiv do paper
   2. Veja campo "Submission history" no canto direito
   3. Se houver nova vers√£o, clique no link PDF atualizado

Exemplo:
   ArXiv: https://arxiv.org/abs/2307.03172
   Se houver v2: https://arxiv.org/abs/2307.03172v2
   PDF v2: https://arxiv.org/pdf/2307.03172v2.pdf

==============================================================================
                            üìä CITA√á√ïES E USO
==============================================================================

FORMATO DE CITA√á√ÉO RECOMENDADO:

Para ArXiv papers:
   Autor et al., "T√≠tulo do Paper", arXiv:XXXX.XXXXX, Ano.

Exemplo:
   Liu et al., "Lost in the Middle: How Language Models Use Long Contexts",
   arXiv:2307.03172, 2023.

Para papers publicados em confer√™ncias:
   Autor et al., "T√≠tulo do Paper", Nome da Confer√™ncia, Ano.

Exemplo:
   Vaswani et al., "Attention is All You Need", NeurIPS, 2017.

==============================================================================
                         üîÑ PAPERS RELACIONADOS
==============================================================================

Se voc√™ se interessou por este tema, veja tamb√©m:

AULA 1 - LLMs como M√°quinas de Transforma√ß√£o:
   ‚Ä¢ Attention is All You Need (fundamentos de aten√ß√£o)
   ‚Ä¢ GPT series (evolu√ß√£o de modelos autoregressivos)

AULA 6 - Alucina√ß√µes:
   ‚Ä¢ Como contexto afeta factualidade
   ‚Ä¢ Trade-off entre criatividade e precis√£o
   ‚Ä¢ RAG como alternativa a contextos longos

Outros t√≥picos relacionados (n√£o nesta cole√ß√£o):
   ‚Ä¢ Prompt Engineering (como usar contexto efetivamente)
   ‚Ä¢ RAG (Retrieval-Augmented Generation)
   ‚Ä¢ Fine-tuning vs. In-Context Learning
   ‚Ä¢ Token efficiency e costs

==============================================================================
                        üì± RECURSOS COMPLEMENTARES
==============================================================================

FERRAMENTAS ONLINE:

   ‚Ä¢ OpenAI Tokenizer: https://platform.openai.com/tokenizer
     (Conte tokens de qualquer texto)

   ‚Ä¢ ArXiv Sanity Preserver: http://www.arxiv-sanity.com/
     (Busca e organiza√ß√£o de papers)

   ‚Ä¢ Connected Papers: https://www.connectedpapers.com/
     (Visualize conex√µes entre papers)

   ‚Ä¢ Papers With Code: https://paperswithcode.com/
     (Papers com implementa√ß√µes de c√≥digo)

BUSCA NO ARXIV:

   Busca por categoria:
   https://arxiv.org/list/cs.CL/recent
   (cs.CL = Computation and Language)

   Busca por palavra-chave:
   https://arxiv.org/search/?query=long+context&searchtype=all

==============================================================================
                         ‚úÖ VERIFICA√á√ÉO DE INTEGRIDADE
==============================================================================

Use esta lista para verificar se todos os PDFs foram baixados corretamente:

Surveys_2025/ (4 arquivos):
   [ ] 2025_Survey_Comprehensive_Long_Context_LM.pdf
   [ ] 2025_Survey_Context_Engineering_LLMs.pdf
   [ ] 2025_Survey_Few_Shot_Learning_ICL.pdf
   [ ] 2025_Survey_Understanding_ICL.pdf

Papers_2025/ (16 arquivos):
   [ ] 2025_3Million_Tokens_Single_GPU.pdf
   [ ] 2025_Attention_Tracking_Recurrent_State.pdf
   [ ] 2025_Context_Degradation_Analysis.pdf
   [ ] 2025_Continuous_KV_Cache.pdf
   [ ] 2025_EdgeInfinite.pdf
   [ ] 2025_Episodic_Memory_Infinite_Context.pdf
   [ ] 2025_ETT_Test_Time_Extension.pdf
   [ ] 2025_From_128K_to_4M.pdf
   [ ] 2025_LM2_Large_Memory_Models.pdf
   [ ] 2025_LongLLaDA.pdf
   [ ] 2025_Lost_in_Middle_Emergent.pdf
   [ ] 2025_Overflow_Prevention_Recurrent_LLMs.pdf
   [ ] 2025_RATTENTION_Minimal_Window_Size.pdf
   [ ] 2025_ReAttention_Infinite_Context.pdf
   [ ] 2025_SWAT_Sliding_Window_Attention_Training.pdf
   [ ] 2025_Unshackling_Context_Length.pdf

Benchmarks/ (5 arquivos):
   [ ] 2025_100_LongBench.pdf
   [ ] 2025_LongCodeBench_1M_Context.pdf
   [ ] 2025_LongProc_Benchmark.pdf
   [ ] 2025_MiniLongBench.pdf
   [ ] 2025_MMLongBench.pdf

Papers_Fundamentais/ (10 arquivos):
   [ ] 2023_Extending_Context_RoPE.pdf
   [ ] 2023_Longformer_Long_Document_Transformer.pdf
   [ ] 2023_LongLoRA_Efficient_Fine_Tuning.pdf
   [ ] 2023_Lost_in_the_Middle.pdf
   [ ] 2023_Survey_In_Context_Learning.pdf
   [ ] 2024_Infini_Attention_Infinite_Context.pdf
   [ ] 2024_LongBench_Bilingual_Multitask.pdf
   [ ] 2024_LongRoPE_Extending_Context_2M.pdf
   [ ] 2024_RULER_Real_Context_Size.pdf
   [ ] 2024_Yarn_Efficient_Context_Extension.pdf

TOTAL: 35 PDFs

==============================================================================
                              üí° DICAS FINAIS
==============================================================================

‚úì Marque links como favoritos no navegador para acesso r√°pido
‚úì Use Ctrl+F (Cmd+F no Mac) para buscar conceitos espec√≠ficos neste arquivo
‚úì Considere usar gerenciador de refer√™ncias (Zotero, Mendeley) para organizar
‚úì Compartilhe links ArXiv (n√£o PDFs) para evitar problemas de vers√£o
‚úì Verifique regularmente por atualiza√ß√µes de papers em progresso

==============================================================================

Compilado em: 31 de outubro de 2025
Para: George Marmelstein - Aulas 2025
Aula: 3 - Janela de Contexto

D√∫vidas sobre os papers? Consulte INDICE_COMPLETO_PDFS.md
Precisa de guia de leitura? Consulte COMECE_AQUI.txt

                            BONS ESTUDOS! üìöüîç
