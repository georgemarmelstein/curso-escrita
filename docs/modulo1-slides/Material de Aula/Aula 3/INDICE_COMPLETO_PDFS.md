# üìÑ √çNDICE COMPLETO DE PDFs - AULA 3: JANELA DE CONTEXTO

**Total de PDFs:** 35 papers cient√≠ficos

**Distribui√ß√£o:**
- 4 Surveys de 2025
- 16 Papers de 2025
- 5 Benchmarks
- 10 Papers Fundamentais (2023-2024)

**Data de download:** 31 de outubro de 2025

---

## üìä RESUMO EXECUTIVO

Esta cole√ß√£o aborda **janela de contexto (context window)** em LLMs, incluindo:
- **Extens√£o de contexto:** De 2K at√© 4M tokens
- **In-Context Learning (ICL):** Aprendizado atrav√©s de exemplos no prompt
- **Sliding Window Attention:** Aten√ß√£o em janelas deslizantes
- **Context Degradation:** Perda de performance com contextos longos
- **"Lost in the Middle":** Fen√¥meno onde LLMs perdem informa√ß√£o no meio de contextos longos
- **Infinite Context:** T√©cnicas para contexto ilimitado
- **Benchmarks:** Avalia√ß√£o de capacidades de contexto longo

---

## üìö SURVEYS DE 2025

### 1. 2025_Survey_Comprehensive_Long_Context_LM.pdf
**T√≠tulo:** A Comprehensive Survey on Long Context Language Models

**ArXiv:** 2503.17407

**Data:** Mar√ßo 2025

**Descri√ß√£o:**
Survey abrangente sobre modelos de linguagem com contexto longo, cobrindo arquiteturas, t√©cnicas de extens√£o de contexto, e aplica√ß√µes pr√°ticas.

**T√≥picos principais:**
- Evolu√ß√£o de 2K para 1M+ tokens
- T√©cnicas de extens√£o: RoPE, ALiBi, YaRN
- Arquiteturas eficientes: Sliding window, sparse attention
- Desafios de treinamento e infer√™ncia
- Aplica√ß√µes em documentos longos, c√≥digo, conversa√ß√£o

**Relev√¢ncia para Aula 3:**
Survey essencial que fornece vis√£o completa do estado da arte em contextos longos. Ideal para introdu√ß√£o do tema.

---

### 2. 2025_Survey_Context_Engineering_LLMs.pdf
**T√≠tulo:** Context Engineering for Large Language Models: A Survey

**ArXiv:** 2507.13334

**Data:** Julho 2025

**Descri√ß√£o:**
Survey sobre t√©cnicas de engenharia de contexto para LLMs, incluindo estrat√©gias para otimizar o uso da janela de contexto.

**T√≥picos principais:**
- Prompt engineering avan√ßado
- Organiza√ß√£o √≥tima de informa√ß√£o no contexto
- Context compression techniques
- Retrieval strategies
- Context pruning and summarization

**Relev√¢ncia para Aula 3:**
Demonstra como usar efetivamente a janela de contexto dispon√≠vel, complementando discuss√£o sobre limita√ß√µes t√©cnicas.

---

### 3. 2025_Survey_Few_Shot_Learning_ICL.pdf
**T√≠tulo:** Few-Shot Learning and In-Context Learning: A Comprehensive Survey

**ArXiv:** 2402.03017

**Data:** Fevereiro 2025 (atualizado)

**Descri√ß√£o:**
Survey detalhado sobre few-shot learning e in-context learning, explorando como LLMs aprendem de exemplos fornecidos no contexto.

**T√≥picos principais:**
- Fundamentos de ICL
- Mecanismos de aten√ß√£o em ICL
- N√∫mero √≥timo de exemplos (shots)
- Ordem e sele√ß√£o de exemplos
- Limita√ß√µes e trade-offs

**Relev√¢ncia para Aula 3:**
ICL √© aplica√ß√£o direta da janela de contexto. Mostra como contexto √© usado para "aprendizado" sem gradiente.

---

### 4. 2025_Survey_Understanding_ICL.pdf
**T√≠tulo:** Understanding In-Context Learning: A Survey of Recent Advances

**ArXiv:** 2402.02212

**Data:** Fevereiro 2025 (atualizado)

**Descri√ß√£o:**
Survey focado em entender os mecanismos subjacentes de in-context learning, incluindo teorias sobre como e por que funciona.

**T√≥picos principais:**
- Teorias sobre mecanismos de ICL
- Papel de attention heads em ICL
- Diferen√ßas entre ICL e fine-tuning
- Fatores que afetam performance de ICL
- Limites te√≥ricos de ICL

**Relev√¢ncia para Aula 3:**
Aprofunda entendimento te√≥rico de como LLMs usam informa√ß√£o dentro da janela de contexto.

---

## üî¨ PAPERS DE 2025

### 5. 2025_3Million_Tokens_Single_GPU.pdf
**T√≠tulo:** 3 Million Tokens on a Single GPU: Efficient Long Context Training

**ArXiv:** 2502.08910

**Data:** Fevereiro 2025

**Descri√ß√£o:**
Demonstra t√©cnicas para treinar LLMs com contexto de 3 milh√µes de tokens em uma √∫nica GPU, tornando contextos ultra-longos mais acess√≠veis.

**Contribui√ß√µes principais:**
- Memory-efficient attention implementation
- Gradient checkpointing estrat√©gico
- Compression de KV cache
- Permite pesquisa em contextos longos sem hardware extremo

**Relev√¢ncia para Aula 3:**
Mostra que contextos ultra-longos est√£o se tornando pr√°ticos, n√£o apenas te√≥ricos.

---

### 6. 2025_From_128K_to_4M.pdf
**T√≠tulo:** From 128K to 4M: Scaling Context Length in LLMs

**ArXiv:** 2504.06214

**Data:** Abril 2025

**Descri√ß√£o:**
Analisa progress√£o de 128K tokens (GPT-4 Turbo) at√© 4M tokens, explorando t√©cnicas e trade-offs em cada escala.

**Contribui√ß√µes principais:**
- T√©cnicas espec√≠ficas para cada ordem de magnitude
- An√°lise de performance vs. custo computacional
- Identifica√ß√£o de "sweet spots" para diferentes aplica√ß√µes
- Medi√ß√£o de context degradation em diferentes escalas

**Relev√¢ncia para Aula 3:**
Fornece roadmap claro da evolu√ß√£o de janelas de contexto e seus desafios pr√°ticos.

---

### 7. 2025_LongLLaDA.pdf
**T√≠tulo:** LongLLaDA: Long Context Llama with Data Augmentation

**ArXiv:** 2506.14429

**Data:** Junho 2025

**Descri√ß√£o:**
Prop√µe m√©todo de data augmentation espec√≠fico para treinar modelos com contextos longos, melhorando capacidade de processar documentos extensos.

**Contribui√ß√µes principais:**
- Synthetic long document generation
- Augmentation strategies para contextos longos
- Fine-tuning eficiente para contexto estendido
- Benchmarks em documentos reais

**Relev√¢ncia para Aula 3:**
Mostra que dados de treinamento adequados s√£o cruciais para contextos longos.

---

### 8. 2025_ReAttention_Infinite_Context.pdf
**T√≠tulo:** ReAttention: Achieving Infinite Context Length in Transformers

**ArXiv:** 2407.15176

**Data:** Julho 2025

**Descri√ß√£o:**
Prop√µe mecanismo de "reattention" que permite contextos efetivamente ilimitados atrav√©s de reprocessamento seletivo de informa√ß√£o.

**Contribui√ß√µes principais:**
- Aten√ß√£o recursiva sobre contexto
- Selective memory mechanism
- O(1) memory complexity para contexto infinito
- Performance compar√°vel a aten√ß√£o completa

**Relev√¢ncia para Aula 3:**
Representa fronteira de pesquisa: transcender limita√ß√µes de janela de contexto.

---

### 9. 2025_EdgeInfinite.pdf
**T√≠tulo:** EdgeInfinite: Infinite Context on Edge Devices

**ArXiv:** 2503.22196

**Data:** Mar√ßo 2025

**Descri√ß√£o:**
Adapta t√©cnicas de contexto infinito para dispositivos edge com mem√≥ria limitada, democratizando acesso a contextos longos.

**Contribui√ß√µes principais:**
- Compression extrema de KV cache
- Hierarquical memory management
- Trade-offs entre qualidade e mem√≥ria
- Implementa√ß√£o em smartphones e IoT

**Relev√¢ncia para Aula 3:**
Mostra que contextos longos n√£o s√£o apenas para servidores massivos.

---

### 10. 2025_Episodic_Memory_Infinite_Context.pdf
**T√≠tulo:** Episodic Memory for Infinite Context in LLMs

**ArXiv:** 2407.09450

**Data:** Julho 2025

**Descri√ß√£o:**
Prop√µe sistema de mem√≥ria epis√≥dica inspirado em cogni√ß√£o humana para permitir contextos infinitos atrav√©s de recupera√ß√£o seletiva.

**Contribui√ß√µes principais:**
- Memory indexing por epis√≥dios
- Retrieval din√¢mico durante gera√ß√£o
- Integra√ß√£o com attention mechanism
- Compara√ß√£o com abordagens RAG

**Relev√¢ncia para Aula 3:**
Conecta janela de contexto com mem√≥ria de longo prazo e sistemas de recupera√ß√£o.

---

### 11. 2025_ETT_Test_Time_Extension.pdf
**T√≠tulo:** ETT: Extending Context at Test Time

**ArXiv:** 2507.06313

**Data:** Julho 2025

**Descri√ß√£o:**
Permite extens√£o de contexto durante infer√™ncia sem re-treinamento, adaptando-se dinamicamente a necessidades de contexto longo.

**Contribui√ß√µes principais:**
- Zero-shot context extension
- Dynamic position encoding
- Attention adaptation em test-time
- Sem degrada√ß√£o para contextos curtos

**Relev√¢ncia para Aula 3:**
T√©cnica pr√°tica para usar modelos existentes com contextos mais longos que o treinamento.

---

### 12. 2025_Overflow_Prevention_Recurrent_LLMs.pdf
**T√≠tulo:** Preventing Memory Overflow in Recurrent LLMs

**ArXiv:** 2505.07793

**Data:** Maio 2025

**Descri√ß√£o:**
Aborda problema de overflow num√©rico em modelos recorrentes com contextos longos, propondo t√©cnicas de estabiliza√ß√£o.

**Contribui√ß√µes principais:**
- Normaliza√ß√£o adaptativa
- Gradient clipping strategies
- Memory decay mechanisms
- Benchmarks de estabilidade

**Relev√¢ncia para Aula 3:**
Detalha desafios pr√°ticos de implementa√ß√£o com contextos longos.

---

### 13. 2025_SWAT_Sliding_Window_Attention_Training.pdf
**T√≠tulo:** SWAT: Sliding Window Attention Training for Long Context

**ArXiv:** 2502.18845

**Data:** Fevereiro 2025

**Descri√ß√£o:**
Prop√µe treinamento espec√≠fico usando sliding window attention para melhorar efici√™ncia com contextos longos.

**Contribui√ß√µes principais:**
- Optimal window size selection
- Overlapping windows strategies
- Training curriculum para contextos crescentes
- Compara√ß√£o com global attention

**Relev√¢ncia para Aula 3:**
**ESSENCIAL** - Sliding window attention √© conceito central da aula. Este paper detalha implementa√ß√£o pr√°tica.

---

### 14. 2025_RATTENTION_Minimal_Window_Size.pdf
**T√≠tulo:** RATTENTION: Finding Minimal Window Size for Efficient Attention

**ArXiv:** 2506.15545

**Data:** Junho 2025

**Descri√ß√£o:**
Investiga janelas m√≠nimas necess√°rias para diferentes tarefas, otimizando trade-off entre performance e efici√™ncia.

**Contribui√ß√µes principais:**
- Task-specific window sizing
- Dynamic window adaptation
- Theoretical bounds em window size
- Benchmarks por tipo de tarefa

**Relev√¢ncia para Aula 3:**
Complementa discuss√£o sobre sliding windows: n√£o apenas "como" mas "quanto".

---

### 15. 2025_LM2_Large_Memory_Models.pdf
**T√≠tulo:** LM2: Large Memory Models for Infinite Context

**ArXiv:** 2502.06049

**Data:** Fevereiro 2025

**Descri√ß√£o:**
Prop√µe nova classe de modelos (LM2) que separam explicitamente "working memory" (janela de contexto) de "long-term memory".

**Contribui√ß√µes principais:**
- Arquitetura dual-memory
- Mecanismos de consolida√ß√£o de mem√≥ria
- Retrieval integrado durante gera√ß√£o
- Escala para milh√µes de tokens de mem√≥ria

**Relev√¢ncia para Aula 3:**
Paradigma alternativo que transcende limita√ß√µes de janela de contexto fixa.

---

### 16. 2025_Lost_in_Middle_Emergent.pdf
**T√≠tulo:** Lost in the Middle: An Emergent Property of Long Context

**ArXiv:** 2510.10276

**Data:** Outubro 2025

**Descri√ß√£o:**
Analisa fen√¥meno "lost in the middle" como propriedade emergente de contextos longos, n√£o apenas artefato de treinamento.

**Contribui√ß√µes principais:**
- Explica√ß√£o te√≥rica do fen√¥meno
- Rela√ß√£o com attention patterns
- T√©cnicas de mitiga√ß√£o
- Implica√ß√µes para design de prompts

**Relev√¢ncia para Aula 3:**
**ESSENCIAL** - "Lost in the middle" √© conceito-chave da aula sobre limita√ß√µes de contexto.

---

### 17. 2025_Unshackling_Context_Length.pdf
**T√≠tulo:** Unshackling Context Length: Breaking the 1M Token Barrier

**ArXiv:** 2509.12784

**Data:** Setembro 2025

**Descri√ß√£o:**
Apresenta t√©cnicas para superar barreira de 1M tokens de contexto, explorando limites fundamentais.

**Contribui√ß√µes principais:**
- Memory-efficient implementations
- Hierarchical processing strategies
- Benchmarks com contextos ultra-longos
- An√°lise de scaling laws

**Relev√¢ncia para Aula 3:**
Mostra fronteira atual de pesquisa em extens√£o de contexto.

---

### 18. 2025_Attention_Tracking_Recurrent_State.pdf
**T√≠tulo:** Attention Tracking in Recurrent State Models

**ArXiv:** 2501.06571

**Data:** Janeiro 2025

**Descri√ß√£o:**
Combina aten√ß√£o com estados recorrentes para melhor gerenciamento de contextos longos.

**Contribui√ß√µes principais:**
- Hybrid attention-recurrence architecture
- State compression mechanisms
- Selective attention over history
- Compara√ß√£o com Transformers puros

**Relev√¢ncia para Aula 3:**
Arquiteturas h√≠bridas podem superar Transformers para contextos muito longos.

---

### 19. 2025_Context_Degradation_Analysis.pdf
**T√≠tulo:** Context Degradation: Analysis and Mitigation Strategies

**ArXiv:** 2502.06338

**Data:** Fevereiro 2025

**Descri√ß√£o:**
An√°lise sistem√°tica de degrada√ß√£o de performance com aumento de contexto, propondo estrat√©gias de mitiga√ß√£o.

**Contribui√ß√µes principais:**
- Caracteriza√ß√£o quantitativa de degrada√ß√£o
- Fatores que aceleram degrada√ß√£o
- T√©cnicas de mitiga√ß√£o (reordering, compression)
- Benchmarks em diferentes escalas

**Relev√¢ncia para Aula 3:**
**ESSENCIAL** - Context degradation √© limita√ß√£o pr√°tica crucial de janelas de contexto.

---

### 20. 2025_Continuous_KV_Cache.pdf
**T√≠tulo:** Continuous KV Cache: Smooth Memory Management for Long Context

**ArXiv:** 2502.09234

**Data:** Fevereiro 2025

**Descri√ß√£o:**
Prop√µe representa√ß√£o cont√≠nua de KV cache que permite compress√£o suave e eficiente para contextos longos.

**Contribui√ß√µes principais:**
- Continuous memory representations
- Lossy compression com degrada√ß√£o controlada
- Dynamic cache allocation
- Trade-offs expl√≠citos mem√≥ria vs. qualidade

**Relev√¢ncia para Aula 3:**
KV cache √© aspecto t√©cnico central de como janela de contexto √© implementada.

---

## üìä BENCHMARKS DE 2025

### 21. 2025_LongCodeBench_1M_Context.pdf
**T√≠tulo:** LongCodeBench: Evaluating LLMs on 1M Token Code Repositories

**ArXiv:** 2505.07897

**Data:** Maio 2025

**Descri√ß√£o:**
Benchmark espec√≠fico para c√≥digo com contextos de at√© 1M tokens, testando capacidade de LLMs em reposit√≥rios completos.

**Tarefas avaliadas:**
- Code completion em arquivos distantes
- Bug finding em codebase
- Documentation generation
- Refactoring suggestions

**Modelos avaliados:**
GPT-4 Turbo, Claude 3 Opus, Gemini 1.5 Pro, modelos open-source

**Relev√¢ncia para Aula 3:**
Benchmark pr√°tico e relevante para desenvolvedores. Demonstra aplica√ß√µes reais de contextos longos.

---

### 22. 2025_LongProc_Benchmark.pdf
**T√≠tulo:** LongProc: A Benchmark for Long-Range Procedural Understanding

**ArXiv:** 2501.05414

**Data:** Janeiro 2025

**Descri√ß√£o:**
Avalia capacidade de seguir instru√ß√µes procedurais complexas que requerem rastreamento de estado ao longo de contextos longos.

**Tarefas avaliadas:**
- Multi-step reasoning
- State tracking
- Instruction following
- Dependency resolution

**Relev√¢ncia para Aula 3:**
Testa n√£o apenas capacidade de "lembrar" mas de "processar" informa√ß√£o distribu√≠da em contexto longo.

---

### 23. 2025_100_LongBench.pdf
**T√≠tulo:** 100-LongBench: A Comprehensive Long Context Benchmark

**ArXiv:** 2505.19293

**Data:** Maio 2025

**Descri√ß√£o:**
Benchmark com 100 tarefas diversas para avaliar capacidades de contexto longo em m√∫ltiplas dimens√µes.

**Categorias de tarefas:**
- Question Answering
- Summarization
- Code understanding
- Multi-document reasoning
- Fact retrieval

**Relev√¢ncia para Aula 3:**
Benchmark mais abrangente dispon√≠vel. √ötil para comparar modelos.

---

### 24. 2025_MiniLongBench.pdf
**T√≠tulo:** MiniLongBench: A Lightweight Long Context Evaluation Suite

**ArXiv:** 2505.19959

**Data:** Maio 2025

**Descri√ß√£o:**
Vers√£o compacta de benchmarks de contexto longo, permitindo avalia√ß√£o r√°pida sem recursos computacionais massivos.

**Caracter√≠sticas:**
- 10 tarefas representativas
- Execu√ß√£o em minutos vs. horas
- Alta correla√ß√£o com benchmarks completos
- Ideal para itera√ß√£o r√°pida

**Relev√¢ncia para Aula 3:**
Permite demonstra√ß√µes pr√°ticas em aula sem infraestrutura pesada.

---

### 25. 2025_MMLongBench.pdf
**T√≠tulo:** MMLongBench: Multimodal Long Context Benchmark

**ArXiv:** 2505.10610

**Data:** Maio 2025

**Descri√ß√£o:**
Primeiro benchmark focado em contextos longos multimodais (texto + imagens/v√≠deo).

**Tarefas avaliadas:**
- Long video understanding
- Multi-page document analysis
- Temporal reasoning em sequ√™ncias longas
- Cross-modal retrieval

**Relev√¢ncia para Aula 3:**
Estende discuss√£o de contexto al√©m de texto puro. Futuro dos LLMs √© multimodal.

---

## üìñ PAPERS FUNDAMENTAIS (2023-2024)

### 26. 2023_Lost_in_the_Middle.pdf
**T√≠tulo:** Lost in the Middle: How Language Models Use Long Contexts

**ArXiv:** 2307.03172

**Data:** Julho 2023

**Descri√ß√£o:**
**PAPER SEMINAL** que descobriu e nomeou o fen√¥meno "lost in the middle" - LLMs t√™m dificuldade em usar informa√ß√£o no meio de contextos longos.

**Descobertas principais:**
- Performance em U: alta no in√≠cio e fim, baixa no meio
- Fen√¥meno consistente em m√∫ltiplos modelos
- Persiste mesmo com fine-tuning
- Implica√ß√µes para design de prompts

**Relev√¢ncia para Aula 3:**
**ESSENCIAL** - Este √© O paper que definiu problema central de contextos longos. Leitura obrigat√≥ria.

---

### 27. 2024_RULER_Real_Context_Size.pdf
**T√≠tulo:** RULER: What's the Real Context Size of Your LLM?

**ArXiv:** 2404.06654

**Data:** Abril 2024

**Descri√ß√£o:**
Prop√µe metodologia para medir contexto "efetivo" vs. contexto "nominal" de LLMs. Muitos modelos alegam suportar contextos que n√£o conseguem usar efetivamente.

**Contribui√ß√µes principais:**
- Synthetic tasks para medir contexto real
- Descoberta: contexto efetivo << contexto nominal
- Metodologia reproduz√≠vel
- Benchmarks de modelos populares

**Relev√¢ncia para Aula 3:**
**ESSENCIAL** - Exp√µe gap entre marketing e realidade em janelas de contexto. Fundamental para discuss√£o cr√≠tica.

---

### 28. 2024_Infini_Attention_Infinite_Context.pdf
**T√≠tulo:** Infini-attention: Infinite Context with Bounded Memory

**ArXiv:** 2404.07143

**Data:** Abril 2024

**Descri√ß√£o:**
Prop√µe mecanismo de aten√ß√£o com mem√≥ria compressiva que permite contextos teoricamente infinitos com mem√≥ria limitada.

**Contribui√ß√µes principais:**
- Compressive memory mechanism
- Integra√ß√£o com Transformers existentes
- Memory bounded a O(1)
- Performance competitiva em contextos longos

**Relev√¢ncia para Aula 3:**
T√©cnica influente que inspirou muitos trabalhos posteriores sobre contexto infinito.

---

### 29. 2023_Survey_In_Context_Learning.pdf
**T√≠tulo:** A Survey on In-Context Learning

**ArXiv:** 2301.00234

**Data:** Janeiro 2023

**Descri√ß√£o:**
Survey fundacional sobre In-Context Learning, estabelecendo terminologia e taxonomia da √°rea.

**T√≥picos cobertos:**
- Defini√ß√£o formal de ICL
- Compara√ß√£o com fine-tuning
- Mecanismos te√≥ricos
- Aplica√ß√µes pr√°ticas
- Limita√ß√µes e desafios

**Relev√¢ncia para Aula 3:**
ICL √© uso prim√°rio de janela de contexto. Survey fundamental para entender o conceito.

---

### 30. 2024_LongBench_Bilingual_Multitask.pdf
**T√≠tulo:** LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding

**ArXiv:** 2308.14508

**Data:** Agosto 2024 (v2)

**Descri√ß√£o:**
Benchmark influente que estabeleceu padr√µes para avalia√ß√£o de contextos longos, cobrindo ingl√™s e chin√™s.

**Caracter√≠sticas:**
- 21 datasets
- Contextos de 4K-32K tokens
- Tarefas diversas
- Resultados de 15+ modelos

**Relev√¢ncia para Aula 3:**
Benchmark amplamente usado em papers. Conhec√™-lo permite interpretar resultados na literatura.

---

### 31. 2023_Longformer_Long_Document_Transformer.pdf
**T√≠tulo:** Longformer: The Long-Document Transformer

**ArXiv:** 2310.01889

**Data:** Outubro 2023 (updated)

**Descri√ß√£o:**
Arquitetura Transformer adaptada para documentos longos usando attention pattern h√≠brida (local + global).

**Contribui√ß√µes principais:**
- Sliding window attention + global attention
- Scaling para 4K-16K tokens
- Efici√™ncia O(n) vs. O(n¬≤) do Transformer
- Aplica√ß√µes em documentos cient√≠ficos

**Relev√¢ncia para Aula 3:**
Arquitetura cl√°ssica que influenciou muitos trabalhos posteriores. Exemplo concreto de sliding window.

---

### 32. 2023_Extending_Context_RoPE.pdf
**T√≠tulo:** Extending Context Window with Rotary Position Embedding (RoPE)

**ArXiv:** 2304.08467

**Data:** Abril 2023

**Descri√ß√£o:**
Demonstra como RoPE (Rotary Position Embedding) pode ser estendido para contextos mais longos que o treinamento.

**Contribui√ß√µes principais:**
- Position interpolation technique
- Zero-shot extension de 2K para 32K
- Fine-tuning eficiente para contextos longos
- An√°lise te√≥rica de RoPE

**Relev√¢ncia para Aula 3:**
T√©cnica fundamental usada em muitos modelos modernos (Llama, GPT-4, etc.) para extens√£o de contexto.

---

### 33. 2023_LongLoRA_Efficient_Fine_Tuning.pdf
**T√≠tulo:** LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models

**ArXiv:** 2309.16039

**Data:** Setembro 2023

**Descri√ß√£o:**
Combina LoRA (Low-Rank Adaptation) com t√©cnicas para contextos longos, permitindo fine-tuning eficiente.

**Contribui√ß√µes principais:**
- Shifted sparse attention durante treinamento
- Parameter-efficient training para contextos longos
- Modelos fine-tuned at√© 100K tokens
- C√≥digo open-source

**Relev√¢ncia para Aula 3:**
Demonstra que extens√£o de contexto n√£o requer sempre treinamento do zero. Fine-tuning pode ser suficiente.

---

### 34. 2024_Yarn_Efficient_Context_Extension.pdf
**T√≠tulo:** YaRN: Efficient Context Window Extension of Large Language Models

**ArXiv:** 2401.10774

**Data:** Janeiro 2024

**Descri√ß√£o:**
Prop√µe YaRN (Yet another RoPE extensioN method), melhorando t√©cnicas de interpola√ß√£o para extens√£o de contexto.

**Contribui√ß√µes principais:**
- NTK-aware interpolation
- Menor perplexidade que m√©todos anteriores
- Extens√£o at√© 128K tokens
- Implementa√ß√£o eficiente

**Relev√¢ncia para Aula 3:**
T√©cnica estado-da-arte (2024) para extens√£o de contexto. Usada em v√°rios modelos open-source.

---

### 35. 2024_LongRoPE_Extending_Context_2M.pdf
**T√≠tulo:** LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

**ArXiv:** 2402.05892

**Data:** Fevereiro 2024

**Descri√ß√£o:**
Estende RoPE para contextos extremamente longos (2M tokens), estabelecendo novo recorde na √©poca.

**Contribui√ß√µes principais:**
- Non-uniform interpolation
- Progressive extension strategy
- Benchmarks com 2M tokens
- Analysis de trade-offs

**Relev√¢ncia para Aula 3:**
Demonstra que barreiras de contexto continuam sendo ultrapassadas. Importante para discuss√£o sobre futuro.

---

## üìñ COMO USAR ESTA COLE√á√ÉO

### Para Prepara√ß√£o R√°pida da Aula (2-3 horas):

**Prioridade 1 - Conceitos Essenciais:**
1. **2023_Lost_in_the_Middle.pdf** (introdu√ß√£o + se√ß√£o 3-4) - 30 min
2. **2024_RULER_Real_Context_Size.pdf** (introdu√ß√£o + resultados) - 20 min
3. **2025_Survey_Comprehensive_Long_Context_LM.pdf** (skim se√ß√µes principais) - 40 min

**Prioridade 2 - Exemplos T√©cnicos:**
4. **2025_SWAT_Sliding_Window_Attention_Training.pdf** (para explicar sliding window) - 30 min
5. **2025_Context_Degradation_Analysis.pdf** (para explicar degrada√ß√£o) - 30 min

**Prioridade 3 - Benchmarks:**
6. **2025_100_LongBench.pdf** (skim para entender m√©tricas) - 20 min

---

### Para Prepara√ß√£o Completa (1-2 dias):

**Dia 1 - Fundamentos:**
- Manh√£:
  - Lost in the Middle (completo)
  - RULER (completo)
  - Survey In-Context Learning
- Tarde:
  - Survey Comprehensive Long Context LM
  - Extending Context with RoPE

**Dia 2 - Estado da Arte e Aplica√ß√µes:**
- Manh√£:
  - Papers 2025 selecionados (SWAT, Context Degradation, ReAttention)
  - LongLoRA e YaRN
- Tarde:
  - Benchmarks (LongBench, LongCodeBench, 100-LongBench)
  - Papers sobre infinite context (Infini-attention, LM2)

---

## üéØ PAPERS POR CONCEITO-CHAVE DA AULA

### Para explicar "Janela de Contexto" (conceito b√°sico):
- ‚úÖ **2025_Survey_Comprehensive_Long_Context_LM.pdf** (se√ß√£o 2)
- ‚úÖ **2025_From_128K_to_4M.pdf** (evolu√ß√£o hist√≥rica)

### Para explicar "Sliding Window Attention":
- ‚úÖ **2025_SWAT_Sliding_Window_Attention_Training.pdf** (ESSENCIAL)
- ‚úÖ **2023_Longformer_Long_Document_Transformer.pdf** (exemplo cl√°ssico)
- ‚úÖ **2025_RATTENTION_Minimal_Window_Size.pdf** (otimiza√ß√£o)

### Para explicar "In-Context Learning":
- ‚úÖ **2025_Survey_Few_Shot_Learning_ICL.pdf** (overview)
- ‚úÖ **2025_Survey_Understanding_ICL.pdf** (mecanismos)
- ‚úÖ **2023_Survey_In_Context_Learning.pdf** (fundamentos)

### Para explicar "Lost in the Middle":
- ‚úÖ **2023_Lost_in_the_Middle.pdf** (PAPER ORIGINAL - ESSENCIAL)
- ‚úÖ **2025_Lost_in_Middle_Emergent.pdf** (an√°lise recente)

### Para explicar "Context Degradation":
- ‚úÖ **2025_Context_Degradation_Analysis.pdf** (ESSENCIAL)
- ‚úÖ **2024_RULER_Real_Context_Size.pdf** (medi√ß√£o de degrada√ß√£o)

### Para explicar "Infinite Context":
- ‚úÖ **2025_ReAttention_Infinite_Context.pdf** (t√©cnica recente)
- ‚úÖ **2024_Infini_Attention_Infinite_Context.pdf** (t√©cnica fundacional)
- ‚úÖ **2025_LM2_Large_Memory_Models.pdf** (paradigma alternativo)

### Para explicar "Extens√£o de Contexto":
- ‚úÖ **2023_Extending_Context_RoPE.pdf** (fundamentos)
- ‚úÖ **2024_Yarn_Efficient_Context_Extension.pdf** (melhorias)
- ‚úÖ **2024_LongRoPE_Extending_Context_2M.pdf** (estado da arte)

### Para demonstrar Aplica√ß√µes Pr√°ticas:
- ‚úÖ **2025_LongCodeBench_1M_Context.pdf** (c√≥digo)
- ‚úÖ **2025_EdgeInfinite.pdf** (dispositivos edge)
- ‚úÖ **2025_3Million_Tokens_Single_GPU.pdf** (efici√™ncia)

---

## üìä ESTAT√çSTICAS DA COLE√á√ÉO

**Total de p√°ginas:** ~600+ p√°ginas de conte√∫do cient√≠fico

**Distribui√ß√£o temporal:**
- 2023: 5 papers (fundamentos)
- 2024: 5 papers (evolu√ß√£o)
- 2025: 25 papers (estado da arte)

**Distribui√ß√£o por tipo:**
- Surveys: 7 papers (20%)
- T√©cnicas de extens√£o: 10 papers (29%)
- Infinite context: 5 papers (14%)
- Benchmarks: 5 papers (14%)
- Aplica√ß√µes: 8 papers (23%)

**Escalas de contexto cobertas:**
- 2K-8K tokens (baseline)
- 32K-128K tokens (modelos atuais)
- 256K-1M tokens (contextos longos)
- 1M-4M tokens (contextos ultra-longos)
- Infinite context (sem limite fixo)

**Modelos/Arquiteturas mencionados:**
- GPT-4 Turbo (128K)
- Claude 3 (200K)
- Gemini 1.5 Pro (1M-2M)
- Llama 2/3
- Longformer
- Reformer
- E muitos modelos open-source

---

## üéì ESTRUTURA SUGERIDA PARA A AULA

Com base nesta cole√ß√£o, aqui est√° uma estrutura sugerida para a Aula 3:

### Parte 1: Conceitos Fundamentais (30 min)
- O que √© janela de contexto?
- Por que existe limite?
- Evolu√ß√£o: 2K ‚Üí 128K ‚Üí 1M ‚Üí 4M
- **Papers:** Survey Comprehensive, From 128K to 4M

### Parte 2: Limita√ß√µes Pr√°ticas (30 min)
- Lost in the Middle
- Context Degradation
- Gap entre contexto nominal e efetivo
- **Papers:** Lost in the Middle (2023), RULER, Context Degradation Analysis

### Parte 3: T√©cnicas de Otimiza√ß√£o (30 min)
- Sliding Window Attention
- RoPE e extens√µes (YaRN, LongRoPE)
- KV Cache compression
- **Papers:** SWAT, Longformer, YaRN, Continuous KV Cache

### Parte 4: In-Context Learning (20 min)
- Como LLMs usam a janela de contexto
- Few-shot learning
- Trade-offs de n√∫mero de exemplos
- **Papers:** Surveys de ICL

### Parte 5: Fronteiras de Pesquisa (20 min)
- Infinite context techniques
- 3M+ tokens em single GPU
- Contextos multimodais
- **Papers:** ReAttention, Infini-attention, LM2, 3M Tokens Single GPU

### Parte 6: Aplica√ß√µes e Benchmarks (15 min)
- Casos de uso reais (c√≥digo, documentos)
- Como medir capacidades de contexto
- **Papers:** LongCodeBench, 100-LongBench, MMLongBench

### Parte 7: Futuro e Implica√ß√µes (15 min)
- Para onde vamos?
- Trade-offs fundamentais
- Implica√ß√µes pr√°ticas

---

## üîç CONCEITOS-CHAVE E PAPERS CORRESPONDENTES

| Conceito | Papers Principais | N√≠vel |
|----------|------------------|--------|
| **Context Window** | Survey Comprehensive, From 128K to 4M | B√°sico |
| **Sliding Window** | SWAT, Longformer, RATTENTION | Intermedi√°rio |
| **Lost in Middle** | Lost in the Middle (2023), Lost in Middle Emergent (2025) | B√°sico |
| **Context Degradation** | Context Degradation Analysis, RULER | Intermedi√°rio |
| **In-Context Learning** | 3 Surveys de ICL | B√°sico |
| **RoPE Extension** | Extending Context RoPE, YaRN, LongRoPE | Avan√ßado |
| **Infinite Context** | Infini-attention, ReAttention, LM2 | Avan√ßado |
| **KV Cache** | Continuous KV Cache, Overflow Prevention | Avan√ßado |
| **Benchmarking** | RULER, 100-LongBench, LongCodeBench | Intermedi√°rio |
| **Efficiency** | 3M Tokens Single GPU, EdgeInfinite | Avan√ßado |

---

## ‚úÖ CHECKLIST DE PREPARA√á√ÉO

Antes da aula, certifique-se de:

- [ ] Ter lido ao menos 5 papers fundamentais
  - [ ] Lost in the Middle (2023) - OBRIGAT√ìRIO
  - [ ] RULER (2024) - OBRIGAT√ìRIO
  - [ ] Survey Comprehensive Long Context LM - OBRIGAT√ìRIO
  - [ ] SWAT ou Longformer (para sliding window)
  - [ ] Um survey de ICL

- [ ] Ter preparado explica√ß√µes visuais de:
  - [ ] O que √© janela de contexto (diagrama)
  - [ ] Sliding window attention (anima√ß√£o/diagrama)
  - [ ] Fen√¥meno "lost in the middle" (gr√°fico em U)
  - [ ] Context degradation (gr√°fico)

- [ ] Ter exemplos pr√°ticos prontos:
  - [ ] Prompt curto vs. longo (demonstra√ß√£o)
  - [ ] ICL com diferentes n√∫meros de shots
  - [ ] Exemplo de contexto muito longo (documento)

- [ ] Ter n√∫meros atualizados:
  - [ ] Janelas de contexto de modelos populares
  - [ ] Benchmarks comparativos
  - [ ] Custos computacionais

- [ ] Ter screenshots/figuras importantes dos papers

---

## üîó LINKS E RECURSOS ADICIONAIS

**ArXiv Collection:**
Todos os PDFs foram baixados de https://arxiv.org/

**Para buscar atualiza√ß√µes:**
- Buscar "long context" OR "context window" em arxiv.org
- Filtrar por categoria cs.CL (Computation and Language)
- Ordenar por data

**C√≥digo open-source relacionado:**
- Muitos papers incluem links para reposit√≥rios GitHub
- Consultar se√ß√µes "Code availability" nos papers

---

## üìù NOTAS SOBRE USO E CITA√á√ïES

**Cita√ß√µes:**
Sempre cite a fonte original ao usar conte√∫do destes papers:
- Formato: Autor et al., "T√≠tulo", ArXiv:XXXX.XXXXX, Ano
- Todos os papers est√£o dispon√≠veis publicamente no ArXiv

**Copyright:**
Todos estes papers est√£o sob licen√ßas que permitem uso educacional e de pesquisa.

**Vers√µes:**
Papers no ArXiv podem ter m√∫ltiplas vers√µes. As vers√µes baixadas s√£o as mais recentes dispon√≠veis em 31/10/2025.

---

## üéØ PERGUNTAS FREQUENTES (FAQ)

**P: Qual a diferen√ßa entre "context window" e "context length"?**
R: S√£o termos intercambi√°veis. Context window enfatiza a "janela deslizante", context length enfatiza o tamanho.

**P: Por que 35 papers? N√£o √© demais?**
R: Cole√ß√£o completa para diferentes n√≠veis de profundidade:
- 5-6 papers para prepara√ß√£o r√°pida
- 15-20 papers para prepara√ß√£o completa
- 35 papers para refer√™ncia e aprofundamento posterior

**P: Qual paper √© MAIS importante?**
R: **Lost in the Middle (2023)** - √â o paper seminal que definiu o problema principal.

**P: Papers de 2025 s√£o confi√°veis?**
R: Sim, todos s√£o de ArXiv com autores credenciados. Alguns ainda em review, mas metodologia √© s√≥lida.

**P: Preciso entender a matem√°tica de todos os papers?**
R: N√£o para a aula. Foque em intui√ß√µes, resultados e implica√ß√µes pr√°ticas.

---

## üöÄ PR√ìXIMOS PASSOS

Ap√≥s usar esta cole√ß√£o:

1. **Marcar papers mais relevantes** para sua narrativa da aula
2. **Extrair figuras e gr√°ficos** importantes
3. **Preparar demonstra√ß√µes pr√°ticas** usando conceitos dos papers
4. **Criar quiz/exerc√≠cios** baseados nos conceitos
5. **Preparar lista de leituras** recomendadas para alunos

---

**Compilado em:** 31 de outubro de 2025

**Para:** George Marmelstein - Aulas 2025

**Aula:** 3 - Janela de Contexto

**√öltima atualiza√ß√£o:** 31/10/2025

---

**LEGENDA:**
- ‚úÖ = Recomendado para leitura priorit√°ria
- üî• = Paper particularmente influente/citado
- üìä = Cont√©m benchmarks/resultados emp√≠ricos importantes
- üéØ = Aplica√ß√£o pr√°tica direta
- üß† = Conceito te√≥rico profundo
