# üìÑ √çNDICE COMPLETO DE PDFs - AULA 4: CONHECIMENTO DA M√ÅQUINA

**Total de PDFs:** 37 papers cient√≠ficos

**Distribui√ß√£o:**
- 5 Surveys de 2024-2025
- 18 Papers de 2025
- 3 Papers sobre RAG
- 11 Papers Fundamentais (2019-2024)

**Data de download:** 31 de outubro de 2025

---

## üìä RESUMO EXECUTIVO

Esta cole√ß√£o aborda **conhecimento em LLMs**, incluindo:
- **Conhecimento Param√©trico:** Conhecimento armazenado nos pesos do modelo
- **Conhecimento Externo:** RAG, knowledge graphs, retrieval
- **Knowledge Editing:** Modifica√ß√£o de conhecimento factual sem re-treinamento
- **Knowledge Localization:** Onde o conhecimento √© armazenado (neurons, layers)
- **Knowledge Conflicts:** Conflitos entre conhecimento interno e externo
- **Memoriza√ß√£o vs. Generaliza√ß√£o:** Como LLMs armazenam e recuperam conhecimento

---

## üìö SURVEYS DE 2024-2025

### 1. 2025_Survey_LLM_Inference_External_Knowledge.pdf
**T√≠tulo:** LLM Inference Enhanced by External Knowledge: A Survey

**ArXiv:** 2505.24377

**Data:** Maio 2025

**Descri√ß√£o:**
Survey sistem√°tico sobre estrat√©gias para usar conhecimento externo para melhorar LLMs, abordando limita√ß√µes de mem√≥ria param√©trica e suscetibilidade a alucina√ß√µes.

**Taxonomia proposta:**
- Conhecimento externo estruturado vs n√£o-estruturado
- Tabelas: reasoning simb√≥lico, neural, h√≠brido
- Knowledge Graphs: loose coupling vs tight coupling

**Relev√¢ncia para Aula 4:**
**ESSENCIAL** - Fornece taxonomia completa de conhecimento externo e m√©todos de integra√ß√£o com conhecimento param√©trico.

---

### 2. 2024_Survey_Knowledge_Editing_LLMs.pdf
**T√≠tulo:** Knowledge Editing for Large Language Models: A Survey

**ArXiv:** 2310.16218 (vers√£o 4, atualizada at√© 2024)

**Data:** Outubro 2024 (v4)

**Descri√ß√£o:**
Survey abrangente sobre Knowledge-based Model Editing (KME), categorizando t√©cnicas em tr√™s grupos: external memorization, global optimization, e local modification.

**M√©todos cobertos:**
- ROME (Rank-One Model Editing)
- MEMIT (Mass-Editing Memory in Transformer)
- SERAC, GRACE, WISE, AlphaEdit
- Fine-tuning approaches

**Desafios discutidos:**
- Locality (n√£o afetar conhecimento n√£o-relacionado)
- Generalization (generalizar edi√ß√£o)
- Scalability (escalar para milhares de edi√ß√µes)

**Relev√¢ncia para Aula 4:**
**ESSENCIAL** - Principal survey sobre knowledge editing, conceito central da aula.

---

### 3. 2024_Survey_Knowledge_Boundary_LLMs.pdf
**T√≠tulo:** Knowledge Boundary of Large Language Models: A Survey

**ArXiv:** 2412.12472

**Data:** Dezembro 2024

**Descri√ß√£o:**
Define fronteiras de conhecimento em LLMs atrav√©s de tr√™s dimens√µes: Universal Knowledge Boundary (conhecimento express√°vel em QA), Parametric Knowledge Boundary (conhecimento nos par√¢metros), e Outward Knowledge Boundary (empiricamente validado).

**Taxonomia:**
Categoriza conhecimento em 4 tipos distintos baseado nas fronteiras

**Lentes de an√°lise:**
- Motiva√ß√£o para estudar fronteiras de conhecimento
- M√©todos para identificar fronteiras
- Estrat√©gias para mitigar desafios

**Relev√¢ncia para Aula 4:**
Framework conceitual importante para entender limites do conhecimento em LLMs.

---

### 4. 2025_Survey_Domain_Specific_Knowledge_Injection.pdf
**T√≠tulo:** Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey

**ArXiv:** 2502.10708

**Data:** Fevereiro 2025

**Descri√ß√£o:**
Survey sobre m√©todos de inje√ß√£o de conhecimento espec√≠fico de dom√≠nio (healthcare, chemistry, legal) em LLMs.

**Quatro abordagens principais:**
1. **Dynamic knowledge injection:** Conhecimento injetado dinamicamente durante infer√™ncia
2. **Static knowledge embedding:** Conhecimento embutido estaticamente nos pesos
3. **Modular adapters:** M√≥dulos especializados adicionados ao modelo
4. **Prompt optimization:** Otimiza√ß√£o de prompts para elicitar conhecimento

**Relev√¢ncia para Aula 4:**
Mostra como conhecimento especializado pode ser injetado em LLMs de diversas formas.

---

### 5. 2025_Survey_Knowledge_Representation_Learning.pdf
**T√≠tulo:** Large Language Model Enhanced Knowledge Representation Learning: A Survey

**ArXiv:** 2407.00936

**Data:** Julho 2024/2025 (atualizado)

**Publica√ß√£o:** Data Science and Engineering, Volume 10, 2025

**Descri√ß√£o:**
Survey sobre como LLMs melhoram Knowledge Representation Learning (KRL), que projeta fatos de Knowledge Graphs em espa√ßos vetoriais.

**Tr√™s abordagens:**
- **Encoder-based:** Contextual information detalhada
- **Encoder-decoder:** Unified Seq2Seq model
- **Decoder-based:** Extensive knowledge from large corpora

**Problemas abordados:**
- KRL sofre de sparsity em KGs
- LLMs incorporam informa√ß√£o textual para mitigar esparsidade

**Relev√¢ncia para Aula 4:**
Conecta conhecimento estruturado (KGs) com conhecimento param√©trico (LLMs).

---

## üî¨ PAPERS DE 2025

### 6. 2025_Rethinking_Parametric_Knowledge.pdf
**T√≠tulo:** Rethinking LLM Parametric Knowledge as Post-retrieval Confidence

**ArXiv:** 2509.06472

**Data:** Setembro 2025

**Descri√ß√£o:**
Categoriza conhecimento param√©trico em tr√™s tipos:

1. **Prompt-agnostic Knowledge:** Corretamente respond√≠vel independentemente da formula√ß√£o do prompt
2. **Prompt-sensitive Knowledge:** Respond√≠vel apenas sob estrat√©gias espec√≠ficas de prompting
3. **Unanswerable Knowledge:** Incapaz de resposta correta sob qualquer prompt

**Contribui√ß√£o principal:**
Prop√µe usar conhecimento param√©trico como confian√ßa p√≥s-retrieval para retrieval din√¢mico e reranking.

**Relev√¢ncia para Aula 4:**
**IMPORTANTE** - Diferencia tipos de conhecimento param√©trico e como usar essa diferencia√ß√£o.

---

### 7. 2025_ParamMute_Knowledge_Critical_FFNs.pdf
**T√≠tulo:** ParamMute: Suppressing Knowledge-Critical FFNs for Faithful RAG

**ArXiv:** 2502.15543

**Data:** Fevereiro 2025 (tamb√©m conhecido como PIP-KAG)

**Descri√ß√£o:**
Identifica que feed-forward networks (FFNs) em camadas m√©dio-profundas s√£o desproporcionalmente ativadas em casos de gera√ß√£o n√£o-fiel (unfaithful) em RAG.

**M√©todo proposto:**
Parametric Pruning: poda conhecimento interno de LLMs e incorpora m√≥dulo de adapta√ß√£o para melhor uso de fontes externas.

**Problema abordado:**
Conhecimento interno pode conflitar com conhecimento externo, causando degrada√ß√£o de performance em RAG.

**Relev√¢ncia para Aula 4:**
Localiza onde conhecimento √© ativado e como suprimir conflitos.

---

### 8. 2025_Bridging_External_Parametric_Knowledge.pdf
**T√≠tulo:** Bridging External and Parametric Knowledge: Mitigating Hallucination with Shared-Private Semantic Synergy (DSSP-RAG)

**ArXiv:** 2506.06240

**Data:** Junho 2025

**Descri√ß√£o:**
Prop√µe framework DSSP-RAG que refina self-attention em mixed-attention, distinguindo sem√¢ntica compartilhada e privada para integra√ß√£o controlada de conhecimento interno-externo.

**Problema:**
Conhecimento externo pode conflitar com conhecimento param√©trico; LLMs atuais n√£o t√™m mecanismos inerentes para resolver tais conflitos.

**Solu√ß√£o:**
Separa√ß√£o expl√≠cita de conhecimento shared (compartilhado) vs private (espec√≠fico de fonte).

**Relev√¢ncia para Aula 4:**
**IMPORTANTE** - Aborda conflitos de conhecimento, tema central da aula.

---

### 9. 2025_Parameters_vs_Context_Knowledge_Reliance.pdf
**T√≠tulo:** Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models (CK-PLUG)

**ArXiv:** 2503.15888

**Data:** Mar√ßo 2025

**Descri√ß√£o:**
Conflitos entre conhecimento param√©trico e contexto retrieved representam desafios quando informa√ß√£o retrieved √© n√£o-confi√°vel ou conhecimento do modelo est√° desatualizado.

**M√©todo proposto:**
- **Confidence Gain:** M√©trica de consist√™ncia de conhecimento que detecta conflitos medindo shifts de entropia em distribui√ß√µes de tokens ap√≥s inser√ß√£o de contexto.

**Aplica√ß√£o:**
Permite LLM decidir dinamicamente quando confiar em par√¢metros vs. contexto.

**Relev√¢ncia para Aula 4:**
Quantifica conflitos de conhecimento e permite controle fino de reliance.

---

### 10. 2025_Training_Dynamics_Parametric_InContext.pdf
**T√≠tulo:** Training Dynamics of Parametric and In-Context Knowledge Utilization

**ArXiv:** 2510.02370

**Data:** Outubro 2025

**Descri√ß√£o:**
LLMs frequentemente encontram conflitos entre in-context knowledge (retrieved em inference) e parametric knowledge (adquirido em pretraining).

**Descobertas:**
- Treinamento em corpora com informa√ß√£o inconsistente ou distributional skew encoraja modelos a desenvolver estrat√©gias robustas para usar ambos tipos de conhecimento.
- Revela din√¢micas de treinamento que levam a diferentes estrat√©gias de utiliza√ß√£o de conhecimento.

**Relev√¢ncia para Aula 4:**
Explica como conhecimento param√©trico e contextual interagem durante treinamento.

---

### 11. 2025_Knowledge_Injection_Low_Resource.pdf
**T√≠tulo:** Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime

**ArXiv:** 2508.06178

**Data:** Agosto 2025

**Descri√ß√£o:**
Investiga atualiza√ß√£o de LLMs com apenas poucos milhares ou milh√µes de tokens (low-resource regime).

**Descobertas:**
- Simplesmente continuar pretraining em dados limitados resulta em melhorias modestas
- Expor modelo a varia√ß√µes textuais diversas melhora significativamente aprendizado de novos fatos

**M√©todos comparados:**
- Continued pretraining
- Synthetic data augmentation
- Prompt-based learning

**Relev√¢ncia para Aula 4:**
Pr√°tico para injetar conhecimento especializado sem recursos massivos.

---

### 12. 2025_MEGa_Memory_Embedded_Gated_LLMs.pdf
**T√≠tulo:** Memorization and Knowledge Injection in Gated LLMs (MEGa)

**ArXiv:** 2504.21239

**Data:** Abril 2025

**Descri√ß√£o:**
Introduz MEGa (Memory Embedded in Gated LLMs), que injeta event memories diretamente nos pesos de LLMs.

**M√©todo:**
Cada mem√≥ria armazenada em conjunto dedicado de gated low-rank weights.

**Vantagem:**
Permite adi√ß√£o incremental de mem√≥rias sem interferir em conhecimento existente.

**Relev√¢ncia para Aula 4:**
Abordagem modular para inje√ß√£o de conhecimento.

---

### 13. 2025_Right_for_Right_Reasons_Commonsense.pdf
**T√≠tulo:** Right for Right Reasons: LLMs for Verifiable Commonsense Knowledge Graph QA

**ArXiv:** 2403.01390

**Data:** Mar√ßo 2025

**Descri√ß√£o:**
M√©todos baseados em LLM para KGQA (Knowledge Graph Question Answering) lutam com alucina√ß√£o em perguntas de common sense.

**M√©todo proposto (R3):**
Right for Right Reasons - metodologia que permite reasoning verific√°vel ao axiomaticamente surfacear conhecimento de common sense intr√≠nseco de LLMs e fundamentar cada passo de racioc√≠nio factual em triplas de KG.

**Relev√¢ncia para Aula 4:**
Combina conhecimento param√©trico (common sense) com conhecimento estruturado (KG).

---

### 14. 2025_WorldLLM_World_Modeling.pdf
**T√≠tulo:** WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making

**ArXiv:** 2506.06725

**Data:** Junho 2025

**Descri√ß√£o:**
Framework que melhora world modeling em LLMs combinando Bayesian inference e autonomous active exploration com reinforcement learning.

**Abordagem:**
Usa in-context learning abilities de LLMs para guiar predi√ß√µes usando natural language hypotheses.

**Relev√¢ncia para Aula 4:**
Conhecimento de "mundo" (world knowledge) √© tipo de conhecimento param√©trico.

---

### 15. 2025_LLMs_as_Commonsense_Heuristics.pdf
**T√≠tulo:** Large Language Models as Common-Sense Heuristics

**ArXiv:** 2501.18816

**Data:** Janeiro 2025

**Descri√ß√£o:**
M√©todo de planejamento que leverages conhecimento parametrizado de LLMs usando output como heur√≠stica para Hill-Climbing Search.

**Resultados:**
Supera sistemas similares em household environments por 22 pontos percentuais.

**Relev√¢ncia para Aula 4:**
Conhecimento de common sense como tipo de conhecimento param√©trico.

---

### 16. 2025_BaFT_Basis_Level_Knowledge_Editing.pdf
**T√≠tulo:** Unlocking Efficient, Scalable, and Continual Knowledge Editing with Basis-Level Representation Fine-Tuning (BaFT)

**ArXiv:** 2503.00306

**Data:** Mar√ßo 2025

**Descri√ß√£o:**
M√©todos projetados para atualizar certo conhecimento em LLMs sem mudar outros (unrelated).

**Desafio:**
Editing-locality trade-off

**M√©todo proposto:**
BaFT (Basis-Level Representation Fine-Tuning) para knowledge editing eficiente e escal√°vel.

**Relev√¢ncia para Aula 4:**
T√©cnica estado-da-arte para knowledge editing.

---

### 17. 2025_Knowledge_Updating_Contextual_Reasoning.pdf
**T√≠tulo:** Knowledge Updating? No More Model Editing! Just Selective Contextual Reasoning

**ArXiv:** 2503.05212

**Data:** Mar√ßo 2025

**Descri√ß√£o:**
Categoriza m√©todos de model editing em cinco tipos principais:

1. **Locate-then-edit methods** (ROME, MEMIT)
2. **Meta-learning based methods**
3. **Fine-tuning methods**
4. **External memory-based methods** (SERAC)
5. **Representation editing methods**

**Argumento:**
Selective contextual reasoning pode ser mais efetivo que model editing para knowledge updating.

**Relev√¢ncia para Aula 4:**
Oferece taxonomia de m√©todos e perspectiva alternativa.

---

### 18. 2025_Editing_as_Unlearning.pdf
**T√≠tulo:** Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for LLM Unlearning?

**ArXiv:** 2505.19855

**Data:** Maio 2025

**Descri√ß√£o:**
Avalia m√©todos estado-da-arte de editing incluindo ROME, MEMIT, GRACE, WISE, e AlphaEdit.

**Descoberta:**
Certos m√©todos de editing, notadamente WISE e AlphaEdit, s√£o baselines efetivos para unlearning.

**Relev√¢ncia para Aula 4:**
Conecta knowledge editing com machine unlearning.

---

### 19. 2025_Analyzing_Memorization_Model_Attribution.pdf
**T√≠tulo:** Analyzing Memorization in LLMs through the Lens of Model Attribution

**ArXiv:** 2501.05078

**Data:** Janeiro 2025

**Descri√ß√£o:**
Fornece framework te√≥rico mostrando que memoriza√ß√£o em LLMs primariamente origina de attention modules em camadas transformer profundas.

**Contribui√ß√£o:**
Localiza onde memoriza√ß√£o ocorre na arquitetura.

**Relev√¢ncia para Aula 4:**
Memoriza√ß√£o √© forma de conhecimento param√©trico; localiza√ß√£o √© importante.

---

### 20. 2025_Memorization_Intrinsic_Dimension.pdf
**T√≠tulo:** Memorization in Language Models through the Lens of Intrinsic Dimension

**ArXiv:** 2506.09591

**Data:** Junho 2025

**Descri√ß√£o:**
Liga memoriza√ß√£o a generaliza√ß√£o, particularmente para dados com long-tailed distributions onde memoriza√ß√£o pode servir como inductive bias.

**Relev√¢ncia para Aula 4:**
Teoriza sobre rela√ß√£o entre memoriza√ß√£o e generaliza√ß√£o.

---

### 21. 2025_LLM_Knowledge_Graph_Construction.pdf
**T√≠tulo:** LLM-empowered knowledge graph construction: A survey

**ArXiv:** 2510.20345

**Data:** Outubro 2025

**Descri√ß√£o:**
Survey abrangente analisando como LLMs remodelam pipeline cl√°ssico de tr√™s camadas: ontology engineering, knowledge extraction, e knowledge fusion.

**Tend√™ncias chave:**
- KG-based reasoning para LLMs
- Dynamic knowledge memory para agentic systems
- Multimodal KG construction

**Relev√¢ncia para Aula 4:**
LLMs constroem knowledge graphs a partir de conhecimento param√©trico.

---

### 22. 2025_Optimizing_KG_LLM_Interface.pdf
**T√≠tulo:** Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning

**ArXiv:** 2505.24478

**Data:** Maio 2025

**Descri√ß√£o:**
Estuda integra√ß√£o de LLMs com Knowledge Graphs, notando que tais sistemas t√™m numerosos hyperparameters que afetam diretamente performance.

**Benchmarks usados:**
HotPotQA, TwoWikiMultiHop, MuSiQue

**Par√¢metros otimizados:**
Chunking, graph construction, retrieval, prompting

**Relev√¢ncia para Aula 4:**
Pr√°tica de integra√ß√£o conhecimento externo (KG) com param√©trico (LLM).

---

### 23. 2024_Knowledge_Graphs_LLMs_Hallucinations.pdf
**T√≠tulo:** Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective

**ArXiv:** 2411.14258

**Data:** Novembro 2024

**Descri√ß√£o:**
Perspectiva NLP sobre como knowledge graphs podem mitigar alucina√ß√µes em LLMs ao fornecer conhecimento factual estruturado.

**Relev√¢ncia para Aula 4:**
Conecta limita√ß√µes de conhecimento param√©trico com solu√ß√µes baseadas em conhecimento externo.

---

## üìä PAPERS SOBRE RAG

### 24. 2025_Survey_RAG_Comprehensive.pdf
**T√≠tulo:** Retrieval-Augmented Generation: A Comprehensive Survey

**ArXiv:** 2506.00054

**Data:** Junho 2025

**Descri√ß√£o:**
Survey abrangente oferecendo taxonomia que categoriza arquiteturas em retriever-centric, generator-centric, hybrid, e robustness-oriented designs.

**An√°lise sistem√°tica:**
- Retrieval optimization
- Context filtering
- Decoding control
- Efficiency improvements

**Frameworks de avalia√ß√£o:**
Revisa state-of-the-art evaluation frameworks e benchmarks.

**Relev√¢ncia para Aula 4:**
**ESSENCIAL** - RAG √© principal m√©todo de conhecimento externo. Survey completo.

---

### 25. 2025_Survey_Knowledge_Oriented_RAG.pdf
**T√≠tulo:** A Survey on Knowledge-Oriented Retrieval-Augmented Generation

**ArXiv:** 2503.10677

**Data:** Mar√ßo 2025

**Descri√ß√£o:**
Examina potencial de RAG para melhorar natural language understanding combinando large-scale retrieval systems com generative models.

**Fontes de conhecimento:**
Documents, databases, structured data

**Relev√¢ncia para Aula 4:**
Foco espec√≠fico em conhecimento em RAG.

---

### 26. 2025_Survey_Agentic_RAG.pdf
**T√≠tulo:** Agentic Retrieval-Augmented Generation: A Survey

**ArXiv:** 2501.09136

**Data:** Janeiro 2025

**Descri√ß√£o:**
Agentic RAG transcende limita√ß√µes tradicionais ao embeber autonomous AI agents no pipeline de RAG.

**Agentic design patterns:**
- Reflection
- Planning
- Tool use
- Multiagent collaboration

**Relev√¢ncia para Aula 4:**
Evolu√ß√£o de RAG com agentes aut√¥nomos.

---

## üìñ PAPERS FUNDAMENTAIS (2019-2024)

### 27. 2019_LAMA_Language_Models_as_Knowledge_Bases.pdf
**T√≠tulo:** Language Models as Knowledge Bases?

**Autores:** Petroni et al.

**Publica√ß√£o:** EMNLP 2019

**Descri√ß√£o:**
**PAPER SEMINAL** que introduziu LAMA benchmark para probing de conhecimento factual e common sense em pretrained language models.

**M√©todo:**
Usa masked sentences como "Paris is the capital of [MASK]" como probes ao inv√©s de structural KB queries.

**Tipos de conhecimento testados:**
- Rela√ß√µes entre entidades (Wikidata)
- Rela√ß√µes de common sense (ConceptNet)
- Conhecimento necess√°rio para QA (SQuAD)

**Relev√¢ncia para Aula 4:**
**ESSENCIAL** - Paper fundacional sobre knowledge probing em LLMs. Leitura obrigat√≥ria.

---

### 28. 2020_RAG_Original_Paper.pdf
**T√≠tulo:** Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

**Autores:** Lewis et al.

**ArXiv:** 2005.11401

**Publica√ß√£o:** NeurIPS 2020

**Descri√ß√£o:**
**PAPER ORIGINAL de RAG** - Explora fine-tuning recipe para retrieval-augmented generation, modelos que combinam parametric e non-parametric memory.

**Contribui√ß√£o:**
- Combina pre-trained parametric memory (seq2seq model) com non-parametric memory (Wikipedia via dense vector index)
- Estado-da-arte em three open domain QA tasks

**Relev√¢ncia para Aula 4:**
**ESSENCIAL** - Paper seminal de RAG, m√©todo principal de conhecimento externo.

---

### 29. 2021_Knowledge_Neurons_Pretrained_Transformers.pdf
**T√≠tulo:** Knowledge Neurons in Pretrained Transformers

**Autores:** Dai et al.

**ArXiv:** 2104.08696

**Publica√ß√£o:** ACL 2022

**Descri√ß√£o:**
Introduz conceito de **knowledge neurons** - neur√¥nios individuais em MLP layers que encodam fatos individuais.

**M√©todo proposto:**
Knowledge attribution method para identificar neurons que expressam fatos espec√≠ficos.

**Descoberta:**
Ativa√ß√£o de knowledge neurons √© positivamente correlacionada √† express√£o de seus fatos correspondentes.

**Aplica√ß√£o:**
Permite editar (update, erase) conhecimento factual espec√≠fico sem fine-tuning.

**Relev√¢ncia para Aula 4:**
**ESSENCIAL** - Localiza conhecimento em neurons espec√≠ficos. Fundamental para knowledge editing.

---

### 30. 2022_ROME_Locating_Editing_Factual_Associations.pdf
**T√≠tulo:** Locating and Editing Factual Associations in GPT (ROME)

**Autores:** Meng et al.

**ArXiv:** 2202.05262

**Publica√ß√£o:** NeurIPS 2022

**Descri√ß√£o:**
Analisa storage e recall de factual associations em autoregressive transformer LMs.

**Descoberta principal:**
Associa√ß√µes factuais correspondem a computa√ß√µes localizadas, diretamente edit√°veis em:
1. MLP module parameters
2. Range de middle layers
3. Especificamente durante processamento do last token do subject

**M√©todo proposto:**
**ROME** (Rank-One Model Editing) - modifica feed-forward weights para atualizar associa√ß√µes factuais espec√≠ficas.

**Relev√¢ncia para Aula 4:**
**ESSENCIAL** - M√©todo fundacional de knowledge editing. Base para MEMIT e outros.

---

### 31. 2022_MEMIT_Mass_Editing_Memory.pdf
**T√≠tulo:** Mass-Editing Memory in a Transformer (MEMIT)

**Autores:** Meng et al.

**ArXiv:** 2210.07229

**Publica√ß√£o:** ICLR 2023

**Descri√ß√£o:**
M√©todo para atualizar diretamente language model com muitas mem√≥rias simultaneamente.

**Escalabilidade:**
Demonstra experimentalmente que pode escalar para **milhares de associa√ß√µes** para GPT-J (6B) e GPT-NeoX (20B), excedendo trabalhos anteriores por ordens de magnitude.

**Contribui√ß√£o:**
Modifica transformer weights para editar mem√≥rias mantendo generalization, specificity, e fluency em escalas al√©m de outros m√©todos.

**Relev√¢ncia para Aula 4:**
Extens√£o escal√°vel de ROME. Permite knowledge editing em massa.

---

### 32. 2022_Quantifying_Memorization_NLMs.pdf
**T√≠tulo:** Quantifying Memorization Across Neural Language Models

**ArXiv:** 2202.07646

**Data:** 2022

**Descri√ß√£o:**
Mostra que large language models memorizam partes de seus dados de treinamento e emitem training data memorizado verbatim quando prompted apropriadamente.

**Contribui√ß√£o:**
M√©todos para quantificar memoriza√ß√£o.

**Relev√¢ncia para Aula 4:**
Memoriza√ß√£o √© tipo de conhecimento param√©trico.

---

### 33. 2023_SoK_Memorization_LLMs.pdf
**T√≠tulo:** SoK: Memorization in General-Purpose Large Language Models

**ArXiv:** 2310.18362

**Data:** 2023

**Descri√ß√£o:**
Prop√µe taxonomia para memoriza√ß√£o em LLMs cobrindo:
- Verbatim text
- Facts
- Ideas and algorithms
- Writing styles
- Distributional properties
- Alignment goals

**Insight:**
Parte importante do sucesso de LLMs √© devido a huge training datasets e unprecedented n√∫mero de model parameters, que permitem memorizar grandes quantidades de informa√ß√£o.

**Relev√¢ncia para Aula 4:**
Taxonomia abrangente de tipos de memoriza√ß√£o/conhecimento.

---

### 34. 2024_Generalization_vs_Memorization.pdf
**T√≠tulo:** Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data

**ArXiv:** 2407.14985

**Data:** 2024

**Descri√ß√£o:**
Demonstra que memoriza√ß√£o desempenha papel maior em tarefas simples e knowledge-intensive, enquanto generaliza√ß√£o √© chave para tarefas de reasoning mais dif√≠ceis.

**Descobertas por model size:**
√Ä medida que model size aumenta:
- Apenas factual QA mostra increased memorization
- Machine translation e reasoning tasks exibem greater generalization

**Relev√¢ncia para Aula 4:**
**IMPORTANTE** - Diferencia quando conhecimento √© memorizado vs generalizado.

---

### 35. 2024_Comprehensive_Study_Knowledge_Editing.pdf
**T√≠tulo:** A Comprehensive Study of Knowledge Editing for Large Language Models

**ArXiv:** 2401.01286

**Data:** 2024 (v4)

**Descri√ß√£o:**
Estudo abrangente classificando knowledge editing methods em tr√™s grupos:

1. **Resorting to external knowledge**
2. **Merging knowledge into the model**
3. **Editing intrinsic knowledge**

**Relev√¢ncia para Aula 4:**
Taxonomia alternativa de knowledge editing methods.

---

### 36. 2023_KoLA_World_Knowledge_Benchmark.pdf
**T√≠tulo:** KoLA: Carefully Benchmarking World Knowledge of Large Language Models

**ArXiv:** 2306.09296

**Data:** 2023

**Descri√ß√£o:**
Constr√≥i benchmark com taxonomia de quatro n√≠veis de knowledge-related abilities, cobrindo 19 tarefas.

**Avalia:**
World knowledge (conhecimento de mundo) em LLMs.

**Relev√¢ncia para Aula 4:**
Benchmark para avaliar conhecimento param√©trico.

---

### 37. 2023_Unifying_LLMs_Knowledge_Graphs.pdf
**T√≠tulo:** Unifying Large Language Models and Knowledge Graphs: A Roadmap

**ArXiv:** 2306.08302

**Data:** 2023

**Descri√ß√£o:**
Prop√µe roadmap para unificar LLMs e Knowledge Graphs.

**Tr√™s frameworks gerais:**
1. **KG-enhanced LLMs:** Incorporar KGs durante pre-training e inference
2. **LLM-augmented KGs:** Usar LLMs para tarefas de KG (embedding, completion)
3. **Synergized LLMs + KGs:** Reasoning bidirecional mutuamente ben√©fico

**Relev√¢ncia para Aula 4:**
Framework conceitual para integra√ß√£o conhecimento param√©trico e estruturado.

---

## üìñ COMO USAR ESTA COLE√á√ÉO

### Para Prepara√ß√£o R√°pida da Aula (2-3 horas):

**Prioridade 1 - Conceitos Essenciais:**
1. **2019_LAMA_Language_Models_as_Knowledge_Bases.pdf** (introdu√ß√£o + se√ß√£o 3) - 20 min
2. **2022_ROME_Locating_Editing_Factual_Associations.pdf** (introdu√ß√£o + m√©todo) - 30 min
3. **2020_RAG_Original_Paper.pdf** (introdu√ß√£o + resultados) - 30 min

**Prioridade 2 - Estado da Arte:**
4. **2024_Survey_Knowledge_Editing_LLMs.pdf** (skim se√ß√µes principais) - 40 min
5. **2025_Survey_LLM_Inference_External_Knowledge.pdf** (taxonomia) - 30 min

**Prioridade 3 - Conflitos de Conhecimento:**
6. **2025_Bridging_External_Parametric_Knowledge.pdf** (se√ß√£o de conflitos) - 20 min

---

## üéØ PAPERS POR CONCEITO-CHAVE DA AULA

### Para explicar "Conhecimento Param√©trico":
- ‚úÖ **2019_LAMA_Language_Models_as_Knowledge_Bases.pdf** (ESSENCIAL)
- ‚úÖ **2021_Knowledge_Neurons_Pretrained_Transformers.pdf** (ESSENCIAL)
- ‚úÖ **2025_Rethinking_Parametric_Knowledge.pdf** (tr√™s tipos)

### Para explicar "Knowledge Editing":
- ‚úÖ **2024_Survey_Knowledge_Editing_LLMs.pdf** (SURVEY PRINCIPAL)
- ‚úÖ **2022_ROME_Locating_Editing_Factual_Associations.pdf** (ESSENCIAL)
- ‚úÖ **2022_MEMIT_Mass_Editing_Memory.pdf** (escalabilidade)
- ‚úÖ **2025_BaFT_Basis_Level_Knowledge_Editing.pdf** (estado-da-arte)

### Para explicar "Conhecimento Externo / RAG":
- ‚úÖ **2020_RAG_Original_Paper.pdf** (PAPER ORIGINAL)
- ‚úÖ **2025_Survey_RAG_Comprehensive.pdf** (survey completo)
- ‚úÖ **2025_Survey_LLM_Inference_External_Knowledge.pdf** (taxonomia)
- ‚úÖ **2025_Survey_Agentic_RAG.pdf** (evolu√ß√µes recentes)

### Para explicar "Knowledge Conflicts":
- ‚úÖ **2025_Bridging_External_Parametric_Knowledge.pdf** (IMPORTANTE)
- ‚úÖ **2025_Parameters_vs_Context_Knowledge_Reliance.pdf** (controle fino)
- ‚úÖ **2025_Training_Dynamics_Parametric_InContext.pdf** (din√¢micas)
- ‚úÖ **2025_ParamMute_Knowledge_Critical_FFNs.pdf** (localiza√ß√£o de conflitos)

### Para explicar "Memoriza√ß√£o vs. Generaliza√ß√£o":
- ‚úÖ **2024_Generalization_vs_Memorization.pdf** (IMPORTANTE)
- ‚úÖ **2022_Quantifying_Memorization_NLMs.pdf** (quantifica√ß√£o)
- ‚úÖ **2023_SoK_Memorization_LLMs.pdf** (taxonomia)
- ‚úÖ **2025_Analyzing_Memorization_Model_Attribution.pdf** (localiza√ß√£o)

### Para explicar "Knowledge Localization":
- ‚úÖ **2021_Knowledge_Neurons_Pretrained_Transformers.pdf** (ESSENCIAL)
- ‚úÖ **2022_ROME_Locating_Editing_Factual_Associations.pdf** (ESSENCIAL)
- ‚úÖ **2025_ParamMute_Knowledge_Critical_FFNs.pdf** (FFNs)

### Para explicar "Knowledge Injection":
- ‚úÖ **2025_Survey_Domain_Specific_Knowledge_Injection.pdf** (survey completo)
- ‚úÖ **2025_Knowledge_Injection_Low_Resource.pdf** (low-resource)
- ‚úÖ **2025_MEGa_Memory_Embedded_Gated_LLMs.pdf** (approach modular)

### Para explicar "Knowledge Graphs + LLMs":
- ‚úÖ **2023_Unifying_LLMs_Knowledge_Graphs.pdf** (roadmap)
- ‚úÖ **2025_LLM_Knowledge_Graph_Construction.pdf** (survey recente)
- ‚úÖ **2025_Optimizing_KG_LLM_Interface.pdf** (otimiza√ß√£o pr√°tica)

---

## üìä ESTAT√çSTICAS DA COLE√á√ÉO

**Total de p√°ginas:** ~700+ p√°ginas de conte√∫do cient√≠fico

**Distribui√ß√£o temporal:**
- 2019: 1 paper (LAMA - seminal)
- 2020: 1 paper (RAG - seminal)
- 2021: 1 paper (Knowledge Neurons - seminal)
- 2022: 3 papers (ROME, MEMIT, Quantifying Memorization)
- 2023: 3 papers (SoK, KoLA, Unifying)
- 2024: 4 papers (Surveys, Generalization vs Memorization)
- 2025: 24 papers (estado da arte)

**Papers MAIS de 2025:** ‚úÖ 24 papers (muito al√©m de "mais de 10")

**M√©todos de Knowledge Editing cobertos:**
- ROME (Rank-One Model Editing)
- MEMIT (Mass-Editing Memory)
- GRACE, SERAC, WISE, AlphaEdit
- BaFT (Basis-Level Fine-Tuning)
- ParamMute

**Conceitos cobertos:**
- Parametric knowledge (tr√™s tipos)
- External knowledge (RAG, KG)
- Knowledge editing (7+ methods)
- Knowledge localization (neurons, FFNs, layers)
- Knowledge conflicts (parametric vs external)
- Memorization vs generalization
- Knowledge injection (4 approaches)
- Knowledge probing (LAMA)
- World knowledge, common sense

---

## üéì ESTRUTURA SUGERIDA PARA A AULA

### Parte 1: Conceitos Fundamentais (25 min)
- O que √© conhecimento em LLMs?
- Tr√™s tipos de conhecimento:
  - Param√©trico (nos pesos)
  - Contextual (no prompt)
  - Externo (RAG, KG)
- **Papers:** LAMA, Survey External Knowledge

### Parte 2: Conhecimento Param√©trico (30 min)
- Como conhecimento √© armazenado nos pesos
- Knowledge neurons e localization
- Tr√™s tipos: Prompt-agnostic, Prompt-sensitive, Unanswerable
- Memoriza√ß√£o vs. Generaliza√ß√£o
- **Papers:** LAMA, Knowledge Neurons, ROME, Rethinking Parametric Knowledge

### Parte 3: Knowledge Editing (30 min)
- Por que editar conhecimento?
- M√©todos: ROME, MEMIT, BaFT
- Desafios: Locality, Generalization, Scalability
- **Papers:** Knowledge Editing Survey, ROME, MEMIT

### Parte 4: Conhecimento Externo (25 min)
- RAG: Retrieval-Augmented Generation
- Knowledge Graphs
- Integra√ß√£o LLM + KG
- **Papers:** RAG Original, RAG Survey, Unifying LLMs + KGs

### Parte 5: Knowledge Conflicts (20 min)
- Conflitos param√©trico vs externo
- Como detectar e resolver
- M√©todos: DSSP-RAG, CK-PLUG, ParamMute
- **Papers:** Bridging External Parametric, Parameters vs Context

### Parte 6: Knowledge Injection (15 min)
- Como injetar novo conhecimento
- Abordagens: Dynamic, Static, Modular, Prompt
- Low-resource scenarios
- **Papers:** Domain-Specific Knowledge Injection Survey

### Parte 7: Futuro e Implica√ß√µes (15 min)
- Tend√™ncias: Agentic RAG, Multimodal KG
- Desafios abertos
- Implica√ß√µes pr√°ticas

---

**Total:** ~160 minutos (2h40min) - Ajuste conforme necess√°rio

---

## ‚úÖ CHECKLIST DE PREPARA√á√ÉO

Antes da aula:

- [ ] Ter lido ao menos 5 papers fundamentais
  - [ ] LAMA (2019) - OBRIGAT√ìRIO
  - [ ] ROME (2022) - OBRIGAT√ìRIO
  - [ ] RAG (2020) - OBRIGAT√ìRIO
  - [ ] Knowledge Neurons (2021) - recomendado
  - [ ] Um survey de 2024-2025

- [ ] Ter preparado explica√ß√µes visuais de:
  - [ ] Tr√™s tipos de conhecimento (diagrama)
  - [ ] Knowledge neurons (visualiza√ß√£o)
  - [ ] ROME method (diagrama de edi√ß√£o)
  - [ ] RAG pipeline (fluxo)
  - [ ] Knowledge conflicts (exemplo)

- [ ] Ter exemplos pr√°ticos prontos:
  - [ ] Fact retrieval de LAMA
  - [ ] Knowledge editing example
  - [ ] RAG vs parametric comparison
  - [ ] Knowledge conflict case

---

**Compilado em:** 31 de outubro de 2025

**Para:** George Marmelstein - Aulas 2025

**Aula:** 4 - Conhecimento da M√°quina
