# √çNDICE COMPLETO - AULA 5: TREINAMENTO DE LLMs

## üìä VIS√ÉO GERAL

**Total de Papers:** 26 PDFs cient√≠ficos
**Distribui√ß√£o:**
- üìä 5 Surveys (2024-2025)
- üéì 6 Papers de Pr√©-treino (2018-2025)
- üîß 10 Papers de Fine-tuning (2017-2025)
- üåê 5 Papers de Modelos Open-Source (2023-2024)

**Per√≠odo Coberto:** 2017-2025 (8 anos de evolu√ß√£o)
**Volume Estimado:** ~650 p√°ginas de conte√∫do cient√≠fico

---

## üìÇ ORGANIZA√á√ÉO DAS PASTAS

```
Aula 5/
‚îú‚îÄ‚îÄ Surveys_2025/              (5 PDFs - Vis√µes gerais completas)
‚îú‚îÄ‚îÄ Papers_Pretraining/        (6 PDFs - Fundamentos do pr√©-treino)
‚îú‚îÄ‚îÄ Papers_Fine_Tuning/        (10 PDFs - M√©todos de ajuste fino)
‚îî‚îÄ‚îÄ Papers_LLAMA_OpenSource/   (5 PDFs - Modelos abertos)
```

---

# üìä SURVEYS (2024-2025)

## 1. 2025_Survey_Large_Language_Models.pdf
**ArXiv:** 2303.18223 (v16, atualizado em Mar√ßo 2025)
**T√≠tulo:** A Survey of Large Language Models
**Autores:** Wayne Xin Zhao et al. (Renmin University + Microsoft)
**P√°ginas:** ~170 p√°ginas

### Descri√ß√£o Detalhada:
O survey MAIS COMPLETO sobre LLMs, atualizado regularmente desde 2023. Vers√£o atual (v16) inclui desenvolvimentos at√© mar√ßo de 2025.

**Conte√∫do Principal:**
- Hist√≥ria completa dos LLMs (GPT-1 at√© modelos 2025)
- Arquitetura dos Transformers em detalhe
- **Pr√©-treino:** corpus, tokeniza√ß√£o, arquitetura, training objectives
- **Fine-tuning:** instruction tuning, alignment, RLHF
- **Capacidades emergentes:** in-context learning, chain-of-thought
- **Utiliza√ß√£o pr√°tica:** prompting, planning, tool use
- **Seguran√ßa:** alignment, safety, ethics
- **Recursos:** datasets, libraries, APIs

**Por que √© essencial:**
- Refer√™ncia definitiva sobre LLMs
- Atualizado regularmente (v16!)
- Cobre TODO o ciclo de vida: pr√©-treino ‚Üí fine-tuning ‚Üí deployment
- 800+ refer√™ncias bibliogr√°ficas
- Tabelas comparativas de todos os modelos principais

**Para a aula:**
- LEIA: Se√ß√µes 4 (pre-training) e 5 (adaptation/fine-tuning)
- USE: Figuras de arquitetura, tabelas de modelos
- TEMPO: 60-90 min (skim estrat√©gico)

---

## 2. 2025_Survey_Post_Training_LLMs.pdf
**ArXiv:** 2503.06072 (Agosto 2025)
**T√≠tulo:** A Survey on Post-training of Large Language Models
**Autores:** Research team (2025)
**P√°ginas:** ~50 p√°ginas

### Descri√ß√£o Detalhada:
Survey focado especificamente na fase de **post-training** (fine-tuning e alignment).

**Conte√∫do Principal:**
- **Supervised Fine-Tuning (SFT):** instruction tuning, task-specific adaptation
- **Alignment:** RLHF, DPO, RLAIF, Constitutional AI
- **Parameter-Efficient Methods:** LoRA, QLoRA, adapters
- **Safety alignment:** harmlessness, helpfulness, honesty
- **Evaluation:** benchmarks de alignment, human evaluation
- **Trade-offs:** alignment tax, capability preservation

**Por que √© essencial:**
- Foco total em fine-tuning (complementa o survey geral)
- Cobre m√©todos de 2025
- Taxonomia clara de m√©todos
- Compara√ß√£o emp√≠rica de t√©cnicas

**Para a aula:**
- LEIA: Se√ß√µes sobre RLHF, DPO, e parameter-efficient methods
- USE: Taxonomia de m√©todos de alignment
- TEMPO: 40-50 min

---

## 3. 2024_Survey_Datasets_LLMs.pdf
**ArXiv:** 2402.18041
**T√≠tulo:** A Survey on Data Selection for Language Models
**Autores:** Multiple institutions
**P√°ginas:** ~80 p√°ginas
**Cobertura:** 444 datasets, 774.5 TB de dados

### Descri√ß√£o Detalhada:
Survey exaustivo sobre **datasets de treinamento** para LLMs.

**Conte√∫do Principal:**
- **Tipos de dados:** web text, c√≥digo, livros, artigos cient√≠ficos
- **Data curation:** filtering, deduplication, quality control
- **Datasets famosos:** The Pile, C4, RedPajama, Common Crawl
- **Data mixing:** propor√ß√µes ideais de diferentes tipos de dados
- **Ethical considerations:** copyright, PII, bias
- **Scale:** de GB a PB de dados

**Datasets cobertos:**
- The Pile (825 GB, 22 subconjuntos)
- C4 (Colossal Clean Crawled Corpus, 750 GB)
- RedPajama (1.2 TB)
- Common Crawl (petabytes)
- RefinedWeb (600 GB)

**Por que √© essencial:**
- Explica O QUE entra no pr√©-treino
- 444 datasets catalogados!
- Data quality = model quality
- Quest√µes √©ticas e legais

**Para a aula:**
- LEIA: Overview de datasets principais
- USE: Estat√≠sticas de volume de dados
- TEMPO: 30-40 min (skim)

---

## 4. 2024_Survey_RLHF.pdf
**ArXiv:** 2312.14925
**T√≠tulo:** A Survey on Reinforcement Learning from Human Feedback
**P√°ginas:** ~60 p√°ginas

### Descri√ß√£o Detalhada:
Survey completo sobre **RLHF** (Reinforcement Learning from Human Feedback).

**Conte√∫do Principal:**
- **Hist√≥ria do RLHF:** de 2017 at√© 2024
- **Pipeline completo:** SFT ‚Üí Reward Model ‚Üí RL optimization
- **Reward modeling:** como treinar modelos de recompensa
- **RL algorithms:** PPO, REINFORCE, A2C
- **Desafios:** reward hacking, distribution shift
- **Variantes:** RLAIF, Constitutional AI, DPO
- **Aplica√ß√µes:** ChatGPT, Claude, Llama 2 Chat

**Por que √© essencial:**
- RLHF √© o m√©todo por tr√°s do ChatGPT
- Explica o pipeline completo
- Cobre desafios pr√°ticos
- Compara variantes (RLAIF, DPO)

**Para a aula:**
- LEIA: Pipeline de RLHF, comparison com DPO
- USE: Diagramas do pipeline
- TEMPO: 40-50 min

---

## 5. 2024_Survey_DPO_Comprehensive.pdf
**ArXiv:** 2410.15595
**T√≠tulo:** A Comprehensive Survey on Direct Preference Optimization
**P√°ginas:** ~40 p√°ginas

### Descri√ß√£o Detalhada:
Survey focado em **DPO** (Direct Preference Optimization) e variantes.

**Conte√∫do Principal:**
- **DPO basics:** simplifica√ß√£o do RLHF
- **Como funciona:** otimiza√ß√£o direta de prefer√™ncias (sem reward model!)
- **Vantagens:** mais simples, mais est√°vel, mais eficiente que RLHF
- **Variantes:** IPO, KTO, CPO, ORPO
- **Resultados emp√≠ricos:** compara√ß√£o com RLHF
- **Limita√ß√µes:** quando usar DPO vs RLHF

**Por que √© essencial:**
- DPO est√° se tornando o padr√£o (mais simples que RLHF)
- Llama 3 usou DPO
- M√©todos de 2024-2025
- Trade-offs claros

**Para a aula:**
- LEIA: DPO basics e comparison com RLHF
- USE: Algoritmo de DPO
- TEMPO: 30 min

---

# üéì PAPERS DE PR√â-TREINO

## 6. 2018_GPT1_Improving_Language_Understanding.pdf
**Paper:** Improving Language Understanding by Generative Pre-Training
**Autores:** Alec Radford et al. (OpenAI)
**Ano:** 2018
**P√°ginas:** 12 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê PAPER SEMINAL

### Descri√ß√£o Detalhada:
O paper que **iniciou a era dos LLMs** com generative pre-training.

**Contribui√ß√µes Principais:**
1. **Generative Pre-training:** pr√©-treino n√£o-supervisionado em corpus gigante
2. **Discriminative Fine-tuning:** ajuste fino supervisionado para tarefas espec√≠ficas
3. **Task-agnostic architecture:** mesma arquitetura serve para m√∫ltiplas tarefas
4. **Transformer decoder:** uso exclusivo do decoder (n√£o encoder-decoder)

**M√©todo:**
- **Pr√©-treino:**
  - Dataset: BooksCorpus (7000 livros, ~1 GB de texto)
  - Objetivo: predi√ß√£o autorregressiva (next token prediction)
  - Modelo: Transformer decoder com 12 camadas, 117M par√¢metros
  - Treino: ~1 m√™s em 8 GPUs

- **Fine-tuning:**
  - Adapta para tarefas: classifica√ß√£o, Q&A, similaridade, inference
  - Apenas 1 camada linear adicional
  - Treino r√°pido (horas, n√£o dias)

**Resultados:**
- SOTA em 9 de 12 tasks testadas
- Provou que pr√©-treino generativo funciona!

**Por que √© essencial:**
- **FUNDA√á√ÉO** de GPT-2, GPT-3, ChatGPT
- Introduziu o paradigma pre-train + fine-tune
- Mostrou que unsupervised pre-training captura conhecimento geral

**Para a aula:**
- LEIA: Introdu√ß√£o + M√©todo + Resultados (20 min)
- USE: Figura da arquitetura, tabela de resultados
- EXPLIQUE: Por que generative pre-training funciona

---

## 7. 2019_GPT2_Unsupervised_Multitask_Learners.pdf
**Paper:** Language Models are Unsupervised Multitask Learners
**Autores:** Alec Radford et al. (OpenAI)
**Ano:** 2019
**P√°ginas:** 24 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê PAPER SEMINAL

### Descri√ß√£o Detalhada:
GPT-2 mostrou que **scale matters**: modelos maiores, mais dados = capacidades emergentes.

**Escalas:**
- GPT-2 Small: 117M par√¢metros
- GPT-2 Medium: 345M par√¢metros
- GPT-2 Large: 762M par√¢metros
- **GPT-2 XL: 1.5B par√¢metros** (o maior em 2019)

**Dataset - WebText:**
- 40 GB de texto
- 8 milh√µes de documentos
- Reddit links com 3+ upvotes
- Curado para qualidade

**Descobertas Revolucion√°rias:**
1. **Zero-shot learning:** GPT-2 resolve tarefas SEM fine-tuning!
2. **Emergent capabilities:** tradu√ß√£o, Q&A, summarization aparecem espontaneamente
3. **Scaling laws preview:** modelos maiores ‚Üí melhor performance

**Resultados:**
- SOTA em 7 de 8 tasks (zero-shot!)
- Gera√ß√£o de texto coerente (100+ tokens)
- Controverso na √©poca: "too dangerous to release" (inicialmente)

**Por que √© essencial:**
- Provou que **scale unlocks capabilities**
- Zero-shot learning emergiu naturalmente
- Inspirou a corrida por modelos maiores

**Para a aula:**
- LEIA: Introdu√ß√£o + Scaling results + Zero-shot results (30 min)
- USE: Gr√°ficos de scaling, exemplos de gera√ß√£o
- EXPLIQUE: Por que zero-shot funciona

---

## 8. 2020_GPT3_Few_Shot_Learners.pdf
**Paper:** Language Models are Few-Shot Learners
**Autores:** Tom Brown et al. (OpenAI)
**Ano:** 2020
**P√°ginas:** 75 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê PAPER SEMINAL

### Descri√ß√£o Detalhada:
GPT-3 levou scaling ao **extremo** e descobriu **in-context learning**.

**Scale Dram√°tico:**
- **175 BILH√ïES de par√¢metros** (117x maior que GPT-2!)
- 300 bilh√µes de tokens de treinamento
- $4.6 milh√µes de custo de treinamento estimado
- Imposs√≠vel de fine-tunar para maioria dos pesquisadores

**Dataset - Mixture:**
- Common Crawl (60%, filtrado)
- WebText2 (22%)
- Books1 + Books2 (16%)
- Wikipedia (3%)

**Descoberta Revolucion√°ria: In-Context Learning (ICL)**
- Modelo aprende de exemplos NO PROMPT (sem atualizar pesos!)
- Few-shot: 10-100 exemplos ‚Üí performance dram√°tica
- Zero-shot tamb√©m funciona bem
- Elimina necessidade de fine-tuning

**Capacidades Emergentes:**
- Aritm√©tica (2 + 2 = 4, at√© 3 d√≠gitos)
- Tradu√ß√£o de qualidade
- Gera√ß√£o de c√≥digo Python
- Racioc√≠nio simples
- Conclus√£o de padr√µes

**Resultados:**
- SOTA em muitas tasks (few-shot)
- Performance aumenta log-linear com scale
- Alguns tasks ainda limitados (racioc√≠nio complexo)

**Por que √© essencial:**
- **Mudou tudo:** eliminou fine-tuning obrigat√≥rio
- In-context learning √© a base dos LLMs modernos
- Provou scaling laws (mais = melhor)
- Inspirou toda a gera√ß√£o seguinte (Llama, PaLM, etc)

**Para a aula:**
- LEIA: Introdu√ß√£o + In-context learning + Scaling plots (50 min - skim)
- USE: Gr√°ficos de scaling, exemplos de ICL, tabelas de performance
- EXPLIQUE: Por que ICL funciona, limita√ß√µes

---

## 9. 2020_Scaling_Laws_Neural_LMs.pdf
**Paper:** Scaling Laws for Neural Language Models
**Autores:** Jared Kaplan et al. (OpenAI)
**Ano:** 2020
**P√°ginas:** 27 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê FOUNDATIONAL

### Descri√ß√£o Detalhada:
Paper que descobriu as **leis de escala** dos LLMs, guiando toda a pesquisa subsequente.

**Descobertas Principais:**

1. **Power Laws:**
   - Loss ‚àù N^(-Œ±) onde N = n√∫mero de par√¢metros
   - Perda diminui de forma previs√≠vel com scale
   - Rela√ß√£o vale por ORDENS DE MAGNITUDE

2. **Tr√™s Fatores de Scaling:**
   - **N (par√¢metros):** maior impacto
   - **D (dados):** importante, mas menos cr√≠tico
   - **C (compute):** pode ser trocado por N ou D

3. **Optimal Allocation:**
   - Dado compute budget fixo, como alocar entre N e D?
   - Resultado: escalar N e D juntos, mas N cresce mais r√°pido
   - Llama 3 405B: 15 trilh√µes de tokens (seguiu essa lei!)

4. **Sample Efficiency:**
   - Modelos maiores s√£o mais sample-efficient
   - GPT-3 175B aprende mais r√°pido que GPT-2 1.5B

5. **Transfer:**
   - Rela√ß√µes de scaling se transferem entre tasks
   - Previs√µes generaliz√°veis

**Implica√ß√µes Pr√°ticas:**
- **GPT-3:** treinou 175B par√¢metros baseado nessas leis
- **Llama 2:** 70B com mais tokens (optimal compute)
- **Chinchilla:** mostrou que muitos modelos estavam undertrained (2022)

**Limita√ß√µes (descobertas depois):**
- N√£o prev√™ capacidades emergentes (racioc√≠nio, etc)
- Satura√ß√£o em alguns regimes
- Data quality matters (n√£o apenas quantidade)

**Por que √© essencial:**
- Guiou decis√µes de TRILH√ïES de d√≥lares
- Base cient√≠fica para scaling
- Explica por que modelos ficam melhores

**Para a aula:**
- LEIA: Introdu√ß√£o + Power laws + Optimal compute allocation (30 min)
- USE: Gr√°ficos de power laws, equa√ß√µes
- EXPLIQUE: Trade-offs entre par√¢metros e dados

---

## 10. 2020_The_Pile_800GB_Dataset.pdf
**Paper:** The Pile: An 800GB Dataset of Diverse Text for Language Modeling
**Autores:** Leo Gao et al. (EleutherAI)
**Ano:** 2020 (atualizado 2021)
**P√°ginas:** 24 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê FOUNDATIONAL DATASET

### Descri√ß√£o Detalhada:
The Pile √© o dataset de pr√©-treino **open-source mais influente**.

**Composi√ß√£o (825 GB, 22 subconjuntos):**

| Component | Size | Description |
|-----------|------|-------------|
| Pile-CC | 227 GB | Common Crawl (web filtrado) |
| PubMed Central | 90 GB | Artigos cient√≠ficos biom√©dicos |
| Books3 | 101 GB | Livros (dom√≠nio p√∫blico + outros) |
| OpenWebText2 | 63 GB | Reddit links (como GPT-2) |
| ArXiv | 56 GB | Papers cient√≠ficos |
| GitHub | 95 GB | C√≥digo open-source |
| Stack Exchange | 32 GB | Q&A t√©cnico |
| USPTO | 22 GB | Patentes |
| FreeLaw | 51 GB | Documentos legais |
| PubMed Abstracts | 20 GB | Abstracts cient√≠ficos |
| + 12 outros | 68 GB | Diversos dom√≠nios |

**Caracter√≠sticas:**
- **Diversidade:** 22 dom√≠nios diferentes
- **Curado:** filtrado para qualidade
- **Reprodut√≠vel:** totalmente documentado
- **Open-source:** dispon√≠vel para todos

**Modelos Treinados com The Pile:**
- **GPT-Neo** (EleutherAI): 1.3B, 2.7B
- **GPT-J** (EleutherAI): 6B
- **Pythia suite** (EleutherAI): 70M at√© 12B
- **BLOOM** (BigScience): usado parcialmente

**Impacto:**
- Democratizou pesquisa em LLMs
- Benchmark padr√£o
- Inspirou datasets similares (RedPajama, etc)

**Controv√©rsias:**
- Books3: copyright issues
- GitHub: licensing ambiguities
- Removido do HuggingFace em 2023

**Por que √© essencial:**
- Mostra O QUE entra no pr√©-treino
- Dataset mais documentado
- Entender diversidade de dados

**Para a aula:**
- LEIA: Composi√ß√£o do dataset + ablations (20 min)
- USE: Tabela de componentes, gr√°ficos de diversidade
- EXPLIQUE: Por que diversidade importa

---

## 11. 2025_Common_Corpus_Ethical_Data.pdf
**Paper:** Common Corpus: An Open Ethical Alternative to Web Scraping
**Autores:** PleIAs initiative
**Ano:** 2025
**P√°ginas:** ~30 p√°ginas
**Import√¢ncia:** ‚≠ê EMERGING

### Descri√ß√£o Detalhada:
Common Corpus √© uma alternativa **√©tica e legal** ao Common Crawl.

**Motiva√ß√£o:**
- Common Crawl: copyright unclear, PII issues, opt-out dif√≠cil
- Lawsuits contra OpenAI, Meta por uso de dados sem permiss√£o
- Necessidade de datasets √©ticos

**Caracter√≠sticas:**

1. **Scale:**
   - **2 trilh√µes de tokens** (compar√°vel ao Llama 3)
   - ~500 l√≠nguas
   - Multil√≠ngue

2. **Ethical:**
   - Apenas conte√∫do com licen√ßa permissiva
   - Dom√≠nio p√∫blico + CC licenses
   - PII removido
   - Respeita opt-out

3. **Composi√ß√£o:**
   - Wikipedia (todas as l√≠nguas)
   - Livros dom√≠nio p√∫blico (Project Gutenberg)
   - ArXiv papers (permiss√£o expl√≠cita)
   - Government documents (p√∫blicos)
   - Code com licen√ßas permissivas

4. **Qualidade:**
   - Filtrado para qualidade
   - Deduplicado
   - Toxic content removido

**Resultados:**
- Modelos treinados: performance compar√°vel
- Mais seguro legalmente
- Melhor para fine-tuning (menos toxicity)

**Limita√ß√µes:**
- Menor diversidade que Common Crawl
- Vi√©s para conte√∫do em ingl√™s
- Menos conte√∫do recente

**Por que √© relevante:**
- Futuro dos datasets de LLMs
- Quest√µes legais e √©ticas
- Alternative para pesquisadores

**Para a aula:**
- LEIA: Motiva√ß√£o + Composi√ß√£o + Trade-offs (20 min)
- USE: Compara√ß√£o com Common Crawl
- DISCUTA: √âtica em datasets de LLMs

---

# üîß PAPERS DE FINE-TUNING

## 12. 2017_PPO_Proximal_Policy_Optimization.pdf
**Paper:** Proximal Policy Optimization Algorithms
**Autores:** John Schulman et al. (OpenAI)
**Ano:** 2017
**P√°ginas:** 12 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê FOUNDATIONAL (RL)

### Descri√ß√£o Detalhada:
PPO √© o **algoritmo de RL usado no RLHF** (ChatGPT, Claude, etc).

**Problema que PPO resolve:**
- Policy gradient methods s√£o inst√°veis (divergem facilmente)
- TRPO √© est√°vel mas complexo
- Necessidade: algoritmo simples E est√°vel

**Ideia Principal:**
- **Clipped objective:** limita quanto a policy pode mudar por update
- Evita updates destrutivos
- Simples de implementar

**Algoritmo:**
```
1. Coleta trajectories com policy atual
2. Computa advantage estimates
3. Otimiza objective clipped por K epochs
4. Repete
```

**Por que PPO funciona bem:**
- **Sample efficiency:** reutiliza trajectories
- **Stability:** clipping previne diverg√™ncia
- **Simplicity:** f√°cil de implementar e tunar

**Uso em RLHF:**
1. SFT model = initial policy
2. Reward model d√° rewards
3. PPO otimiza policy para maximizar reward
4. Clipping garante que policy n√£o diverge do SFT model

**Hiperpar√¢metros Cr√≠ticos:**
- Œµ (clipping): 0.1-0.2 t√≠pico
- K (epochs): 3-10
- Learning rate: 1e-5 a 3e-5

**Alternativas:**
- A2C/A3C (menos est√°vel)
- TRPO (mais complexo)
- DPO (evita RL completamente!)

**Por que √© essencial:**
- Algoritmo por tr√°s do ChatGPT
- Funda√ß√£o do RLHF
- Entender para entender RLHF

**Para a aula:**
- LEIA: Introdu√ß√£o + Clipped objective (15 min)
- USE: Pseudoc√≥digo, intui√ß√£o do clipping
- EXPLIQUE: Por que estabilidade importa em RLHF

---

## 13. 2021_LoRA_Low_Rank_Adaptation.pdf
**Paper:** LoRA: Low-Rank Adaptation of Large Language Models
**Autores:** Edward Hu et al. (Microsoft)
**Ano:** 2021
**P√°ginas:** 14 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê HIGHLY INFLUENTIAL

### Descri√ß√£o Detalhada:
LoRA revolucionou fine-tuning com **parameter-efficient methods**.

**Problema:**
- Fine-tuning completo de GPT-3 175B: imposs√≠vel para 99% dos pesquisadores
- Cada tarefa = novo modelo completo (175B params)
- Memory e compute proibitivos

**Solu√ß√£o LoRA:**
- **Freezar todos os pesos** do modelo pr√©-treinado
- Adicionar matrizes de **baixo rank** A, B nos layers
- Apenas treinar A, B (0.1% dos par√¢metros!)

**Matem√°tica:**
```
W = W‚ÇÄ + ŒîW
ŒîW = A √ó B

Onde:
- W‚ÇÄ ‚àà ‚Ñù^(d√ók) (frozen)
- A ‚àà ‚Ñù^(d√ór), B ‚àà ‚Ñù^(r√ók) (trainable)
- r << min(d,k) (rank baixo, ex: r=8)
```

**Exemplo Num√©rico:**
- GPT-3 175B: 175 bilh√µes de par√¢metros
- LoRA r=8: **10.7 milh√µes de par√¢metros trein√°veis** (0.006%)
- Redu√ß√£o: 10,000x em par√¢metros!

**Vantagens:**
1. **Memory:** 3x menos GPU memory que fine-tuning completo
2. **Velocidade:** treino 25% mais r√°pido
3. **Storage:** m√∫ltiplas adapta√ß√µes = apenas m√∫ltiplos arquivos pequenos (10-100MB cada)
4. **Performance:** igual ou melhor que fine-tuning completo
5. **Switching:** trocar adapta√ß√µes em tempo real (sem recarregar modelo)

**Onde aplicar LoRA:**
- Query, Key, Value projections (Q, K, V)
- Tipicamente: apenas Q, V (suficiente)
- Rank r: 4-64 (8 √© comum)

**Resultados:**
- GPT-3 175B com LoRA: SOTA em v√°rios benchmarks
- RoBERTa, DeBERTa: performance superior
- Democratizou fine-tuning

**Impacto:**
- Usado em Stable Diffusion (text-to-image)
- Hugging Face PEFT library
- Base para QLoRA, LoRA+, etc

**Por que √© essencial:**
- Democratizou fine-tuning de LLMs
- M√©todo padr√£o hoje
- Trade-off ideal: performance vs efici√™ncia

**Para a aula:**
- LEIA: Introdu√ß√£o + M√©todo + Resultados (25 min)
- USE: Diagrama de LoRA, compara√ß√£o de par√¢metros
- EXPLIQUE: Por que baixo rank funciona (intrinsic dimensionality)

---

## 14. 2022_InstructGPT_Training_Follow_Instructions.pdf
**Paper:** Training language models to follow instructions with human feedback
**Autores:** Long Ouyang et al. (OpenAI)
**Ano:** 2022 (publicado em 2022, base do ChatGPT lan√ßado em Nov 2022)
**P√°ginas:** 68 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê PAPER MAIS IMPORTANTE (ChatGPT)

### Descri√ß√£o Detalhada:
InstructGPT √© o paper por tr√°s do **ChatGPT**. Mostrou como alinhar LLMs com inten√ß√µes humanas.

**Problema:**
- GPT-3 √© poderoso mas n√£o alinhado
- Gera texto t√≥xico, falso, in√∫til
- N√£o segue instru√ß√µes confiablemente

**Solu√ß√£o: RLHF Pipeline**

**Step 1: Supervised Fine-Tuning (SFT)**
- Coleta 13k prompts de usu√°rios reais (API)
- Humanos escrevem respostas ideais
- Fine-tune GPT-3 nos pares (prompt, resposta ideal)
- Resultado: SFT model (melhor que GPT-3, mas n√£o √≥timo)

**Step 2: Reward Model (RM)**
- Gera 4-9 respostas por prompt com SFT model
- Humanos ranqueiam respostas (melhor ‚Üí pior)
- Treina reward model para prever ranking
- Resultado: RM que prediz "qualidade" de respostas (6B params)

**Step 3: Reinforcement Learning (PPO)**
- Usa RM como fun√ß√£o de reward
- PPO otimiza SFT model para maximizar reward
- KL penalty garante n√£o divergir muito do SFT model
- Resultado: InstructGPT (aligned model)

**Dataset:**
- 13k SFT examples (demonstration)
- 33k comparison pairs (ranking)
- Labelers: ~40 contratados, treinados

**Resultados Surpreendentes:**
- **InstructGPT 1.3B > GPT-3 175B** em prefer√™ncia humana
- 135x menor, mas preferido!
- Menos t√≥xico (25% ‚Üì)
- Mais truthful
- Menos hallucinations

**Limita√ß√µes:**
- Ainda falha em complex reasoning
- Pode ser over-cautious (refuses harmless requests)
- Alignment tax: piora em alguns benchmarks tradicionais

**Por que √© essencial:**
- **Funda√ß√£o do ChatGPT** (mesmo m√©todo)
- Provou que RLHF funciona em scale
- Paradigma: pr√©-treino + alignment
- Inspirou toda a ind√∫stria

**Para a aula:**
- LEIA: Introdu√ß√£o + M√©todo completo + Resultados (60 min - paper longo mas essencial)
- USE: Diagramas do pipeline de 3 steps, gr√°ficos de prefer√™ncia
- EXPLIQUE: Por que RLHF funciona, limita√ß√µes

---

## 15. 2022_Constitutional_AI_Harmlessness.pdf
**Paper:** Constitutional AI: Harmlessness from AI Feedback
**Autores:** Yuntao Bai et al. (Anthropic)
**Ano:** 2022 (Dezembro)
**P√°ginas:** 67 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê HIGHLY INFLUENTIAL (Anthropic's approach)

### Descri√ß√£o Detalhada:
Constitutional AI (CAI) √© a abordagem da Anthropic (criadores do Claude) para alignment.

**Problema com RLHF:**
- Requer 10,000+ labels humanos (caro: $50k-100k+)
- Vi√©s dos labelers
- Slow iteration (precisa re-coletar labels)
- Hard to scale

**Solu√ß√£o: RLAIF (RL from AI Feedback)**
- Replace humanos por AI em grande parte do processo
- Define "constitution": princ√≠pios de comportamento
- AI critica e revisa suas pr√≥prias respostas

**M√©todo CAI (2 fases):**

**Phase 1: Supervised (Constitutional) Fine-Tuning**
1. Gera resposta potencialmente harmful
2. AI critica baseado em princ√≠pio constitucional
   - Ex: "Esta resposta √© racista?"
3. AI revisa para alinhar com princ√≠pio
4. Repete com m√∫ltiplos princ√≠pios
5. Fine-tune no dataset revisado

**Phase 2: RL from AI Feedback (RLAIF)**
1. Gera pares de respostas
2. **AI julga** qual √© melhor (baseado em constitution)
3. Treina reward model nas prefer√™ncias da AI (n√£o humanos!)
4. RL (PPO) para otimizar policy

**"Constitution" (exemplo de princ√≠pios):**
1. "Please choose the response that is most helpful, honest, and harmless."
2. "Which response is less racist?"
3. "Which response avoids being threatening or aggressive?"
4. ... (16 princ√≠pios no paper)

**Vantagens:**
- **10x mais barato:** humanos apenas para constitution (16 princ√≠pios) n√£o para 10k+ labels
- **Transparente:** constitution √© expl√≠cita (n√£o "black box" de prefer√™ncias)
- **Iter√°vel:** mudar constitution sem re-coletar data
- **Scalable:** AI feedback √© gr√°tis

**Resultados:**
- Performance compar√°vel ao RLHF tradicional
- Menos harmful (mais aligned com harmlessness)
- Base do Claude (Anthropic's chatbot)

**Trade-offs:**
- AI feedback pode ter vieses do modelo base
- Constitution requer design cuidadoso
- Menos "natural" que prefer√™ncias humanas

**Por que √© essencial:**
- Alternativa pr√°tica ao RLHF
- M√©todo usado pela Anthropic (Claude)
- Demonstra que AI feedback funciona
- Inspirou RLAIF como categoria

**Para a aula:**
- LEIA: Introdu√ß√£o + M√©todo CAI + Compara√ß√£o com RLHF (40 min)
- USE: Constitution examples, diagrama de 2 fases
- EXPLIQUE: Trade-offs RLHF vs CAI

---

## 16. 2022_FLAN_Scaling_Instruction_Finetuned.pdf
**Paper:** Scaling Instruction-Finetuned Language Models
**Autores:** Hyung Won Chung et al. (Google)
**Ano:** 2022 (Outubro)
**P√°ginas:** 59 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê HIGHLY INFLUENTIAL

### Descri√ß√£o Detalhada:
FLAN (Finetuned Language Net) mostrou que **instruction tuning escala** dramaticamente.

**Ideia Principal:**
- Fine-tune em MIX MASSIVO de tarefas formatadas como instru√ß√µes
- Modelo aprende a seguir instru√ß√µes de forma geral
- Zero-shot em tarefas novas

**Evolu√ß√£o:**
- **FLAN (2021):** 62 tasks, LaMDA 137B
- **FLAN-T5 (2022):** 1,836 tasks, T5/PaLM
- **FLAN-PaLM (2022):** 540B parameters

**Dataset - Flan Collection:**
- **1,836 tasks** de 4 mixtures:
  - Muffin (previos instruction tuning datasets)
  - T0 (zero-shot tasks)
  - NIV2 (1,600+ tasks!)
  - Chain-of-thought (CoT reasoning)
- **473 datasets** √∫nicos
- **146 categorias** de tarefas

**Scaling Insights:**

1. **Model Scale:**
   - FLAN melhora mais em modelos maiores
   - PaLM 540B + FLAN: SOTA em m√∫ltiplas tasks

2. **Task Scale:**
   - Mais tasks = melhor generaliza√ß√£o
   - 1,836 tasks >> 62 tasks originais

3. **Instruction Templates:**
   - M√∫ltiplos templates por task (10+ varia√ß√µes)
   - Aumenta robustez

**Resultados:**
- **MMLU:** 75.2% (FLAN-PaLM 540B) vs 69.3% (PaLM 540B)
- **BBH (reasoning):** 57.9% vs 44.1% (HUGE improvement!)
- Zero-shot em novas tasks: dram√°tico improvement

**Por que funciona:**
- Exp√µe modelo a ENORME variedade de tarefas
- Aprende "meta-skill" de seguir instru√ß√µes
- Transfer learning em n√≠vel de instru√ß√µes

**Formato de Instru√ß√£o:**
```
Template: "Read this and answer: {passage}\n{question}"
Template: "Answer based on context:\n\n{passage}\n\n{question}"
Template: "{passage}\n\nQ: {question}\nA:"
... (10+ varia√ß√µes)
```

**Por que √© essencial:**
- M√©todo padr√£o para instruction tuning hoje
- Llama 2, Mistral, etc usam FLAN collection
- Mostra que diversidade de tasks > quantidade de data em uma task

**Para a aula:**
- LEIA: Introdu√ß√£o + Scaling results + Dataset composition (35 min)
- USE: Gr√°ficos de scaling, tabela de tasks
- EXPLIQUE: Por que diversidade de instru√ß√µes importa

---

## 17. 2023_FLAN_Collection_Instruction_Tuning.pdf
**Paper:** The Flan Collection: Designing Data and Methods for Effective Instruction Tuning
**Autores:** Shayne Longpre et al. (Google)
**Ano:** 2023
**P√°ginas:** 38 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê DATASET PAPER

### Descri√ß√£o Detalhada:
Paper que documenta em detalhe a **Flan Collection**, o dataset de instruction tuning mais usado.

**Contribui√ß√µes:**
1. **Dataset Release:** Flan Collection p√∫blico no HuggingFace
2. **Design Principles:** como construir datasets de instruction tuning
3. **Ablations:** o que importa (e o que n√£o importa)

**Flan Collection Detalhada:**

| Component | Tasks | Examples | Description |
|-----------|-------|----------|-------------|
| Muffin | 62 | 1.8M | Original FLAN tasks |
| T0 | 193 | 19M | T0++ datasets |
| NIV2 | 1,554 | 5M | Natural Instructions v2 |
| CoT | 9 | 0.1M | Chain-of-thought reasoning |
| Dialog | 13 | 0.3M | Conversational tasks |
| **Total** | **1,836** | **25M+** | Mixed |

**Design Principles:**

1. **Task Diversity:**
   - M√∫ltiplas categorias (QA, NLI, summarization, etc)
   - Evitar dom√≠nio √∫nico

2. **Template Diversity:**
   - 10+ templates por task
   - Aumenta robustness

3. **Input Inversion:**
   - "What is capital of France?" ‚Üí "Paris is capital of which country?"
   - Dobra dataset size, melhora understanding

4. **Balancing:**
   - Limite por task (para evitar domina√ß√£o)
   - Mix estrat√©gico

**Ablations (o que importa):**
- ‚úÖ **Task diversity:** muito importante
- ‚úÖ **Template diversity:** importante
- ‚úÖ **Input inversion:** helpful
- ‚ùå **Example ordering:** n√£o importa muito
- ‚ùå **Few-shot examples:** opcional (zero-shot suficiente)

**Uso Pr√°tico:**
- Dataset dispon√≠vel: `google/flan_v2` no HuggingFace
- Usado para treinar Flan-T5, Flan-PaLM, Flan-UL2
- Community usa para treinar modelos open-source

**Por que √© relevante:**
- Documenta o dataset padr√£o de instruction tuning
- Design principles aplic√°veis
- Recurso pr√°tico

**Para a aula:**
- LEIA: Dataset composition + Design principles + Ablations (25 min)
- USE: Tabela de componentes, design principles
- PR√ÅTICO: Mostrar exemplos do dataset no HuggingFace

---

## 18. 2023_DPO_Direct_Preference_Optimization.pdf
**Paper:** Direct Preference Optimization: Your Language Model is Secretly a Reward Model
**Autores:** Rafael Rafailov et al. (Stanford)
**Ano:** 2023 (Maio)
**P√°ginas:** 28 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê HIGHLY INFLUENTIAL (simplifica RLHF)

### Descri√ß√£o Detalhada:
DPO √© a **simplifica√ß√£o revolucion√°ria do RLHF** que est√° se tornando o padr√£o.

**Problema com RLHF:**
- Pipeline complexo: SFT ‚Üí Reward Model ‚Üí RL (PPO)
- Treinar reward model separado (6B params)
- PPO √© inst√°vel (requer tuning cuidadoso)
- 3 modelos em mem√≥ria simultaneamente
- Expensive e lento

**Insight de DPO:**
- Reward model pode ser expresso em termos da policy DIRETAMENTE
- **Elimina reward model e RL!**
- Treina policy diretamente nas prefer√™ncias

**Matem√°tica (simplificado):**

RLHF objetivo:
```
max E[r(x,y)] - Œ≤ KL(œÄ || œÄ_ref)
```

DPO descobre que a policy √≥tima satisfaz:
```
œÄ*(y|x) ‚àù œÄ_ref(y|x) ¬∑ exp(r(x,y) / Œ≤)
```

Reorganizando:
```
r(x,y) = Œ≤ log(œÄ*(y|x) / œÄ_ref(y|x))
```

**DPO Loss:**
```
L = -E[log œÉ(Œ≤ log(œÄ(y_w|x)/œÄ_ref(y_w|x)) - Œ≤ log(œÄ(y_l|x)/œÄ_ref(y_l|x)))]

Onde:
- y_w = resposta preferida (chosen)
- y_l = resposta rejeitada
- œÉ = sigmoid
```

**Pipeline Simplificado:**

RLHF:
1. SFT
2. Train reward model (collect comparisons, train 6B model)
3. RL with PPO (complex, unstable)

DPO:
1. SFT
2. **DPO diretamente** (single training run!)

**Vantagens:**
- **10x mais simples:** 1 training run, n√£o 3
- **Est√°vel:** n√£o requer PPO tuning
- **Memory:** 2x menos GPU memory
- **R√°pido:** training 2-3x mais r√°pido
- **Performance:** igual ou melhor que RLHF

**Resultados:**
- Controlado summarization: 58% win rate vs RLHF
- Sentiment: compar√°vel
- Dialog: melhor que SFT, compar√°vel a RLHF
- Adoption: Llama 3 usou DPO (n√£o RLHF!)

**Limita√ß√µes:**
- Requer preference data (como RLHF)
- Menos controle fino que separar reward model
- Pode overfit a dataset pequenos

**Por que √© essencial:**
- **Futuro do alignment:** mais simples = melhor
- Llama 3, Mistral, outros usam DPO
- Elimina complexidade de RLHF
- Trade-off ideal: simplicidade vs performance

**Para a aula:**
- LEIA: Introdu√ß√£o + Deriva√ß√£o + Algoritmo + Compara√ß√£o com RLHF (35 min)
- USE: Diagrama comparando RLHF vs DPO, algoritmo
- EXPLIQUE: Por que DPO funciona, quando usar DPO vs RLHF

---

## 19. 2023_QLoRA_Efficient_Finetuning_Quantized.pdf
**Paper:** QLoRA: Efficient Finetuning of Quantized LLMs
**Autores:** Tim Dettmers, Artidoro Pagnoni et al.
**Ano:** 2023 (Maio)
**P√°ginas:** 25 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê HIGHLY PRACTICAL (democratiza√ß√£o)

### Descri√ß√£o Detalhada:
QLoRA leva efici√™ncia ao **extremo**: fine-tune 65B em **1 GPU** de 48GB!

**Problema:**
- LoRA: 10,000x menos par√¢metros, mas ainda requer modelo em 16-bit
- LLaMA 65B em 16-bit: 130 GB de GPU memory (imposs√≠vel em 1 GPU)
- Solu√ß√£o √≥bvia: quantizar para 4-bit? Mas degrada performance...

**Inova√ß√µes de QLoRA:**

**1. 4-bit NormalFloat (NF4):**
- Weights seguem distribui√ß√£o normal
- NF4: quantiza√ß√£o √≥tima para distribui√ß√£o normal
- Preserva informa√ß√£o melhor que int4 ou float4 simples

**2. Double Quantization:**
- Quantiza os pesos: 16-bit ‚Üí 4-bit (normal)
- **Quantiza tamb√©m as constantes de quantiza√ß√£o!**
- Economia adicional: 0.37 bits/param em m√©dia

**3. Paged Optimizers:**
- Gerencia memory spikes (durante gradient computation)
- Usa CPU memory como overflow (paging autom√°tico)
- Permite treinar modelos que "n√£o caberiam"

**Resultado:**
- **LLaMA 65B em 48GB de GPU** (single A100)
- Performance igual a 16-bit LoRA!
- 4x menos memory que LoRA normal

**Guanaco Models:**
- Fine-tuned LLaMA com QLoRA
- Dataset: OASST1 (OpenAssistant, 10k conversations)
- **Guanaco-65B:** 99.3% da performance do ChatGPT no Vicuna benchmark
- Training: **24 horas em 1 GPU**

**Compara√ß√£o:**

| Method | LLaMA 65B Memory | Performance | Cost |
|--------|------------------|-------------|------|
| Full FT | 1300 GB | 100% | $100k+ |
| LoRA 16-bit | 160 GB | 99.9% | $5k |
| QLoRA 4-bit | **48 GB** | 99.8% | **$500** |

**Impacto:**
- Democratizou fine-tuning de modelos grandes
- Qualquer pesquisador com 1 GPU pode fine-tunar 65B+
- Usado em Hugging Face PEFT library
- Base para treinar milhares de modelos open-source

**Limita√ß√µes:**
- Training 2-3x mais lento que 16-bit (quantiza√ß√£o overhead)
- 4-bit inference tamb√©m requer adapta√ß√µes

**Por que √© essencial:**
- Democratiza√ß√£o extrema
- Mostra que quantiza√ß√£o agressiva pode preservar quality
- M√©todo padr√£o para fine-tuning com recursos limitados

**Para a aula:**
- LEIA: Introdu√ß√£o + NF4 + Double Quantization + Resultados (30 min)
- USE: Tabela de memory usage, resultados do Guanaco
- EXPLIQUE: Trade-offs memory vs speed vs quality
- PR√ÅTICO: Como usar QLoRA na pr√°tica

---

## 20. 2023_RLAIF_AI_Feedback.pdf
**Paper:** RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
**Autores:** Harrison Lee et al. (Google)
**Ano:** 2023 (Setembro)
**P√°ginas:** 23 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê PRACTICAL ALTERNATIVE

### Descri√ß√£o Detalhada:
RLAIF (RL from AI Feedback) escala alignment usando **AI labels** ao inv√©s de humanos.

**Problema com RLHF:**
- Requer 10,000+ human labels
- Custo: $50k-100k+ para dataset
- Tempo: semanas para coletar
- N√£o escala para m√∫ltiplas tarefas

**Solu√ß√£o RLAIF:**
- Replace humans por **LLM as a judge**
- Prompt LLM para julgar qual resposta √© melhor
- Train reward model nas prefer√™ncias da AI
- **10-100x mais barato, 10x mais r√°pido**

**M√©todo:**

1. **Generate comparisons:**
   - Gera pares de respostas
   - Prompt LLM juiz: "Which response is better? A or B?"
   - Coleta 10k+ judgments (autom√°tico!)

2. **Train reward model:**
   - Mesmo que RLHF, mas labels da AI

3. **RL (PPO):**
   - Id√™ntico ao RLHF

**Prompt para LLM Judge (exemplo):**
```
Task: Summarize the following article.

Article: {article}

Response A: {summary_A}
Response B: {summary_B}

Which response is a better summary? Consider:
- Accuracy
- Conciseness
- Fluency

Answer: (A or B)
```

**Resultados (Summarization):**
- RLAIF vs RLHF: **71% agreement**
- Human preference: RLAIF = 79%, RLHF = 72% (RLAIF ganha!)
- Custo: **$100 vs $10,000**

**Quando RLAIF funciona bem:**
- ‚úÖ Tarefas objetivas (summarization, factuality)
- ‚úÖ Quando h√° crit√©rios claros
- ‚úÖ LLM juiz √© suficientemente capaz

**Limita√ß√µes:**
- ‚ùå Tarefas subjetivas (humor, tone)
- ‚ùå LLM juiz pode ter vieses
- ‚ùå "Cheating": modelo pode aprender a manipular juiz

**Variantes:**
- **Self-critique:** usar mesmo modelo como juiz
- **Constitution AI:** especificar princ√≠pios expl√≠citos
- **Hybrid:** humanos + AI

**Por que √© relevante:**
- **Scalability:** treinar modelos para 100+ tasks (custo vi√°vel)
- Itera√ß√£o r√°pida
- Futuro: AI feedback > human feedback?

**Para a aula:**
- LEIA: Introdu√ß√£o + M√©todo + Compara√ß√£o com RLHF (20 min)
- USE: Exemplo de prompt para judge, compara√ß√£o de custos
- DISCUTA: Trade-offs RLHF vs RLAIF, quando usar cada um

---

## 21. 2025_Massive_SFT_Experiments.pdf
**Paper:** Supervised Fine-Tuning at Scale: Lessons from 1000+ Models
**Autores:** Research team
**Ano:** 2025
**P√°ginas:** ~40 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê EMPIRICAL INSIGHTS

### Descri√ß√£o Detalhada:
Paper emp√≠rico massivo que treinou **1000+ modelos SFT** para entender o que funciona.

**Motiva√ß√£o:**
- SFT √© primeiro passo em todos os pipelines (antes de RLHF/DPO)
- Muitas decis√µes emp√≠ricas (learning rate, epochs, dataset size, etc)
- Faltam ablations sistem√°ticos

**Escopo do Estudo:**
- **1000+ modelos** treinados
- Varia√ß√µes: 7B, 13B, 70B parameters
- Datasets: 1k at√© 1M examples
- Hyperparameters: 100+ combina√ß√µes

**Findings Principais:**

**1. Dataset Size:**
- **Qualidade > Quantidade** (at√© certo ponto)
- 10k high-quality examples > 100k low-quality
- Saturation: ~100k examples para 7B, ~1M para 70B
- Curva em S: pouco ganho ap√≥s satura√ß√£o

**2. Data Quality:**
- Diversity importa: 10k diverse > 50k similar
- Length: preferir exemplos 100-500 tokens (nem muito curtos, nem muito longos)
- Formato: instru√ß√µes claras > conversa√ß√£o vaga

**3. Epochs:**
- **1-3 epochs:** sweet spot
- >3 epochs: overfitting em small datasets
- Large datasets: 1 epoch suficiente

**4. Learning Rate:**
- 1e-5 a 5e-5: faixa segura
- 2e-5: default bom para maioria
- Maior LR: risco de catastrofic forgetting

**5. Model Size:**
- Modelos maiores aprendem mais r√°pido (few epochs)
- Modelos menores: precisam mais epochs ou mais data

**6. Forgetting:**
- SFT causa forgetting de capacidades do base model
- Mitigar: mix pre-training data (5-10%)
- Replay: misturar samples do base model

**7. Task Composition:**
- Multi-task SFT > single-task (generaliza√ß√£o)
- Balancing: limitar cada task a 10-20k examples

**Recomenda√ß√µes Pr√°ticas:**

Para 7B model:
- 10k-50k high-quality examples
- 2-3 epochs
- LR: 2e-5
- Mix 10% pre-training data

Para 70B model:
- 100k-1M examples
- 1-2 epochs
- LR: 1e-5
- Mix 5% pre-training data

**Por que √© relevante:**
- Guia pr√°tico para SFT
- Economiza tentativa-e-erro
- Baseado em escala massiva (1000+ models)

**Para a aula:**
- LEIA: Key findings + Recommendations (25 min)
- USE: Gr√°ficos de scaling, recomenda√ß√µes pr√°ticas
- PR√ÅTICO: Como aplicar em seus pr√≥prios experimentos

---

# üåê MODELOS OPEN-SOURCE

## 22. 2023_LLaMA1_Open_Efficient_Foundation.pdf
**Paper:** LLaMA: Open and Efficient Foundation Language Models
**Autores:** Hugo Touvron et al. (Meta AI)
**Ano:** 2023 (Fevereiro)
**P√°ginas:** 27 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê FOUNDATIONAL (democratiza√ß√£o)

### Descri√ß√£o Detalhada:
LLaMA 1 **democratizou LLMs de qualidade** ao provar que modelos menores, bem treinados, superam gigantes.

**Filosofia:**
- Focar em **training tokens**, n√£o apenas par√¢metros
- Chinchilla scaling laws: modelos eram undertrained
- Melhor: modelo menor + mais tokens

**Modelos Lan√ßados:**

| Model | Params | Tokens | Training Compute |
|-------|--------|--------|------------------|
| LLaMA-7B | 7B | 1.0T | 82k GPU-hours |
| LLaMA-13B | 13B | 1.0T | 135k GPU-hours |
| LLaMA-33B | 33B | 1.4T | 530k GPU-hours |
| LLaMA-65B | 65B | 1.4T | 1M GPU-hours |

**Dataset (1.4 Trillion tokens):**

| Source | Percentage | Tokens | Description |
|--------|------------|--------|-------------|
| CommonCrawl | 67% | 938B | Web (filtrado) |
| C4 | 15% | 210B | Colossal Clean Crawled |
| GitHub | 4.5% | 63B | C√≥digo |
| Wikipedia | 4.5% | 63B | 20 l√≠nguas |
| Books | 4.5% | 63B | Gutenberg + Books3 |
| ArXiv | 2.5% | 35B | Papers cient√≠ficos |
| StackExchange | 2% | 28B | Q&A t√©cnico |

**Arquitetura (melhorias sobre GPT-3):**
- RMSNorm (mais eficiente que LayerNorm)
- SwiGLU activation (em vez de ReLU)
- Rotary Positional Embeddings (RoPE)
- Attention optimizations

**Resultados Surpreendentes:**
- **LLaMA-13B > GPT-3 175B** em muitos benchmarks!
- LLaMA-65B: competitivo com Chinchilla-70B e PaLM-540B
- Efici√™ncia: menor, mais r√°pido, open-source

**Performance:**

| Benchmark | LLaMA-65B | GPT-3 175B |
|-----------|-----------|------------|
| MMLU (5-shot) | 63.4% | 70.0% |
| HellaSwag | 84.2% | 78.9% |
| PIQA | 82.8% | 81.0% |
| ARC | 80.0% | 85.2% |

**Impacto:**
- Spawned ecosystem: Alpaca, Vicuna, Orca, etc
- Democratizou pesquisa em LLMs
- Provou: training > pure scale
- Open-source (weights liberados)

**Controv√©rsia:**
- Licen√ßa restritiva inicialmente (research only)
- Weights "leaked" para torrents
- Meta liberou ap√≥s press√£o da comunidade

**Por que √© essencial:**
- Ponto de virada: open-source competitivo
- Funda√ß√£o de centenas de modelos derivados
- Provou viabilidade de modelos menores

**Para a aula:**
- LEIA: Dataset composition + Training + Resultados (30 min)
- USE: Tabela de modelos, compara√ß√£o com GPT-3
- EXPLIQUE: Por que LLaMA-13B supera GPT-3 175B

---

## 23. 2023_Llama2_Open_Foundation_Fine_Tuned.pdf
**Paper:** Llama 2: Open Foundation and Fine-Tuned Chat Models
**Autores:** Hugo Touvron et al. (Meta AI)
**Ano:** 2023 (Julho)
**P√°ginas:** 77 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê FOUNDATIONAL (open ChatGPT alternative)

### Descri√ß√£o Detalhada:
Llama 2 √© a **resposta open-source ao ChatGPT**, com modelos base E chat alinhados.

**Modelos Lan√ßados:**

**Base models:**
- Llama 2 7B
- Llama 2 13B
- Llama 2 70B

**Chat models (fine-tuned + RLHF):**
- Llama 2 7B-Chat
- Llama 2 13B-Chat
- Llama 2 70B-Chat

**Pr√©-treino:**
- **2 TRILH√ïES de tokens** (40% mais que Llama 1)
- Context: 4k tokens
- Training: ~1.7M GPU-hours (70B model)

**Dataset Composition:**
- N√£o divulgado em detalhe (por seguran√ßa)
- Publicly available data
- Mais c√≥digo que Llama 1
- Upsampled factual sources

**Fine-tuning (Llama 2-Chat):**

**Phase 1: Supervised Fine-Tuning (SFT)**
- 27,540 high-quality examples
- Humanos escreveram respostas ideais
- Focuses: helpfulness, safety

**Phase 2: Reinforcement Learning (RLHF)**
- **5 iterations** de RLHF (n√£o apenas 1!)
- Cada itera√ß√£o:
  1. Coleta comparisons (100k+)
  2. Treina reward model
  3. PPO optimization
  4. Repete com novo modelo

**Safety Alignment:**
- **Llama 2-Chat √© o modelo open-source mais seguro**
- Safety reward model separado
- Adversarial testing (red teaming)
- < 1% unsafe responses

**Resultados:**

| Benchmark | Llama 2 70B | GPT-3.5 | PaLM-2 |
|-----------|-------------|---------|---------|
| MMLU | 68.9% | 70.0% | 78.3% |
| HellaSwag | 87.3% | 85.5% | 86.8% |
| HumanEval | 29.9% | 48.1% | - |

**Llama 2-Chat vs ChatGPT (human eval):**
- Helpfulness: 7.2/10 vs 7.0/10 (tie!)
- Safety: better than ChatGPT em alguns testes

**Licen√ßa:**
- **Open commercial license** (grande mudan√ßa!)
- Gratuito at√© 700M users
- Democratiza uso comercial

**Impacto:**
- Alternativa open-source vi√°vel ao ChatGPT
- Base de centenas de modelos (Mistral, etc)
- Padr√£o da ind√∫stria (Ollama, LM Studio, etc)

**Por que √© essencial:**
- Documenta RLHF completo (5 itera√ß√µes!)
- Open-source competitive chatbot
- Safety considerations
- Commercial viability

**Para a aula:**
- LEIA: Pre-training + RLHF iterations + Safety + Resultados (50 min)
- USE: Diagramas de RLHF, compara√ß√£o com ChatGPT, tabelas de safety
- EXPLIQUE: Por que 5 RLHF iterations (vs 1 em InstructGPT)

---

## 24. 2023_Mistral_7B.pdf
**Paper:** Mistral 7B
**Autores:** Albert Jiang et al. (Mistral AI)
**Ano:** 2023 (Outubro)
**P√°ginas:** 13 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê HIGHLY INFLUENTIAL (SOTA small model)

### Descri√ß√£o Detalhada:
Mistral 7B provou que **7B pode superar 13B** com arquitetura certa.

**Claim Principal:**
- **Mistral 7B > Llama 2 13B** em todos os benchmarks
- Melhor model-per-parameter

**Inova√ß√µes Arquiteturais:**

**1. Grouped-Query Attention (GQA):**
- Reduz KV cache (memory)
- Faster inference que Multi-Head Attention
- Minimal quality loss

**2. Sliding Window Attention (SWA):**
- Attention com window de 4096 tokens
- Reduz compute de O(n¬≤) para O(n√ów)
- Cada token atende apenas a 4096 tokens anteriores
- Mas informa√ß√£o propaga por camadas!

**Matem√°tica SWA:**
```
Layer 1: cada token v√™ 4096 tokens atr√°s
Layer 2: cada token v√™ 8192 tokens atr√°s (indiretamente!)
Layer N: cada token v√™ N√ó4096 tokens atr√°s

32 layers √ó 4096 = alcance te√≥rico de 131k tokens
```

**Treinamento:**
- N√£o divulgado em detalhe
- "Trained on diverse data"
- Context: 8k tokens (com SWA)

**Resultados:**

| Benchmark | Mistral 7B | Llama 2 7B | Llama 2 13B |
|-----------|------------|------------|-------------|
| MMLU | 62.5% | 45.3% | 55.0% |
| HellaSwag | 83.3% | 77.2% | 79.2% |
| ARC | 80.0% | 74.5% | 77.3% |
| GSM8K | 52.2% | 14.6% | 28.7% |

**Mistral 7B-Instruct:**
- Fine-tuned version
- Supera Llama 2 13B-Chat

**Efici√™ncia:**
- 2x faster inference que Llama 2 7B
- Menor memory footprint (GQA)

**Impacto:**
- Novo SOTA para 7B models
- Arquitetura influenciou modelos seguintes
- Mistral AI: startup francesa, agora valorizada em $6B

**Limita√ß√µes:**
- Training details n√£o divulgados
- Sem chat version t√£o segura quanto Llama 2

**Por que √© essencial:**
- Prova: arquitetura > scale
- GQA e SWA s√£o t√©cnicas importantes
- Benchmark para small models

**Para a aula:**
- LEIA: Arquitetura (GQA, SWA) + Resultados (20 min)
- USE: Diagrama de SWA, tabelas comparativas
- EXPLIQUE: Trade-offs SWA (efici√™ncia vs full attention)

---

## 25. 2024_Llama3_Herd_of_Models.pdf
**Paper:** The Llama 3 Herd of Models
**Autores:** Meta AI Team (200+ autores)
**Ano:** 2024 (Julho)
**P√°ginas:** 92 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê‚≠ê STATE-OF-THE-ART (open-source frontier)

### Descri√ß√£o Detalhada:
Llama 3 √© o **open-source model mais capaz** (at√© 2024), com 405B par√¢metros.

**Modelos Lan√ßados:**

**Text models:**
- Llama 3.1 8B (substitui 7B)
- Llama 3.1 70B
- **Llama 3.1 405B** (flagship)

**Multimodal models (vision):**
- Llama 3.2 11B Vision
- Llama 3.2 90B Vision

**Pr√©-treino (405B):**
- **15.6 TRILH√ïES de tokens** (record!)
- Context: **128k tokens** (scaling RoPE)
- Training: ~31M GPU-hours (H100)
- Cost estimado: **$50-100 milh√µes**

**Dataset:**
- N√£o divulgado em detalhe
- Web (filtrado agressivamente)
- C√≥digo (30%+)
- Matem√°tica (synthetic)
- Multilingue (mais l√≠nguas que Llama 2)

**Arquitetura:**
- GQA (Grouped-Query Attention)
- RoPE com frequency scaling (para 128k context)
- Standard Transformer (mais par√¢metros)

**Post-Training:**
- **SFT:** 10M+ instruction examples
- **DPO** (n√£o RLHF!): preference optimization direta
- 6 rounds de refinement
- Safety: adversarial testing em escala

**Resultados (405B):**

| Benchmark | Llama 3.1 405B | GPT-4 | Claude 3.5 |
|-----------|----------------|-------|------------|
| MMLU | 87.3% | 86.4% | 88.7% |
| GPQA | 51.1% | 50.6% | 59.4% |
| HumanEval | 89.0% | 90.2% | 92.0% |
| GSM8K | 96.8% | 95.3% | 96.4% |
| MATH | 73.8% | 64.5% | 71.1% |

**Capacidades Emergentes:**
- **128k context:** processa livros inteiros
- **Multilingue:** 8 l√≠nguas principais
- **Tool use:** function calling, agents
- **Reasoning:** competitivo com GPT-4

**Llama 3.1 70B:**
- **Supera GPT-3.5 Turbo** em quase tudo
- Vi√°vel para deployment (vs 405B)
- Sweet spot: performance vs cost

**Open Weights:**
- **Totalmente open-source** (pesos, c√≥digo, datasets parciais)
- Licen√ßa permissiva (commercial use)
- Maior democratiza√ß√£o

**Impacto:**
- Primeiro open model a competir com GPT-4
- Prova: open-source pode alcan√ßar frontier
- Base para pr√≥xima gera√ß√£o (Llama 4?)

**Por que √© essencial:**
- State-of-the-art open-source
- Documenta treinamento em escala (405B, 15T tokens)
- Usa DPO (n√£o RLHF)
- Mostra que open pode competir

**Para a aula:**
- LEIA: Pre-training + Post-training (DPO) + Resultados (60 min - paper longo)
- USE: Scaling plots, compara√ß√£o com GPT-4, tabelas de resultados
- EXPLIQUE: Por que DPO (vs RLHF do Llama 2)

---

## 26. 2024_Phi3_Highly_Capable_Small_Model.pdf
**Paper:** Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
**Autores:** Microsoft Research (Marah Abdin et al.)
**Ano:** 2024 (Abril)
**P√°ginas:** 22 p√°ginas
**Import√¢ncia:** ‚≠ê‚≠ê PRACTICAL (small but mighty)

### Descri√ß√£o Detalhada:
Phi-3 mostra que **data quality > size**: 3.8B que compete com 7-13B models.

**Filosofia:**
- Small Language Models (SLMs) para edge devices
- **Data quality matters more than quantity**
- Synthetic data de alta qualidade

**Modelos:**
- **Phi-3-mini:** 3.8B params, 3.3T tokens
- **Phi-3-small:** 7B params, 4.8T tokens
- **Phi-3-medium:** 14B params, 4.8T tokens

**Foco: Phi-3-mini (3.8B)**

**Dataset (n√£o p√∫blico):**
- **Heavily filtered web data:** qualidade > quantidade
- **Synthetic data gerado por LLMs:** reasoning, code, math
- "Textbook quality": dados did√°ticos, estruturados
- Multilingual

**Resultados (Phi-3-mini 3.8B):**

| Benchmark | Phi-3-mini | Llama 3 8B | Mistral 7B | GPT-3.5 |
|-----------|------------|------------|------------|---------|
| MMLU | 69% | 66% | 62% | 70% |
| HellaSwag | 76% | 82% | 83% | - |
| HumanEval | 59% | 62% | 40% | 48% |
| GSM8K | 83% | 79% | 52% | 57% |
| MT-Bench | 8.38 | - | - | ~8.0 |

**Phi-3-mini compete com Llama 3 8B e Mixtral 8x7B!**

**Deployment:**
- **3.8B roda em smartphones** (quantizado para 4-bit: ~2GB)
- iPhone 14: inference em tempo real
- Edge devices (IoT)

**Use Cases:**
- Personal assistants (privacy: on-device)
- Offline applications
- Low-latency requirements

**Limita√ß√µes:**
- Factual knowledge menor que modelos grandes
- Context: 4k tokens (vs 128k do Llama 3)
- Alguns tasks complexos: below 7-13B models

**Por que √© relevante:**
- Prova: data quality > model size
- SLMs s√£o vi√°veis (n√£o apenas LLMs)
- Future: edge AI
- Synthetic data de LLMs para treinar smaller models

**Para a aula:**
- LEIA: Data quality approach + Resultados + Deployment (20 min)
- USE: Compara√ß√£o de performance vs size, use cases
- EXPLIQUE: Trade-offs SLMs vs LLMs

---

# üìö COMO USAR ESTE √çNDICE

## Para Prepara√ß√£o R√°pida (2-3 horas):

**Leia estes 5 papers (ordem):**
1. GPT-1 (2018) - Funda√ß√£o do pr√©-treino generativo
2. InstructGPT (2022) - RLHF e ChatGPT
3. LoRA (2021) - Fine-tuning eficiente
4. DPO (2023) - Simplifica√ß√£o do RLHF
5. Llama 2 (2023) - Open-source completo

**Total:** ~2h30 de leitura focada

---

## Para Prepara√ß√£o Completa (1-2 semanas):

**Semana 1: Fundamentos**
- Dia 1-2: Pr√©-treino (GPT-1, GPT-2, GPT-3, Scaling Laws)
- Dia 3-4: Datasets (The Pile, Common Corpus, Datasets Survey)
- Dia 5-6: Open-source (Llama 1, Llama 2)

**Semana 2: Fine-tuning**
- Dia 7-8: RLHF (PPO, InstructGPT, RLHF Survey)
- Dia 9-10: Alternatives (DPO, RLAIF, Constitutional AI)
- Dia 11-12: Efficiency (LoRA, QLoRA)
- Dia 13-14: State-of-the-art (Llama 3, Mistral, Phi-3)

---

## Por T√≥pico Espec√≠fico:

**Pr√©-treino:**
- Papers: 6-11 (GPTs, Scaling Laws, Datasets)
- Surveys: 1, 3

**Fine-tuning:**
- Papers: 12-21
- Surveys: 2, 4, 5

**Open-source models:**
- Papers: 22-26

**Alignment:**
- Papers: 14 (InstructGPT), 15 (Constitutional AI), 18 (DPO), 20 (RLAIF)
- Surveys: 2, 4, 5

**Efficiency:**
- Papers: 13 (LoRA), 19 (QLoRA)

---

## Estrutura de Aula Sugerida (120 min):

**M√≥dulo 1: Pr√©-treino (35 min)**
- Hist√≥ria: GPT-1 ‚Üí GPT-2 ‚Üí GPT-3
- Scaling laws
- Datasets (The Pile, Common Corpus)

**M√≥dulo 2: Fine-tuning Methods (50 min)**
- SFT b√°sico
- RLHF pipeline (InstructGPT)
- Alternatives: DPO, RLAIF, Constitutional AI
- Efficiency: LoRA, QLoRA

**M√≥dulo 3: Open-source Revolution (25 min)**
- Llama series (1, 2, 3)
- Mistral 7B
- Phi-3

**M√≥dulo 4: Q&A e Discuss√£o (10 min)**

---

## Figuras Imprescind√≠veis para Slides:

1. **GPT-1:** Arquitetura de pre-training + fine-tuning
2. **Scaling Laws:** Gr√°ficos de power laws (loss vs params)
3. **InstructGPT:** Pipeline de 3 steps (SFT ‚Üí RM ‚Üí PPO)
4. **LoRA:** Diagrama de low-rank adaptation
5. **DPO:** Compara√ß√£o RLHF vs DPO (simplifica√ß√£o)
6. **Llama 3:** Tabelas de resultados vs GPT-4
7. **The Pile:** Composi√ß√£o dos 22 subconjuntos

---

## Conceitos-Chave para Cobrir:

‚úì Generative pre-training
‚úì Next token prediction
‚úì Scaling laws (power laws)
‚úì Training datasets (The Pile, etc)
‚úì Supervised Fine-Tuning (SFT)
‚úì Instruction tuning
‚úì RLHF pipeline (3 steps)
‚úì PPO (Proximal Policy Optimization)
‚úì Reward modeling
‚úì DPO (Direct Preference Optimization)
‚úì RLAIF (RL from AI Feedback)
‚úì Constitutional AI
‚úì LoRA (Low-Rank Adaptation)
‚úì QLoRA (Quantized LoRA)
‚úì Alignment tax
‚úì Catastrophic forgetting
‚úì Zero-shot, few-shot, in-context learning
‚úì Emergent capabilities

---

## Estat√≠sticas Importantes:

- GPT-1: 117M params, 1GB de texto
- GPT-2: 1.5B params, 40GB de texto
- GPT-3: 175B params, 300B tokens, $4.6M treino
- Llama 1: at√© 65B params, 1.4T tokens
- Llama 2: at√© 70B params, 2T tokens
- Llama 3: at√© 405B params, 15.6T tokens (!!)
- The Pile: 825GB, 22 subconjuntos
- InstructGPT: 13k SFT examples, 33k comparisons
- LoRA: 10,000x menos par√¢metros trein√°veis
- QLoRA: Fine-tune 65B em 48GB GPU

---

√öltima atualiza√ß√£o: 31 de outubro de 2025
Compilado por: Claude Code (Anthropic)
