â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘           âœ… DOWNLOAD COMPLETO - AULA 5: TREINAMENTO DE LLMs                  â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DATA: 31 de outubro de 2025
STATUS: âœ… TODOS OS DOWNLOADS COMPLETOS

==============================================================================
                              ðŸ“Š RESUMO ESTATÃSTICO
==============================================================================

TOTAL DE PDFs BAIXADOS: 26 papers cientÃ­ficos

DISTRIBUIÃ‡ÃƒO:
   ðŸ“Š 5  Surveys (2024-2025)
   ðŸŽ“ 6  Papers de PrÃ©-treino (2018-2025)
   ðŸ”§ 10 Papers de Fine-tuning (2017-2025)
   ðŸŒ 5  Papers de Modelos Open-Source (2023-2024)

VOLUME TOTAL: ~650 pÃ¡ginas de conteÃºdo cientÃ­fico

==============================================================================
                           ðŸ“ ESTRUTURA DE PASTAS
==============================================================================

ðŸ“‚ Aula 5/
   â”‚
   â”œâ”€â”€ ðŸ“‚ Surveys_2025/ (5 PDFs)
   â”‚   â”œâ”€â”€ 2024_Survey_Datasets_LLMs.pdf
   â”‚   â”‚   â””â”€â”€ 444 datasets, 774.5 TB catalogados
   â”‚   â”œâ”€â”€ 2024_Survey_DPO_Comprehensive.pdf
   â”‚   â”‚   â””â”€â”€ Survey completo de DPO e variantes
   â”‚   â”œâ”€â”€ 2024_Survey_RLHF.pdf
   â”‚   â”‚   â””â”€â”€ HistÃ³ria e mÃ©todo de RLHF
   â”‚   â”œâ”€â”€ 2025_Survey_Large_Language_Models.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ v16 (atualizado MarÃ§o 2025), ~170 pÃ¡ginas
   â”‚   â””â”€â”€ 2025_Survey_Post_Training_LLMs.pdf
   â”‚       â””â”€â”€ Foco em fine-tuning e alignment
   â”‚
   â”œâ”€â”€ ðŸ“‚ Papers_Pretraining/ (6 PDFs)
   â”‚   â”œâ”€â”€ 2018_GPT1_Improving_Language_Understanding.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ Paper SEMINAL: generative pre-training
   â”‚   â”œâ”€â”€ 2019_GPT2_Unsupervised_Multitask_Learners.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ Zero-shot learning, 1.5B parÃ¢metros
   â”‚   â”œâ”€â”€ 2020_GPT3_Few_Shot_Learners.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ 175B parÃ¢metros, in-context learning
   â”‚   â”œâ”€â”€ 2020_Scaling_Laws_Neural_LMs.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ Power laws: loss âˆ N^(-Î±)
   â”‚   â”œâ”€â”€ 2020_The_Pile_800GB_Dataset.pdf â­â­
   â”‚   â”‚   â””â”€â”€ 825GB, 22 componentes, dataset open-source
   â”‚   â””â”€â”€ 2025_Common_Corpus_Ethical_Data.pdf
   â”‚       â””â”€â”€ 2T tokens, alternativa Ã©tica ao Common Crawl
   â”‚
   â”œâ”€â”€ ðŸ“‚ Papers_Fine_Tuning/ (10 PDFs)
   â”‚   â”œâ”€â”€ 2017_PPO_Proximal_Policy_Optimization.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ Algoritmo de RL usado no RLHF
   â”‚   â”œâ”€â”€ 2021_LoRA_Low_Rank_Adaptation.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ 10,000x menos parÃ¢metros treinÃ¡veis
   â”‚   â”œâ”€â”€ 2022_Constitutional_AI_Harmlessness.pdf â­â­
   â”‚   â”‚   â””â”€â”€ RLAIF, abordagem da Anthropic (Claude)
   â”‚   â”œâ”€â”€ 2022_FLAN_Scaling_Instruction_Finetuned.pdf â­â­
   â”‚   â”‚   â””â”€â”€ Instruction tuning em 1,836 tasks
   â”‚   â”œâ”€â”€ 2022_InstructGPT_Training_Follow_Instructions.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ Base do ChatGPT, RLHF completo
   â”‚   â”œâ”€â”€ 2023_DPO_Direct_Preference_Optimization.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ Simplifica RLHF (1 step), Llama 3 usa
   â”‚   â”œâ”€â”€ 2023_FLAN_Collection_Instruction_Tuning.pdf
   â”‚   â”‚   â””â”€â”€ Dataset: 1,836 tasks, 25M+ examples
   â”‚   â”œâ”€â”€ 2023_QLoRA_Efficient_Finetuning_Quantized.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ Fine-tune 65B em 48GB GPU
   â”‚   â”œâ”€â”€ 2023_RLAIF_AI_Feedback.pdf â­â­
   â”‚   â”‚   â””â”€â”€ AI feedback (10-100x mais barato)
   â”‚   â””â”€â”€ 2025_Massive_SFT_Experiments.pdf
   â”‚       â””â”€â”€ 1000+ modelos SFT, lessons learned
   â”‚
   â”œâ”€â”€ ðŸ“‚ Papers_LLAMA_OpenSource/ (5 PDFs)
   â”‚   â”œâ”€â”€ 2023_LLaMA1_Open_Efficient_Foundation.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ Democratizou LLMs, 13B > GPT-3 175B
   â”‚   â”œâ”€â”€ 2023_Llama2_Open_Foundation_Fine_Tuned.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ RLHF completo (5 rounds), safety
   â”‚   â”œâ”€â”€ 2023_Mistral_7B.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ GQA, SWA, 7B > 13B
   â”‚   â”œâ”€â”€ 2024_Llama3_Herd_of_Models.pdf â­â­â­
   â”‚   â”‚   â””â”€â”€ 405B, 15.6T tokens, DPO, compete com GPT-4
   â”‚   â””â”€â”€ 2024_Phi3_Highly_Capable_Small_Model.pdf â­â­
   â”‚       â””â”€â”€ 3.8B compete com 7-13B, data quality
   â”‚
   â”œâ”€â”€ ðŸ“„ INDICE_COMPLETO_PDFS.md (32 KB)
   â”‚   â””â”€â”€ DescriÃ§Ã£o detalhada de TODOS os 26 papers
   â”‚
   â”œâ”€â”€ ðŸ“„ COMECE_AQUI.txt (30 KB)
   â”‚   â””â”€â”€ Guia com 3 cenÃ¡rios de preparaÃ§Ã£o
   â”‚
   â””â”€â”€ ðŸ“„ RESUMO_DOWNLOAD.txt
       â””â”€â”€ Este arquivo

==============================================================================
                         ðŸŽ¯ DESTAQUES DA COLEÃ‡ÃƒO
==============================================================================

âœ¨ COBERTURA TEMPORAL COMPLETA âœ“
   â†’ Desde fundamentos (GPT-1 2018) atÃ© estado-da-arte (2025)
   â†’ 8 anos de evoluÃ§Ã£o: 2017-2025

âœ¨ PAPERS SEMINAIS INCLUÃDOS:
   â†’ GPT-1 (2018): fundaÃ§Ã£o do generative pre-training
   â†’ GPT-2 (2019): zero-shot learning
   â†’ GPT-3 (2020): in-context learning, 175B params
   â†’ Scaling Laws (2020): power laws que guiaram a indÃºstria
   â†’ PPO (2017): algoritmo por trÃ¡s do RLHF
   â†’ InstructGPT (2022): base do ChatGPT
   â†’ LoRA (2021): democratizou fine-tuning
   â†’ DPO (2023): simplificaÃ§Ã£o revolucionÃ¡ria do RLHF

âœ¨ COBERTURA COMPLETA:
   â†’ PrÃ©-treino: GPT series, scaling laws, datasets
   â†’ Fine-tuning: SFT, RLHF, DPO, RLAIF, Constitutional AI
   â†’ Efficiency: LoRA, QLoRA
   â†’ Open-source: Llama 1/2/3, Mistral, Phi-3
   â†’ Datasets: The Pile, Common Corpus, curation

âœ¨ DOCUMENTAÃ‡ÃƒO EXCEPCIONAL:
   â†’ INDICE_COMPLETO_PDFS.md: 26 papers descritos em detalhe
   â†’ COMECE_AQUI.txt: 3 cenÃ¡rios (2h, 2 dias, 1-2 semanas)
   â†’ Estrutura de aula sugerida (120-160 min)
   â†’ ExercÃ­cios prÃ¡ticos
   â†’ Dicas pedagÃ³gicas

==============================================================================
                        ðŸ“š TOP 5 PAPERS PARA COMEÃ‡AR
==============================================================================

Se tempo limitado, comece por estes (ordem sugerida):

1. ðŸ“– Papers_Pretraining/2018_GPT1_Improving_Language_Understanding.pdf
   â†’ Paper SEMINAL: fundaÃ§Ã£o do prÃ©-treino generativo
   â†’ Introduziu paradigma: unsupervised pre-training â†’ supervised fine-tuning
   â†’ LEIA: IntroduÃ§Ã£o + MÃ©todo + Resultados (20 min)

2. ðŸ“– Papers_Pretraining/2020_Scaling_Laws_Neural_LMs.pdf
   â†’ Descobriu power laws: loss âˆ N^(-Î±)
   â†’ Guiou decisÃµes de TRILHÃ•ES de dÃ³lares
   â†’ LEIA: IntroduÃ§Ã£o + Power laws + Optimal allocation (25 min)

3. ðŸ“– Papers_Fine_Tuning/2022_InstructGPT_Training_Follow_Instructions.pdf
   â†’ Base do ChatGPT: RLHF completo
   â†’ Pipeline: SFT â†’ Reward Model â†’ PPO
   â†’ InstructGPT 1.3B > GPT-3 175B em preferÃªncia humana
   â†’ LEIA: IntroduÃ§Ã£o + MÃ©todo completo + Resultados (40 min)

4. ðŸ“– Papers_Fine_Tuning/2023_DPO_Direct_Preference_Optimization.pdf
   â†’ SimplificaÃ§Ã£o revolucionÃ¡ria: elimina RM e PPO
   â†’ Llama 3 usou DPO (nÃ£o RLHF)
   â†’ LEIA: IntroduÃ§Ã£o + MÃ©todo + ComparaÃ§Ã£o com RLHF (30 min)

5. ðŸ“– Papers_LLAMA_OpenSource/2023_Llama2_Open_Foundation_Fine_Tuned.pdf
   â†’ Open-source competitivo: Llama 2-Chat â‰ˆ ChatGPT
   â†’ Documenta RLHF em escala (5 iteraÃ§Ãµes)
   â†’ Safety: modelo open-source mais seguro
   â†’ LEIA: Pre-training + RLHF + Safety + Resultados (40 min)

â±ï¸ TOTAL: ~2h30min de leitura focada
âœ… RESULTADO: Material suficiente para aula completa de 90-120 minutos!

==============================================================================
                          ðŸŽ“ CONCEITOS COBERTOS
==============================================================================

PRÃ‰-TREINO:
âœ“ Generative Pre-training (next token prediction)
âœ“ Unsupervised Learning (sem labels)
âœ“ Parametric Knowledge (conhecimento nos pesos)
âœ“ Scaling Laws (power laws)
âœ“ Training Datasets (The Pile, Common Corpus)
âœ“ Data Curation (filtering, deduplication, quality)
âœ“ Tokenization
âœ“ Training Compute (GPU-hours, custos)

CAPACIDADES:
âœ“ Zero-shot Learning (sem exemplos)
âœ“ Few-shot Learning (poucos exemplos)
âœ“ In-Context Learning (aprender do prompt)
âœ“ Emergent Capabilities (aparecem com scale)
âœ“ Transfer Learning

FINE-TUNING:
âœ“ Supervised Fine-Tuning (SFT)
âœ“ Instruction Tuning (formato de instruÃ§Ã£o)
âœ“ Task-specific Adaptation
âœ“ Multi-task Learning
âœ“ Catastrophic Forgetting (esquecimento)

ALIGNMENT:
âœ“ RLHF (Reinforcement Learning from Human Feedback)
âœ“ Reward Model (prediz preferÃªncias)
âœ“ PPO (Proximal Policy Optimization)
âœ“ DPO (Direct Preference Optimization)
âœ“ RLAIF (RL from AI Feedback)
âœ“ Constitutional AI (princÃ­pios explÃ­citos)
âœ“ Alignment Tax (trade-off com capabilities)
âœ“ Safety Alignment (harmlessness)

EFFICIENCY:
âœ“ LoRA (Low-Rank Adaptation)
âœ“ QLoRA (Quantized LoRA)
âœ“ Parameter-Efficient Fine-Tuning (PEFT)
âœ“ Quantization (4-bit, 8-bit)
âœ“ Memory Optimization

OPEN-SOURCE:
âœ“ LLaMA series (1, 2, 3)
âœ“ Mistral (GQA, SWA)
âœ“ Phi-3 (Small Language Models)
âœ“ Weights Release
âœ“ Commercial Licenses

==============================================================================
                         ðŸ“Š MÃ‰TODOS E TÃ‰CNICAS
==============================================================================

PRÃ‰-TREINO METHODS:
   âœ“ Next Token Prediction
     - Autoregressive language modeling
     - Objective: maximize P(token_t | context)

   âœ“ Data Mixing
     - The Pile: 22 componentes
     - Optimal proportions (code, text, etc)

   âœ“ Scaling Strategies
     - Chinchilla: mais tokens > mais params
     - Llama 3: 15.6T tokens para 405B model

FINE-TUNING METHODS:

   âœ“ Supervised Fine-Tuning (SFT)
     - Task-specific examples
     - InstructGPT: 13k examples
     - Epochs: 1-3 typical

   âœ“ Instruction Tuning
     - FLAN: 1,836 tasks
     - Template diversity (10+ por task)

   âœ“ RLHF (3 steps)
     1. SFT (initial policy)
     2. Reward Model (train on comparisons)
     3. PPO (optimize for reward)
     - Llama 2: 5 iterations
     - Complex, expensive, powerful

   âœ“ DPO (1 step)
     - Direct optimization
     - No reward model
     - No PPO
     - Llama 3: usou DPO
     - Simpler, faster, comparable performance

   âœ“ RLAIF
     - AI feedback (nÃ£o humanos)
     - 10-100x mais barato
     - Scalable

   âœ“ Constitutional AI
     - Self-critique
     - Explicit principles (16+)
     - Anthropic's approach (Claude)

EFFICIENCY METHODS:

   âœ“ LoRA
     - Low-rank matrices A, B
     - Rank r: 4-64 (8 comum)
     - 10,000x menos parÃ¢metros
     - Performance: â‰ˆ full fine-tuning

   âœ“ QLoRA
     - 4-bit NormalFloat (NF4)
     - Double Quantization
     - Paged Optimizers
     - Fine-tune 65B em 48GB GPU
     - Guanaco-65B: 99.3% do ChatGPT

ARCHITECTURAL INNOVATIONS:

   âœ“ Grouped-Query Attention (GQA)
     - Mistral 7B
     - Faster inference
     - Smaller KV cache

   âœ“ Sliding Window Attention (SWA)
     - Mistral 7B
     - O(nÃ—w) instead of O(nÂ²)
     - 32 layers Ã— 4096 tokens = 131k reach

   âœ“ RoPE (Rotary Position Embeddings)
     - LLaMA series
     - Better extrapolation
     - Llama 3: scaled to 128k context

==============================================================================
                      ðŸš€ PRÃ“XIMOS PASSOS RECOMENDADOS
==============================================================================

1. âœ… Abra COMECE_AQUI.txt
   â†’ Escolha seu cenÃ¡rio (1, 2 ou 3)
   â†’ Siga roadmap sugerido

2. âœ… Consulte INDICE_COMPLETO_PDFS.md
   â†’ DescriÃ§Ãµes detalhadas de cada paper
   â†’ Papers por tema especÃ­fico
   â†’ Estrutura de aula sugerida

3. âœ… Comece lendo os Top 5
   â†’ Em 2-3 horas vocÃª terÃ¡ base sÃ³lida
   â†’ Suficiente para aula de 90-120 minutos

4. âœ… Prepare slides enquanto lÃª
   â†’ Extraia figuras importantes
   â†’ Anote estatÃ­sticas e nÃºmeros
   â†’ Marque exemplos prÃ¡ticos

5. âœ… Prepare demonstraÃ§Ãµes
   â†’ Compare outputs: base model vs chat model
   â†’ Mostre scaling laws (grÃ¡ficos)
   â†’ Dataset composition (pie chart do The Pile)
   â†’ Pipeline RLHF (diagrama de 3 steps)

==============================================================================
                           âœ¨ PONTOS FORTES
==============================================================================

âœ“ 26 papers - ColeÃ§Ã£o completa e balanceada
âœ“ Cobertura temporal: 2017-2025 (8 anos)
âœ“ Papers SEMINAIS incluÃ­dos:
  - GPT-1, GPT-2, GPT-3 (fundaÃ§Ã£o)
  - Scaling Laws (guiou a indÃºstria)
  - InstructGPT (base do ChatGPT)
  - LoRA (democratizou fine-tuning)
  - DPO (futuro do alignment)
âœ“ EquilÃ­brio: surveys + pesquisas + modelos open-source
âœ“ DocumentaÃ§Ã£o excepcional (guias, estrutura de aula)
âœ“ Foco em papers de altÃ­ssimo impacto
âœ“ OrganizaÃ§Ã£o clara em 4 categorias
âœ“ TODOS os conceitos-chave da solicitaÃ§Ã£o:
  - PrÃ©-treino e corpus de conhecimento âœ“
  - Conhecimento paramÃ©trico âœ“
  - Fine-tuning methods (RLHF, DPO, RLAIF, etc) âœ“
  - Efficiency (LoRA, QLoRA) âœ“
  - Open-source (Llama, Mistral, Phi-3) âœ“

==============================================================================
                        ðŸ“ˆ COMPARAÃ‡ÃƒO COM OUTRAS AULAS
==============================================================================

AULA 1 - LLMs como MÃ¡quinas de TransformaÃ§Ã£o:
   â€¢ 10 PDFs fundamentais
   â€¢ Foco: Arquitetura Transformer e conceitos bÃ¡sicos

AULA 3 - Janela de Contexto:
   â€¢ 35 PDFs (4 surveys, 16 papers 2025, 5 benchmarks, 10 fundamentais)
   â€¢ Foco: Context window e in-context learning

AULA 4 - Conhecimento da MÃ¡quina:
   â€¢ 37 PDFs (5 surveys, 18 papers 2025, 3 RAG, 11 fundamentais)
   â€¢ Foco: Conhecimento paramÃ©trico vs externo, editing

AULA 5 - Treinamento de LLMs (ESTA):
   â€¢ 26 PDFs (5 surveys, 6 prÃ©-treino, 10 fine-tuning, 5 open-source)
   â€¢ Foco: Como LLMs sÃ£o treinados (pre-training + fine-tuning)
   â€¢ Ãšnica aula com foco em TODO o ciclo de vida do treinamento
   â€¢ Papers seminais: GPT-1/2/3, InstructGPT, LoRA, DPO
   â€¢ Cobre desde fundamentos (2018) atÃ© estado-da-arte (2025)

AULA 6 - AlucinaÃ§Ãµes:
   â€¢ 16 PDFs (8 surveys 2025, 5 papers recentes, 3 fundamentais)
   â€¢ Foco: Factualidade e mitigaÃ§Ã£o de alucinaÃ§Ãµes

==============================================================================
                          ðŸŽ‰ CONCLUSÃƒO
==============================================================================

âœ… MISSÃƒO CUMPRIDA COM SUCESSO!

VocÃª agora tem:
   â†’ 26 papers cientÃ­ficos de altÃ­ssima qualidade
   â†’ Cobertura COMPLETA do ciclo de treinamento
   â†’ DocumentaÃ§Ã£o completa e prÃ¡tica
   â†’ Estrutura de aula pronta (120-160 min)
   â†’ ExercÃ­cios sugeridos
   â†’ Material para semanas de estudo

Esta Ã© uma das coleÃ§Ãµes MAIS COMPLETAS sobre treinamento de LLMs que
vocÃª poderia ter. Cobre desde papers seminais (GPT-1 2018, Scaling Laws 2020,
InstructGPT 2022) atÃ© fronteira da pesquisa (Llama 3 2024, papers 2025).

Temas centrais cobertos:
   âœ… PRÃ‰-TREINO: como LLMs aprendem conhecimento paramÃ©trico
      - Generative pre-training
      - Scaling laws
      - Datasets (The Pile, Common Corpus)
      - GPT-1 â†’ GPT-2 â†’ GPT-3

   âœ… FINE-TUNING: como adaptar LLMs para tarefas especÃ­ficas
      - Supervised Fine-Tuning (SFT)
      - Instruction tuning (FLAN)
      - Parameter-efficient methods (LoRA, QLoRA)

   âœ… ALIGNMENT: como alinhar LLMs com intenÃ§Ãµes humanas
      - RLHF (InstructGPT, Llama 2)
      - DPO (Llama 3)
      - RLAIF (Google)
      - Constitutional AI (Anthropic/Claude)

   âœ… OPEN-SOURCE: como modelos abertos democratizam LLMs
      - LLaMA 1/2/3 (Meta)
      - Mistral 7B (Mistral AI)
      - Phi-3 (Microsoft)

   âœ… EFFICIENCY: como treinar com recursos limitados
      - LoRA (10,000x menos parÃ¢metros)
      - QLoRA (65B em 1 GPU)

                        ESTÃ PRONTO PARA A AULA! ðŸš€

==============================================================================

                            ðŸ“Š ESTATÃSTICAS FINAIS

==============================================================================

MODELOS COBERTOS (params):
   â€¢ GPT-1: 117M
   â€¢ GPT-2: 1.5B
   â€¢ GPT-3: 175B
   â€¢ LLaMA 1: 7B-65B
   â€¢ Llama 2: 7B-70B
   â€¢ Mistral 7B: 7B
   â€¢ Phi-3: 3.8B-14B
   â€¢ Llama 3: 8B-405B

TRAINING TOKENS:
   â€¢ GPT-1: ~1GB texto
   â€¢ GPT-2: 40GB (WebText)
   â€¢ GPT-3: 300B tokens
   â€¢ LLaMA 1: 1.4T tokens
   â€¢ Llama 2: 2T tokens
   â€¢ Phi-3: 3.3-4.8T tokens
   â€¢ Llama 3: 15.6T tokens (!)

DATASETS:
   â€¢ The Pile: 825GB, 22 componentes
   â€¢ Common Corpus: 2T tokens
   â€¢ FLAN Collection: 1,836 tasks, 25M+ examples
   â€¢ InstructGPT: 13k SFT, 33k comparisons

CUSTOS ESTIMADOS:
   â€¢ GPT-3 training: $4.6M
   â€¢ Llama 3 405B: $50-100M
   â€¢ RLHF labeling: $50-100k
   â€¢ RLAIF: 10-100x mais barato

EFFICIENCY GAINS:
   â€¢ LoRA: 10,000x menos parÃ¢metros treinÃ¡veis
   â€¢ QLoRA: Fine-tune 65B em 48GB (vs 130GB)
   â€¢ Guanaco-65B: 24h em 1 GPU

PERFORMANCE HIGHLIGHTS:
   â€¢ InstructGPT 1.3B > GPT-3 175B (preferÃªncia humana)
   â€¢ LLaMA 13B > GPT-3 175B (alguns benchmarks)
   â€¢ Mistral 7B > Llama 2 13B (todos benchmarks)
   â€¢ Llama 3 405B â‰ˆ GPT-4 (MMLU: 87.3% vs 86.4%)
   â€¢ Phi-3 3.8B â‰ˆ Llama 3 8B (data quality > size)

==============================================================================

Compilado por: Claude Code (Anthropic)
Data: 31 de outubro de 2025
Para: George Marmelstein - Aulas 2025

QuestÃµes? Consulte:
   â†’ COMECE_AQUI.txt (guia de inÃ­cio rÃ¡pido)
   â†’ INDICE_COMPLETO_PDFS.md (detalhes de cada paper)

                              BOA AULA! ðŸ“šâœ¨
