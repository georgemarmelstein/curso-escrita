‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                               ‚ïë
‚ïë                    üöÄ COMECE AQUI - AULA 5: TREINAMENTO                       ‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

DATA: 31 de outubro de 2025
MATERIAL COMPLETO: 26 PDFs cient√≠ficos

==============================================================================
                           üìã O QUE VOC√ä TEM AQUI
==============================================================================

Voc√™ acabou de receber uma cole√ß√£o COMPLETA de papers sobre treinamento de
LLMs, cobrindo:

   ‚úÖ PR√â-TREINO: Como LLMs aprendem conhecimento param√©trico
   ‚úÖ DATASETS: O que entra no treinamento (The Pile, Common Corpus, etc)
   ‚úÖ FINE-TUNING: Como alinhar LLMs (RLHF, DPO, RLAIF, etc)
   ‚úÖ EFFICIENCY: Como treinar com recursos limitados (LoRA, QLoRA)
   ‚úÖ OPEN-SOURCE: Como Llama, Mistral, Phi-3 funcionam

Total: 26 papers (2017-2025)
Volume: ~650 p√°ginas de conte√∫do cient√≠fico

==============================================================================
                        ‚ö° ESCOLHA SEU CEN√ÅRIO
==============================================================================

Este guia oferece 3 cen√°rios de prepara√ß√£o, dependendo do seu tempo:

   üìå CEN√ÅRIO 1: Prepara√ß√£o Rel√¢mpago (2-3 horas)
      ‚Üí Para quando a aula √© AMANH√É
      ‚Üí 5 papers essenciais
      ‚Üí Foco em conceitos principais

   üìå CEN√ÅRIO 2: Prepara√ß√£o S√≥lida (2-3 dias)
      ‚Üí Para prepara√ß√£o adequada
      ‚Üí 12 papers fundamentais
      ‚Üí Cobertura completa dos t√≥picos

   üìå CEN√ÅRIO 3: Prepara√ß√£o Completa (1-2 semanas)
      ‚Üí Para dom√≠nio total do tema
      ‚Üí Todos os 26 papers
      ‚Üí Profundidade m√°xima


==============================================================================
                üìå CEN√ÅRIO 1: PREPARA√á√ÉO REL√ÇMPAGO
==============================================================================

TEMPO NECESS√ÅRIO: 2-3 horas de leitura focada
QUANDO USAR: Aula √© amanh√£, preciso do essencial AGORA

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ROADMAP: 5 PAPERS ESSENCIAIS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìñ PAPER 1: GPT-1 (2018) - 20 MIN
   üìÇ Arquivo: Papers_Pretraining/2018_GPT1_Improving_Language_Understanding.pdf

   O QUE LER:
   ‚Üí Introdu√ß√£o (p√°ginas 1-2)
   ‚Üí M√©todo de pr√©-treino generativo (p√°ginas 2-4)
   ‚Üí Resultados (p√°ginas 5-6)

   O QUE EXTRAIR:
   ‚úì Por que "generative pre-training" funciona
   ‚úì Pipeline: unsupervised pre-training ‚Üí supervised fine-tuning
   ‚úì Task-agnostic architecture

   PARA SLIDES:
   ‚Ä¢ Figura da arquitetura (pre-train + fine-tune)
   ‚Ä¢ Tabela de resultados (9 de 12 tasks = SOTA)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìñ PAPER 2: Scaling Laws (2020) - 25 MIN
   üìÇ Arquivo: Papers_Pretraining/2020_Scaling_Laws_Neural_LMs.pdf

   O QUE LER:
   ‚Üí Introdu√ß√£o (p√°ginas 1-2)
   ‚Üí Power laws (p√°ginas 3-5)
   ‚Üí Optimal compute allocation (p√°ginas 8-10)

   O QUE EXTRAIR:
   ‚úì Loss ‚àù N^(-Œ±) (loss cai com mais par√¢metros)
   ‚úì Trade-offs: par√¢metros vs dados vs compute
   ‚úì Por que modelos ficam melhores com scale

   PARA SLIDES:
   ‚Ä¢ Gr√°ficos de power laws (loss vs params)
   ‚Ä¢ Equa√ß√£o de scaling
   ‚Ä¢ Optimal allocation (dados vs par√¢metros)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìñ PAPER 3: InstructGPT (2022) - 40 MIN ‚≠ê ESSENCIAL
   üìÇ Arquivo: Papers_Fine_Tuning/2022_InstructGPT_Training_Follow_Instructions.pdf

   O QUE LER:
   ‚Üí Introdu√ß√£o + Motiva√ß√£o (p√°ginas 1-3)
   ‚Üí M√©todo RLHF (p√°ginas 4-8)
   ‚Üí Resultados (p√°ginas 10-15)

   O QUE EXTRAIR:
   ‚úì Pipeline de RLHF (3 steps):
     1. Supervised Fine-Tuning (SFT)
     2. Reward Model training
     3. RL with PPO
   ‚úì Por que InstructGPT 1.3B > GPT-3 175B
   ‚úì Alignment: helpfulness, harmlessness, honesty

   PARA SLIDES:
   ‚Ä¢ Diagrama do pipeline RLHF (3 steps)
   ‚Ä¢ Gr√°fico: prefer√™ncia humana (1.3B > 175B!)
   ‚Ä¢ Exemplos de respostas aligned vs unaligned

   CONCEITO CENTRAL:
   Este √© o paper do ChatGPT! Entender RLHF √© cr√≠tico.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìñ PAPER 4: DPO (2023) - 30 MIN
   üìÇ Arquivo: Papers_Fine_Tuning/2023_DPO_Direct_Preference_Optimization.pdf

   O QUE LER:
   ‚Üí Introdu√ß√£o (p√°ginas 1-2)
   ‚Üí DPO method (p√°ginas 3-5)
   ‚Üí Compara√ß√£o com RLHF (p√°ginas 8-10)

   O QUE EXTRAIR:
   ‚úì DPO elimina reward model e PPO
   ‚úì Otimiza√ß√£o direta de prefer√™ncias
   ‚úì Simplicidade: 1 training run (vs 3 no RLHF)
   ‚úì Performance: igual ou melhor que RLHF

   PARA SLIDES:
   ‚Ä¢ Compara√ß√£o: RLHF (3 steps) vs DPO (1 step)
   ‚Ä¢ DPO loss equation
   ‚Ä¢ Resultados: DPO vs RLHF

   POR QUE IMPORTA:
   Llama 3 usou DPO. Est√° se tornando o padr√£o.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìñ PAPER 5: Llama 2 (2023) - 35 MIN
   üìÇ Arquivo: Papers_LLAMA_OpenSource/2023_Llama2_Open_Foundation_Fine_Tuned.pdf

   O QUE LER:
   ‚Üí Pr√©-treino (p√°ginas 5-8)
   ‚Üí RLHF completo (p√°ginas 9-15)
   ‚Üí Resultados e safety (p√°ginas 20-25)

   O QUE EXTRAIR:
   ‚úì 2 trilh√µes de tokens de treinamento
   ‚úì 5 itera√ß√µes de RLHF (n√£o apenas 1!)
   ‚úì Llama 2-Chat: competitive com ChatGPT
   ‚úì Safety: modelo open-source mais seguro

   PARA SLIDES:
   ‚Ä¢ Tabela: Llama 2 70B vs GPT-3.5 vs PaLM-2
   ‚Ä¢ Diagrama: 5 rounds de RLHF
   ‚Ä¢ Safety metrics

   POR QUE IMPORTA:
   Documenta RLHF completo, open-source state-of-the-art.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚è±Ô∏è TEMPO TOTAL: ~2h30 de leitura focada

‚úÖ RESULTADO: Voc√™ ter√° conhecimento dos conceitos fundamentais:
   ‚Ä¢ Pr√©-treino generativo
   ‚Ä¢ Scaling laws
   ‚Ä¢ RLHF completo
   ‚Ä¢ DPO como alternativa
   ‚Ä¢ Open-source state-of-the-art

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ESTRUTURA DE AULA SUGERIDA (90 MIN)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

[0-25 min] PR√â-TREINO
‚Ä¢ GPT-1: generative pre-training
‚Ä¢ Scaling laws: por que bigger = better
‚Ä¢ Dataset: mencionar The Pile

[25-60 min] FINE-TUNING ‚≠ê
‚Ä¢ RLHF pipeline (InstructGPT)
  - Step 1: SFT
  - Step 2: Reward Model
  - Step 3: PPO
‚Ä¢ DPO: simplifica√ß√£o
‚Ä¢ Compara√ß√£o: quando usar cada um

[60-80 min] OPEN-SOURCE
‚Ä¢ Llama 2: RLHF em escala
‚Ä¢ Resultados competitivos

[80-90 min] Q&A

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DICAS DE APRESENTA√á√ÉO
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

1. COMECE COM MOTIVA√á√ÉO:
   "Por que GPT-3 gera texto t√≥xico e in√∫til, mas ChatGPT √© √∫til?
    Resposta: TREINAMENTO (especificamente, alignment)"

2. ENFATIZE PIPELINE:
   Pre-training ‚Üí Fine-tuning ‚Üí Alignment
   (cada fase tem prop√≥sito diferente)

3. USE ANALOGIAS:
   ‚Ä¢ Pr√©-treino = Educa√ß√£o geral (ler tudo na internet)
   ‚Ä¢ Fine-tuning = Especializa√ß√£o (treinamento espec√≠fico)
   ‚Ä¢ Alignment = √âtica e valores (seguir instru√ß√µes, ser seguro)

4. MOSTRE TRADE-OFFS:
   ‚Ä¢ RLHF: mais controle, mais complexo
   ‚Ä¢ DPO: mais simples, menos controle fino
   ‚Ä¢ LoRA: eficiente, mas performance ligeiramente inferior

5. EXEMPLOS PR√ÅTICOS:
   ‚Ä¢ "Paris is the capital of [MASK]" ‚Üí pr√©-treino captura isso
   ‚Ä¢ "Write a poem about Paris" ‚Üí fine-tuning ensina a seguir
   ‚Ä¢ Evitar gerar conte√∫do t√≥xico ‚Üí alignment


==============================================================================
                üìå CEN√ÅRIO 2: PREPARA√á√ÉO S√ìLIDA
==============================================================================

TEMPO NECESS√ÅRIO: 2-3 dias (4-6 horas/dia de leitura)
QUANDO USAR: Quero dar uma aula COMPLETA e bem fundamentada

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ROADMAP: 12 PAPERS FUNDAMENTAIS (ordem de leitura)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìÖ DIA 1: FUNDAMENTOS DE PR√â-TREINO (4 papers, 2-3 horas)

   1Ô∏è‚É£ GPT-1 (2018) - 20 min
      Papers_Pretraining/2018_GPT1_Improving_Language_Understanding.pdf
      Funda√ß√£o do paradigma pre-train + fine-tune

   2Ô∏è‚É£ GPT-2 (2019) - 30 min
      Papers_Pretraining/2019_GPT2_Unsupervised_Multitask_Learners.pdf
      Zero-shot learning, scale matters

   3Ô∏è‚É£ GPT-3 (2020) - 50 min (skim estrat√©gico)
      Papers_Pretraining/2020_GPT3_Few_Shot_Learners.pdf
      In-context learning, 175B par√¢metros

   4Ô∏è‚É£ Scaling Laws (2020) - 25 min
      Papers_Pretraining/2020_Scaling_Laws_Neural_LMs.pdf
      Power laws, optimal compute allocation

   üìù ANOTE: Evolu√ß√£o GPT-1 ‚Üí GPT-2 ‚Üí GPT-3
            (scale unlock capabilities)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìÖ DIA 2: DATASETS E OPEN-SOURCE (4 papers, 2-3 horas)

   5Ô∏è‚É£ The Pile (2020) - 25 min
      Papers_Pretraining/2020_The_Pile_800GB_Dataset.pdf
      O que entra no pr√©-treino (825GB, 22 subconjuntos)

   6Ô∏è‚É£ LLaMA 1 (2023) - 30 min
      Papers_LLAMA_OpenSource/2023_LLaMA1_Open_Efficient_Foundation.pdf
      Open-source, training > pure scale

   7Ô∏è‚É£ Llama 2 (2023) - 40 min
      Papers_LLAMA_OpenSource/2023_Llama2_Open_Foundation_Fine_Tuned.pdf
      RLHF completo (5 itera√ß√µes), safety

   8Ô∏è‚É£ Mistral 7B (2023) - 25 min
      Papers_LLAMA_OpenSource/2023_Mistral_7B.pdf
      Arquitetura eficiente (GQA, SWA), 7B > 13B

   üìù ANOTE: Dataset composition (The Pile)
            Open-source democratiza pesquisa

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìÖ DIA 3: FINE-TUNING E ALIGNMENT (4 papers, 2-3 horas)

   9Ô∏è‚É£ InstructGPT (2022) - 50 min ‚≠ê‚≠ê‚≠ê
      Papers_Fine_Tuning/2022_InstructGPT_Training_Follow_Instructions.pdf
      RLHF completo, base do ChatGPT

   üîü LoRA (2021) - 25 min
      Papers_Fine_Tuning/2021_LoRA_Low_Rank_Adaptation.pdf
      Parameter-efficient fine-tuning (10,000x menos params)

   1Ô∏è‚É£1Ô∏è‚É£ DPO (2023) - 30 min
      Papers_Fine_Tuning/2023_DPO_Direct_Preference_Optimization.pdf
      Simplifica√ß√£o do RLHF, futuro do alignment

   1Ô∏è‚É£2Ô∏è‚É£ Constitutional AI (2022) - 35 min
      Papers_Fine_Tuning/2022_Constitutional_AI_Harmlessness.pdf
      RLAIF (AI feedback), abordagem da Anthropic

   üìù ANOTE: Pipeline de alignment (SFT ‚Üí RM ‚Üí PPO)
            Alternativas (DPO, RLAIF)
            Efficiency (LoRA)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚è±Ô∏è TEMPO TOTAL: ~8 horas de leitura focada ao longo de 3 dias

‚úÖ RESULTADO: Dom√≠nio completo dos conceitos principais:
   ‚Ä¢ Pr√©-treino: GPT series, scaling laws
   ‚Ä¢ Datasets: composi√ß√£o, qualidade
   ‚Ä¢ Open-source: Llama, Mistral
   ‚Ä¢ Fine-tuning: RLHF, DPO, RLAIF, LoRA
   ‚Ä¢ Trade-offs e design decisions

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ESTRUTURA DE AULA SUGERIDA (120 MIN)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

[0-10 min] INTRODU√á√ÉO
‚Ä¢ Motiva√ß√£o: Por que ChatGPT funciona?
‚Ä¢ Roadmap da aula

[10-40 min] M√ìDULO 1: PR√â-TREINO
‚Ä¢ GPT-1, GPT-2, GPT-3 (evolu√ß√£o)
‚Ä¢ Generative pre-training
‚Ä¢ Scaling laws (por que bigger = better)
‚Ä¢ Datasets (The Pile: 825GB, 22 componentes)
‚Ä¢ Conhecimento param√©trico

[40-85 min] M√ìDULO 2: FINE-TUNING E ALIGNMENT ‚≠ê
‚Ä¢ SFT b√°sico
‚Ä¢ RLHF pipeline completo:
  - Step 1: Supervised Fine-Tuning
  - Step 2: Reward Model training
  - Step 3: RL with PPO
‚Ä¢ Alternativas modernas:
  - DPO: simplifica√ß√£o (1 step)
  - RLAIF: AI feedback (10x mais barato)
  - Constitutional AI: princ√≠pios expl√≠citos
‚Ä¢ Parameter-efficient methods:
  - LoRA: 10,000x menos par√¢metros
  - QLoRA: 65B em 1 GPU

[85-110 min] M√ìDULO 3: OPEN-SOURCE REVOLUTION
‚Ä¢ LLaMA 1: democratiza√ß√£o
‚Ä¢ Llama 2: RLHF em escala, safety
‚Ä¢ Mistral 7B: arquitetura eficiente
‚Ä¢ Trade-offs: open vs closed

[110-120 min] Q&A E DISCUSS√ÉO

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
EXERC√çCIOS PR√ÅTICOS SUGERIDOS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

1. COMPARE OUTPUTS:
   Prompt: "Explain quantum computing"
   ‚Ä¢ GPT-3 (base): factual mas dry
   ‚Ä¢ ChatGPT (RLHF): conversational, structured
   ‚Üí Mostra efeito do alignment

2. SCALING LAWS:
   Plot loss vs par√¢metros (dados do paper)
   ‚Üí Visualiza power law

3. DATASET COMPOSITION:
   Mostre composi√ß√£o do The Pile (pie chart)
   ‚Üí Discuss√£o: o que deve entrar?

4. RLHF vs DPO:
   Diagrama lado a lado (3 steps vs 1 step)
   ‚Üí Trade-offs: complexidade vs controle

5. LoRA EFFICIENCY:
   Calcule: quantos par√¢metros treinar?
   ‚Ä¢ Full fine-tuning: 7B
   ‚Ä¢ LoRA r=8: 7M (0.1%)
   ‚Üí 1000x redu√ß√£o!


==============================================================================
                üìå CEN√ÅRIO 3: PREPARA√á√ÉO COMPLETA
==============================================================================

TEMPO NECESS√ÅRIO: 1-2 semanas (3-4 horas/dia)
QUANDO USAR: Quero dom√≠nio TOTAL do tema, incluindo nuances e estado-da-arte

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ROADMAP COMPLETO: TODOS OS 26 PAPERS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìÖ SEMANA 1: FUNDAMENTOS (12 papers)

DIA 1-2: Pr√©-treino Completo (6 papers, 4h)
   ‚Üí GPT-1 (2018)
   ‚Üí GPT-2 (2019)
   ‚Üí GPT-3 (2020)
   ‚Üí Scaling Laws (2020)
   ‚Üí The Pile (2020)
   ‚Üí Common Corpus (2025)

DIA 3: Surveys Gerais (2 papers, 3h)
   ‚Üí Survey LLMs 2025 (v16) - Se√ß√µes de pre-training e fine-tuning
   ‚Üí Survey Post-Training LLMs 2025

DIA 4: Open-Source Foundations (4 papers, 4h)
   ‚Üí LLaMA 1 (2023)
   ‚Üí Llama 2 (2023)
   ‚Üí Mistral 7B (2023)
   ‚Üí Phi-3 (2024)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìÖ SEMANA 2: FINE-TUNING E ESTADO-DA-ARTE (14 papers)

DIA 5-6: RLHF Completo (5 papers, 6h)
   ‚Üí PPO (2017) - Funda√ß√£o
   ‚Üí InstructGPT (2022) - RLHF pipeline
   ‚Üí FLAN papers (2022-2023) - Instruction tuning
   ‚Üí Survey RLHF (2024)

DIA 7: Alternativas ao RLHF (4 papers, 4h)
   ‚Üí DPO (2023)
   ‚Üí RLAIF (2023)
   ‚Üí Constitutional AI (2022)
   ‚Üí Survey DPO (2024)

DIA 8: Efficiency (3 papers, 3h)
   ‚Üí LoRA (2021)
   ‚Üí QLoRA (2023)
   ‚Üí Massive SFT Experiments (2025)

DIA 9: Datasets Deep Dive (1 paper + survey, 3h)
   ‚Üí Survey Datasets LLMs (2024) - 444 datasets!
   ‚Üí (Re-leia The Pile com mais aten√ß√£o)

DIA 10: State-of-the-Art (1 paper, 3h)
   ‚Üí Llama 3 (2024) - 405B, DPO, 15T tokens

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚è±Ô∏è TEMPO TOTAL: 30-35 horas de leitura ao longo de 10 dias

‚úÖ RESULTADO: Expertise completa em treinamento de LLMs:
   ‚Ä¢ Hist√≥ria completa: 2017-2025
   ‚Ä¢ Todos os m√©todos: RLHF, DPO, RLAIF, Constitutional AI
   ‚Ä¢ Efficiency: LoRA, QLoRA
   ‚Ä¢ Datasets: composi√ß√£o, curation, ethics
   ‚Ä¢ Trade-offs e nuances
   ‚Ä¢ Estado-da-arte (Llama 3 405B)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ESTRUTURA DE AULA SUGERIDA (160 MIN - aula dupla)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

[0-15 min] INTRODU√á√ÉO
‚Ä¢ Motiva√ß√£o: Da gera√ß√£o aleat√≥ria ao ChatGPT
‚Ä¢ Roadmap da aula
‚Ä¢ Conceitos-chave: pre-training vs fine-tuning vs alignment

[15-50 min] M√ìDULO 1: PR√â-TREINO EM PROFUNDIDADE
‚Ä¢ Hist√≥ria: GPT-1 ‚Üí GPT-2 ‚Üí GPT-3 ‚Üí Llama 3
‚Ä¢ Generative pre-training (next token prediction)
‚Ä¢ Scaling laws:
  - Power laws (loss ‚àù N^(-Œ±))
  - Optimal compute allocation
  - Chinchilla scaling (mais tokens!)
‚Ä¢ Datasets:
  - The Pile (825GB, 22 componentes)
  - Common Corpus (2T tokens, √©tico)
  - Data quality vs quantity
  - Ethical considerations (copyright, PII)
‚Ä¢ Conhecimento param√©trico:
  - Stored in weights (not in prompts)
  - Emergent capabilities

[50-115 min] M√ìDULO 2: FINE-TUNING COMPLETO ‚≠ê
‚Ä¢ SFT (Supervised Fine-Tuning):
  - Task-specific adaptation
  - Instruction tuning (FLAN)
  - Data composition matters
  - Forgetting mitigation

‚Ä¢ RLHF Pipeline:
  - Step 1: SFT (13k examples - InstructGPT)
  - Step 2: Reward Model (33k comparisons)
  - Step 3: PPO (why PPO? stability)
  - Llama 2: 5 iterations (not just 1!)
  - Challenges: expensive, complex, slow

‚Ä¢ Alternativas Modernas:
  - DPO: elimina RM e PPO (1 step)
    ‚Ä¢ Llama 3 usou DPO
    ‚Ä¢ Trade-off: simplicidade vs controle
  - RLAIF: AI feedback (10x cheaper)
    ‚Ä¢ When it works (objective tasks)
    ‚Ä¢ Limitations (subjective tasks)
  - Constitutional AI: explicit principles
    ‚Ä¢ Anthropic's approach (Claude)
    ‚Ä¢ Self-critique and revision

‚Ä¢ Parameter-Efficient Methods:
  - LoRA: 10,000x menos par√¢metros
    ‚Ä¢ Low-rank adaptation
    ‚Ä¢ Where to apply (Q, V projections)
    ‚Ä¢ Performance: igual ao full fine-tuning
  - QLoRA: 65B em 1 GPU
    ‚Ä¢ 4-bit NormalFloat
    ‚Ä¢ Double quantization
    ‚Ä¢ Democratiza√ß√£o extrema

‚Ä¢ Compara√ß√£o de M√©todos:
  - RLHF: controle, complexidade
  - DPO: simplicidade, performance
  - RLAIF: custo, scalability
  - LoRA: efici√™ncia, democratiza√ß√£o

[115-145 min] M√ìDULO 3: OPEN-SOURCE REVOLUTION
‚Ä¢ LLaMA 1: democratiza√ß√£o
  - Training > pure scale
  - 13B > GPT-3 175B (!)
‚Ä¢ Llama 2:
  - RLHF completo (5 rounds)
  - Safety: adversarial testing
  - Open commercial license
‚Ä¢ Mistral 7B:
  - Arquitetura: GQA, SWA
  - 7B > 13B (architecture matters)
‚Ä¢ Phi-3:
  - Small but mighty (3.8B)
  - Data quality > size
  - Edge deployment
‚Ä¢ Llama 3:
  - 405B, 15.6T tokens
  - DPO (not RLHF)
  - Competitive com GPT-4
  - 128k context

[145-160 min] Q&A, DISCUSS√ÉO, FUTURO
‚Ä¢ Open questions:
  - Scaling limits?
  - Data exhaustion?
  - Synthetic data?
  - Multimodal?

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
EXERC√çCIOS AVAN√áADOS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

1. IMPLEMENTAR MINI-RLHF:
   C√≥digo para treinar reward model em toy dataset
   ‚Üí Entender pipeline na pr√°tica

2. LoRA MATH:
   Calcular n√∫mero de par√¢metros para diferentes ranks
   ‚Üí Trade-off rank vs performance

3. DATASET CURATION:
   Design: como montar dataset para fine-tuning?
   ‚Üí Quality, diversity, size trade-offs

4. SCALING LAWS:
   Prever loss para modelo hipot√©tico (1T params)
   ‚Üí Aplicar power laws

5. COMPARE METHODS:
   Tabela comparativa: RLHF, DPO, RLAIF, Constitutional AI
   ‚Üí Cost, complexity, performance, use cases

6. CASE STUDY:
   Llama 3 (DPO) vs Llama 2 (RLHF): por que mudou?
   ‚Üí Meta's rationale

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
T√ìPICOS AVAN√áADOS PARA DISCUSS√ÉO
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

1. ALIGNMENT TAX:
   RLHF melhora chat, mas pode piorar alguns benchmarks
   ‚Üí Trade-off: alignment vs capabilities

2. CATASTROPHIC FORGETTING:
   Fine-tuning pode fazer modelo "esquecer" pr√©-treino
   ‚Üí Mitigation: replay, mix pre-training data

3. SCALING LIMITS:
   At√© onde scaling laws v√£o?
   ‚Üí Data exhaustion, compute limits, emergent saturation

4. SYNTHETIC DATA:
   Phi-3: LLMs gerando dados para treinar LLMs
   ‚Üí Recursive improvement? Collapse?

5. ETHICAL CONSIDERATIONS:
   ‚Ä¢ Copyright (Books3, GitHub code)
   ‚Ä¢ PII (personal information in training data)
   ‚Ä¢ Bias amplification
   ‚Ä¢ Safety vs censorship

6. OPEN VS CLOSED:
   ‚Ä¢ Llama 3 405B: competitive com GPT-4, open
   ‚Ä¢ Safety concerns
   ‚Ä¢ Democratization vs misuse

7. MULTIMODAL:
   Llama 3.2 Vision: extending to images
   ‚Üí Training multimodal models

8. FUTURE DIRECTIONS:
   ‚Ä¢ Mixture-of-Experts (MoE)
   ‚Ä¢ Sparse models
   ‚Ä¢ Continual learning (n√£o forget)
   ‚Ä¢ Test-time training


==============================================================================
                        üìä RECURSOS ADICIONAIS
==============================================================================

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CONSULTE O √çNDICE COMPLETO
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Para descri√ß√µes DETALHADAS de cada paper:
   üìÑ INDICE_COMPLETO_PDFS.md

Esse arquivo cont√©m:
   ‚Ä¢ Resumo detalhado de cada um dos 26 papers
   ‚Ä¢ P√°ginas a ler
   ‚Ä¢ Conceitos principais
   ‚Ä¢ Figuras importantes
   ‚Ä¢ Por que cada paper √© relevante

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
FIGURAS ESSENCIAIS PARA SLIDES
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

De cada paper, extraia:

GPT-1:
   ‚Ä¢ Figura: Pre-training + Fine-tuning pipeline
   ‚Ä¢ Tabela: Results on 12 tasks

GPT-3:
   ‚Ä¢ Gr√°ficos: Few-shot performance vs model size
   ‚Ä¢ Exemplos: In-context learning (arithmetic, translation)

Scaling Laws:
   ‚Ä¢ Gr√°ficos: Loss vs Parameters (power law)
   ‚Ä¢ Gr√°ficos: Optimal compute allocation

The Pile:
   ‚Ä¢ Pie chart: 22 componentes do dataset
   ‚Ä¢ Tabela: Size de cada componente

InstructGPT:
   ‚Ä¢ Diagrama: Pipeline de 3 steps (SFT ‚Üí RM ‚Üí PPO)
   ‚Ä¢ Gr√°fico: Human preference (1.3B > 175B)
   ‚Ä¢ Exemplos: Aligned vs unaligned responses

LoRA:
   ‚Ä¢ Diagrama: Low-rank adaptation
   ‚Ä¢ Tabela: Par√¢metros trein√°veis (LoRA vs full FT)

DPO:
   ‚Ä¢ Compara√ß√£o: RLHF (3 steps) vs DPO (1 step)
   ‚Ä¢ Equa√ß√£o: DPO loss

Llama 2:
   ‚Ä¢ Tabela: Performance vs GPT-3.5
   ‚Ä¢ Diagrama: 5 rounds de RLHF
   ‚Ä¢ Safety metrics

Llama 3:
   ‚Ä¢ Tabela: 405B vs GPT-4 vs Claude 3.5
   ‚Ä¢ Gr√°ficos: Scaling (8B, 70B, 405B)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CONCEITOS-CHAVE (gloss√°rio)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

PR√â-TREINO:
   ‚Ä¢ Generative Pre-training: treino n√£o-supervisionado com next token prediction
   ‚Ä¢ Parametric Knowledge: conhecimento armazenado nos pesos do modelo
   ‚Ä¢ Scaling Laws: rela√ß√£o power-law entre loss e par√¢metros/dados/compute
   ‚Ä¢ Training Dataset: corpus massivo (trilh√µes de tokens)

FINE-TUNING:
   ‚Ä¢ SFT: Supervised Fine-Tuning (exemplos de input-output)
   ‚Ä¢ Instruction Tuning: SFT com formato de instru√ß√£o
   ‚Ä¢ Task-specific: adaptar modelo para tarefa espec√≠fica
   ‚Ä¢ Catastrophic Forgetting: modelo "esquece" pr√©-treino ap√≥s fine-tuning

ALIGNMENT:
   ‚Ä¢ RLHF: Reinforcement Learning from Human Feedback
   ‚Ä¢ Reward Model: modelo que prediz prefer√™ncia humana
   ‚Ä¢ PPO: Proximal Policy Optimization (algoritmo de RL)
   ‚Ä¢ DPO: Direct Preference Optimization (alternativa ao RLHF)
   ‚Ä¢ RLAIF: RL from AI Feedback (usa AI em vez de humanos)
   ‚Ä¢ Constitutional AI: self-critique baseado em princ√≠pios
   ‚Ä¢ Alignment Tax: perda de capabilities ap√≥s alignment

EFFICIENCY:
   ‚Ä¢ LoRA: Low-Rank Adaptation (adapters de baixo rank)
   ‚Ä¢ QLoRA: Quantized LoRA (4-bit quantization)
   ‚Ä¢ Parameter-Efficient: m√©todos que treinam poucos par√¢metros
   ‚Ä¢ Rank: dimens√£o das matrizes de LoRA (tipicamente 8-64)

CAPABILITIES:
   ‚Ä¢ Zero-shot: resolver tarefa sem exemplos
   ‚Ä¢ Few-shot: resolver tarefa com poucos exemplos (no prompt)
   ‚Ä¢ In-Context Learning: aprender de exemplos no contexto
   ‚Ä¢ Emergent Capabilities: capacidades que aparecem com scale

OPEN-SOURCE:
   ‚Ä¢ Weights: par√¢metros do modelo (released ou n√£o)
   ‚Ä¢ Base Model: modelo pr√©-treinado (sem fine-tuning)
   ‚Ä¢ Chat Model: modelo alinhado para conversa√ß√£o
   ‚Ä¢ Commercial License: pode usar comercialmente

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ESTAT√çSTICAS IMPORTANTES
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

MODELOS (par√¢metros e tokens):
   ‚Ä¢ GPT-1: 117M params, ~1GB texto
   ‚Ä¢ GPT-2: 1.5B params, 40GB texto (WebText)
   ‚Ä¢ GPT-3: 175B params, 300B tokens
   ‚Ä¢ LLaMA 1: 7B-65B, 1.4T tokens
   ‚Ä¢ Llama 2: 7B-70B, 2T tokens
   ‚Ä¢ Mistral 7B: 7B params
   ‚Ä¢ Llama 3: 8B-405B, 15.6T tokens
   ‚Ä¢ Phi-3: 3.8B-14B, 3.3-4.8T tokens

DATASETS:
   ‚Ä¢ The Pile: 825GB, 22 componentes
   ‚Ä¢ Common Corpus: 2T tokens
   ‚Ä¢ InstructGPT: 13k SFT examples, 33k comparisons

CUSTOS ESTIMADOS:
   ‚Ä¢ GPT-3 training: $4.6M
   ‚Ä¢ Llama 3 405B: $50-100M
   ‚Ä¢ InstructGPT labeling: $50-100k
   ‚Ä¢ RLAIF: 10-100x mais barato que RLHF

EFFICIENCY:
   ‚Ä¢ LoRA: 10,000x menos par√¢metros trein√°veis
   ‚Ä¢ QLoRA: Fine-tune 65B em 48GB GPU (vs 130GB)
   ‚Ä¢ Guanaco 65B: 24h em 1 GPU (QLoRA)

PERFORMANCE:
   ‚Ä¢ InstructGPT 1.3B > GPT-3 175B (prefer√™ncia humana)
   ‚Ä¢ LLaMA 13B > GPT-3 175B (alguns benchmarks)
   ‚Ä¢ Llama 3 405B ‚âà GPT-4 (MMLU: 87.3% vs 86.4%)


==============================================================================
                        üéØ DICAS FINAIS
==============================================================================

1. N√ÉO LEIA LINEARMENTE:
   Papers cient√≠ficos N√ÉO devem ser lidos palavra por palavra.
   ‚Üí Leia: Intro ‚Üí M√©todo ‚Üí Resultados ‚Üí Conclus√£o
   ‚Üí Skip: Related work (geralmente), Appendix (se necess√°rio)

2. FOQUE EM FIGURAS E TABELAS:
   Uma figura vale mais que 1000 palavras.
   ‚Üí Entenda os gr√°ficos primeiro
   ‚Üí Leia texto para entender as figuras

3. ANOTE ENQUANTO L√ä:
   Mantenha um documento com:
   ‚Üí Conceitos principais
   ‚Üí Estat√≠sticas importantes
   ‚Üí Figuras para slides
   ‚Üí Perguntas que surgirem

4. COMPARE PAPERS:
   Muitos papers falam das mesmas coisas.
   ‚Üí Compare abordagens
   ‚Üí Identifique consensos
   ‚Üí Note diverg√™ncias

5. PRIORIZE PAPERS SEMINAIS:
   Alguns papers s√£o mais importantes:
   ‚≠ê‚≠ê‚≠ê GPT-1, InstructGPT, Llama 2, Scaling Laws
   ‚≠ê‚≠ê DPO, LoRA, RLAIF
   ‚≠ê Surveys (para contextualiza√ß√£o)

6. USE SURVEYS COMO GUIAS:
   Surveys organizam conhecimento:
   ‚Üí Leia surveys para big picture
   ‚Üí Aprofunde em papers espec√≠ficos

7. N√ÉO TENTE ENTENDER TUDO:
   Alguns detalhes t√©cnicos s√£o muito complexos.
   ‚Üí Entenda a IDEIA geral
   ‚Üí N√£o se perca em deriva√ß√µes matem√°ticas
   ‚Üí Foque em INTUI√á√ÉO

8. CONECTE COM PR√ÅTICA:
   Pense em aplica√ß√µes:
   ‚Üí Como isso funciona no ChatGPT?
   ‚Üí Por que Llama 3 usou DPO e n√£o RLHF?
   ‚Üí Como treinar modelo com recursos limitados?

9. PREPARE EXEMPLOS:
   Exemplos concretos ajudam:
   ‚Üí "Paris is the capital of France" (parametric knowledge)
   ‚Üí "Write a poem" (instruction following)
   ‚Üí Compara√ß√£o: base model vs chat model

10. TESTE SUA COMPREENS√ÉO:
    Explique para algu√©m (ou para si mesmo):
    ‚Üí Se consegue explicar em palavras simples, entendeu
    ‚Üí Se n√£o consegue, releia


==============================================================================
                        ‚úÖ CHECKLIST FINAL
==============================================================================

Antes da aula, certifique-se:

‚ñ° Li os papers do cen√°rio escolhido
‚ñ° Anotei conceitos principais
‚ñ° Extra√≠ figuras importantes
‚ñ° Preparei exemplos pr√°ticos
‚ñ° Estruturei a aula (introdu√ß√£o ‚Üí m√≥dulos ‚Üí conclus√£o)
‚ñ° Testei explica√ß√µes (expliquei para algu√©m ou gravei)
‚ñ° Preparei respostas para perguntas comuns:
   ‚ñ° "Qual a diferen√ßa entre pr√©-treino e fine-tuning?"
   ‚ñ° "Por que RLHF √© necess√°rio?"
   ‚ñ° "DPO √© melhor que RLHF?"
   ‚ñ° "Como Llama 3 compete com GPT-4?"
   ‚ñ° "Posso treinar meu pr√≥prio LLM?"
‚ñ° Tenho slides prontos com figuras
‚ñ° Tenho timing definido (n√£o ultrapassar tempo da aula)


==============================================================================
                        üöÄ VOC√ä EST√Å PRONTO!
==============================================================================

Com este material, voc√™ tem tudo para dar uma aula EXCEPCIONAL sobre
treinamento de LLMs.

Lembre-se:
   ‚Ä¢ Seja claro e did√°tico
   ‚Ä¢ Use exemplos pr√°ticos
   ‚Ä¢ Conecte conceitos (n√£o isole)
   ‚Ä¢ Mostre evolu√ß√£o (2018 ‚Üí 2025)
   ‚Ä¢ Enfatize trade-offs (n√£o h√° "solu√ß√£o perfeita")

                              BOA AULA! üéì

==============================================================================

Compilado por: Claude Code (Anthropic)
Data: 31 de outubro de 2025
Para: George Marmelstein - Aulas 2025

D√∫vidas? Consulte INDICE_COMPLETO_PDFS.md para detalhes de cada paper.
