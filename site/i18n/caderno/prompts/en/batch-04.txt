# Translation Task

## Instructions
Translate from Brazilian Portuguese to English.
Keep technical terms in their original form when commonly used in English (e.g., LLM, API, RAG).
Maintain HTML formatting exactly as provided.
For Brazilian legal references (e.g., "CNJ", "STF"), keep the abbreviation and add a brief explanation in parentheses on first use.
For idiomatic expressions, adapt to natural English equivalents.

## Source Lesson (Portuguese)

**Lesson 1.4: M√°quinas Estoc√°sticas**

**Module:** M√≥dulo 1 ‚Äî Fundamentos

**Objective:** Perceber que os LLMs fornecem respostas diferentes a cada intera√ß√£o, pois seu processamento √© probabil√≠stico e estoc√°stico.

**Content:**

                    <div class="block">
                        <h2 class="block-title">O que √© Estoc√°stico?</h2>
                        <p>Na aula anterior, voc√™ aprendeu que LLMs funcionam como "m√°quinas de adivinha√ß√£o" ‚Äî prevendo o pr√≥ximo token com base em probabilidades. Agora vamos explorar uma consequ√™ncia fundamental disso: <strong>LLMs s√£o m√°quinas estoc√°sticas</strong>.</p>
                        <p><strong>Estoc√°stico</strong> significa "baseado em probabilidade e aleatoriedade". Diferente de uma calculadora que sempre d√° o mesmo resultado para 2+2, um LLM pode dar respostas diferentes para a mesma pergunta.</p>
                        <div class="code-block">
<span class="code-comment">// Calculadora (determin√≠stica)</span>
2 + 2 = 4  (sempre)
2 + 2 = 4  (sempre)
2 + 2 = 4  (sempre)

<span class="code-comment">// LLM (estoc√°stico)</span>
"Me diz uma fruta" ‚Üí "Ma√ß√£"
"Me diz uma fruta" ‚Üí "Banana"
"Me diz uma fruta" ‚Üí "Laranja"
                        </div>
                    </div>

                    <div class="block">
                        <h2 class="block-title">Por que as Respostas Variam?</h2>
                        <p>Lembre-se: o modelo calcula uma <strong>distribui√ß√£o de probabilidades</strong> para cada token. Para a pergunta "Me diz uma fruta", a distribui√ß√£o pode ser:</p>
                        <div class="table-wrapper">
                            <table>
                                <thead><tr><th>Token</th><th>Probabilidade</th></tr></thead>
                                <tbody>
                                    <tr><td>Ma√ß√£</td><td>35%</td></tr>
                                    <tr><td>Banana</td><td>25%</td></tr>
                                    <tr><td>Laranja</td><td>20%</td></tr>
                                    <tr><td>Morango</td><td>10%</td></tr>
                                    <tr><td>Outros</td><td>10%</td></tr>
                                </tbody>
                            </table>
                        </div>
                        <p>O modelo <strong>amostra</strong> dessa distribui√ß√£o ‚Äî como girar uma roleta tendenciosa. "Ma√ß√£" tem maior chance de sair, mas n√£o √© garantida. Cada execu√ß√£o √© um novo giro da roleta.</p>
                        <img src="assets/img/1Je65TdJbDm55KQCja7rPVw.png" alt="Representa√ß√£o visual de uma rede neural com camadas de processamento" style="max-width: 100%; margin: 20px 0; border-radius: 8px;" loading="lazy">
                    </div>

                    <div class="block">
                        <h2 class="block-title">Padr√µes Ocultos nos Dados de Treinamento</h2>
                        <p>Embora as respostas variem, elas seguem <strong>padr√µes estat√≠sticos</strong> aprendidos durante o treinamento. Alguns exemplos curiosos:</p>
                        <div class="table-wrapper">
                            <table>
                                <thead><tr><th>Pergunta</th><th>Resposta mais frequente</th><th>Por qu√™?</th></tr></thead>
                                <tbody>
                                    <tr><td>"Me diz uma fruta"</td><td>Ma√ß√£</td><td>Fruta mais citada em textos</td></tr>
                                    <tr><td>"Me diz uma ferramenta"</td><td>Martelo</td><td>Ferramenta ic√¥nica culturalmente</td></tr>
                                    <tr><td>"Pense num n√∫mero de 1 a 100"</td><td>27 ou 37</td><td>N√∫meros "aleat√≥rios" preferidos por humanos</td></tr>
                                </tbody>
                            </table>
                        </div>
                        <p>Os LLMs absorvem vieses culturais e estat√≠sticos dos dados de treinamento. Quando humanos escrevem "escolha um n√∫mero aleat√≥rio", tendem a escolher n√∫meros como 27 ou 37. O modelo aprendeu esse padr√£o.</p>
                    </div>

                    <div class="block">
                        <h2 class="block-title">Regress√£o √† Moda</h2>
                        <p>Na estat√≠stica, existe o conceito de <strong>regress√£o √† m√©dia</strong>: em medi√ß√µes repetidas, valores extremos tendem a se aproximar da m√©dia. Os LLMs exibem um fen√¥meno similar, mas diferente: a <strong>regress√£o √† moda</strong>.</p>
                        <p>A <strong>moda</strong> √© o valor que aparece com mais frequ√™ncia em um conjunto de dados. Nos LLMs, isso significa que o modelo tende a gravitar para as respostas mais comuns nos dados de treinamento.</p>
                        <div class="code-block">
<span class="code-comment">// Regress√£o √† M√©dia (estat√≠stica tradicional)</span>
Valores extremos ‚Üí tendem √† M√âDIA (valor central)

<span class="code-comment">// Regress√£o √† Moda (LLMs)</span>
Respostas poss√≠veis ‚Üí tendem √† MODA (mais frequente)
                        </div>
                        <p><strong>Por que isso acontece?</strong> O modelo foi treinado para prever o pr√≥ximo token mais prov√°vel. "Mais prov√°vel" significa "mais frequente nos dados de treinamento". Portanto, sem instru√ß√µes espec√≠ficas, o modelo naturalmente gravita para:</p>
                        <div class="table-wrapper">
                            <table>
                                <thead><tr><th>Tipo de Tarefa</th><th>A Moda (resposta mais comum)</th></tr></thead>
                                <tbody>
                                    <tr><td>Escrever uma introdu√ß√£o</td><td>F√≥rmulas gen√©ricas ("Nos dias atuais...", "√â sabido que...")</td></tr>
                                    <tr><td>Analisar um contrato</td><td>Pontos √≥bvios que qualquer an√°lise mencionaria</td></tr>
                                    <tr><td>Sugerir argumentos</td><td>Argumentos mais comuns na jurisprud√™ncia</td></tr>
                                    <tr><td>Resumir um texto</td><td>Estrutura padr√£o de resumo acad√™mico</td></tr>
                                </tbody>
                            </table>
                        </div>
                        <p><strong>Implica√ß√£o crucial:</strong> Sem instru√ß√µes espec√≠ficas, voc√™ receber√° a resposta "m√©dia" ‚Äî aquela que aparece com mais frequ√™ncia em textos similares. Para obter respostas diferenciadas, voc√™ precisa <strong>afastar o modelo da moda</strong> com instru√ß√µes precisas, exemplos espec√≠ficos, ou pedindo explicitamente por abordagens n√£o-convencionais.</p>
                    </div>

                    <div class="block">
                        <h2 class="block-title">Vi√©s de Frequ√™ncia</h2>
                        <p>A regress√£o √† moda √© uma manifesta√ß√£o de um fen√¥meno mais amplo: o <strong>vi√©s de frequ√™ncia</strong>. O modelo reproduz padr√µes na propor√ß√£o em que aparecem nos dados de treinamento.</p>
                        <div class="code-block">
<span class="code-comment">// Nos dados de treinamento:</span>
"Ma√ß√£" aparece 10.000 vezes como exemplo de fruta
"Jabuticaba" aparece 500 vezes como exemplo de fruta

<span class="code-comment">// Na resposta do modelo:</span>
"Ma√ß√£" tem ~20x mais chance de ser gerada que "Jabuticaba"
                        </div>
                        <p><strong>Consequ√™ncias pr√°ticas do vi√©s de frequ√™ncia:</strong></p>
                        <div class="table-wrapper">
                            <table>
                                <thead><tr><th>√Årea</th><th>O que √© mais frequente nos dados</th><th>O que o modelo tende a gerar</th></tr></thead>
                                <tbody>
                                    <tr><td>Jurisdi√ß√£o</td><td>Textos de direito americano (em ingl√™s)</td><td>Conceitos de common law mesmo quando perguntado sobre civil law</td></tr>
                                    <tr><td>Jurisprud√™ncia</td><td>Tribunais superiores mais citados</td><td>STF e STJ, mesmo quando TRF seria mais relevante</td></tr>
                                    <tr><td>Argumentos</td><td>Teses majorit√°rias e consolidadas</td><td>Argumentos "seguros" em vez de teses inovadoras</td></tr>
                                    <tr><td>Estilo</td><td>Linguagem jur√≠dica formal tradicional</td><td>Texto rebuscado mesmo quando clareza √© pedida</td></tr>
                                    <tr><td>Doutrina</td><td>Autores mais citados (cl√°ssicos)</td><td>Refer√™ncias a doutrinadores famosos, n√£o necessariamente os mais adequados</td></tr>
                                </tbody>
                            </table>
                        </div>
                        <p><strong>Por que isso importa no Direito?</strong> O vi√©s de frequ√™ncia pode fazer o modelo:</p>
                        <ul style="margin: 16px 0; padding-left: 24px; color: var(--text-secondary);">
                            <li>Privilegiar interpreta√ß√µes majorit√°rias sobre minorit√°rias (mesmo quando a minorit√°ria √© mais adequada ao caso)</li>
                            <li>Reproduzir vieses hist√≥ricos presentes na jurisprud√™ncia</li>
                            <li>Sugerir solu√ß√µes "padr√£o" quando o caso exige criatividade</li>
                            <li>Ignorar especificidades locais em favor de padr√µes mais universais</li>
                        </ul>
                        <p><strong>Ant√≠doto:</strong> Seja espec√≠fico. Em vez de "analise este contrato", diga "analise este contrato de loca√ß√£o comercial sob a perspectiva do locat√°rio, identificando cl√°usulas potencialmente abusivas segundo o CDC e a jurisprud√™ncia do TJSP".</p>
                    </div>

                    <div class="block">
                        <h2 class="block-title">Implica√ß√µes Pr√°ticas</h2>
                        <div class="table-wrapper">
                            <table>
                                <thead><tr><th>Situa√ß√£o</th><th>Consequ√™ncia</th><th>O que fazer</th></tr></thead>
                                <tbody>
                                    <tr><td>Gerar textos criativos</td><td>Cada gera√ß√£o √© √∫nica</td><td>Gere v√°rias vers√µes e escolha</td></tr>
                                    <tr><td>An√°lise de documentos</td><td>Pode destacar aspectos diferentes</td><td>Execute m√∫ltiplas vezes, compare</td></tr>
                                    <tr><td>Tarefas objetivas</td><td>Respostas podem variar ligeiramente</td><td>Use instru√ß√µes mais espec√≠ficas</td></tr>
                                    <tr><td>Reprodutibilidade</td><td>Dif√≠cil obter mesma resposta exata</td><td>Aceite varia√ß√£o ou use seed fixa</td></tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                

**Exercise:**
Title: Experimentando a Estocasticidade
Prompt: undefined
Checklist:
- Entendo que LLMs s√£o m√°quinas estoc√°sticas (probabil√≠sticas)
- Percebi que a mesma pergunta pode gerar respostas diferentes
- Identifiquei padr√µes estat√≠sticos nas respostas (Ma√ß√£, Martelo, 27)
- Compreendo o conceito de 'regress√£o √† moda' ‚Äî o modelo gravita para respostas comuns
- Entendo o 'vi√©s de frequ√™ncia' ‚Äî o modelo reproduz o que √© mais comum nos dados
- Sei que instru√ß√µes gen√©ricas produzem respostas gen√©ricas
- Compreendo que varia√ß√£o √© caracter√≠stica, n√£o defeito
- Sei que posso usar essa varia√ß√£o a meu favor (m√∫ltiplas gera√ß√µes)


**Tip:** A estocasticidade pode ser <strong>controlada</strong> (n√£o eliminada). Atrav√©s de <strong>prompts estruturados</strong> ‚Äî t√©cnica que voc√™ aprender√° no m√≥dulo de Engenharia de Prompt ‚Äî conseguimos dirigir o modelo para respostas mais <strong>previs√≠veis</strong>, mais <strong>padronizadas</strong> e mais <strong>profundas</strong>. Quanto mais espec√≠fico o prompt, menor a varia√ß√£o.

**Warning:** <strong>"Preciso de respostas sempre iguais, ent√£o LLMs n√£o servem."</strong><br>Errado. A varia√ß√£o √© caracter√≠stica, n√£o defeito ‚Äî mas pode ser <strong>minimizada</strong>. Instru√ß√µes detalhadas, exemplos (few-shot), formatos espec√≠ficos e restri√ß√µes expl√≠citas reduzem drasticamente a varia√ß√£o. O segredo est√° na qualidade do prompt, n√£o na natureza do modelo.

## Output Format

Return ONLY a valid JSON object with this exact structure:
{
  "number": "1.4",
  "module": "[translated module name]",
  "title": "[translated title]",
  "objective": "[translated objective]",
  "content": "[translated HTML content - keep all HTML tags]",
  "exercise": {
    "title": "[translated exercise title]",
    "prompt": "[translated prompt]",
    "checklist": ["[translated item 1]", "[translated item 2]", ...],
    "example": "[translated example if present]"
  },
  "tip": "[translated tip if present]",
  "warning": "[translated warning if present]"
}

---

# Translation Task

## Instructions
Translate from Brazilian Portuguese to English.
Keep technical terms in their original form when commonly used in English (e.g., LLM, API, RAG).
Maintain HTML formatting exactly as provided.
For Brazilian legal references (e.g., "CNJ", "STF"), keep the abbreviation and add a brief explanation in parentheses on first use.
For idiomatic expressions, adapt to natural English equivalents.

## Source Lesson (Portuguese)

**Lesson 1.5: Maldi√ß√£o da Revers√£o**

**Module:** M√≥dulo 1 ‚Äî Fundamentos

**Objective:** Perceber que os LLMs aprendem com base na frequ√™ncia das palavras na base de treinamento, tendo facilidade em uma dire√ß√£o e dificuldade na dire√ß√£o inversa.

**Content:**

                    <div class="block">
                        <h2 class="block-title">O que √© a Maldi√ß√£o da Revers√£o?</h2>
                        <p>A <strong>Maldi√ß√£o da Revers√£o</strong> (Reversal Curse) √© um fen√¥meno documentado em <a href="https://arxiv.org/abs/2309.12288" target="_blank">estudo de 2023</a> que demonstra uma limita√ß√£o fundamental dos LLMs: eles aprendem associa√ß√µes em uma <strong>dire√ß√£o espec√≠fica</strong>, n√£o bidirecionalmente como os humanos.</p>
                        <p>Se um LLM aprendeu que "A est√° relacionado a B" nos dados de treinamento, ele <strong>n√£o necessariamente</strong> consegue deduzir que "B est√° relacionado a A".</p>
                        <div class="code-block">
<span class="code-comment">// Humano (relacionamento bidirecional)</span>
"Anitta √© filha de Miriam Macedo"
   ‚Üî automaticamente sabe que
"Miriam Macedo √© m√£e de Anitta"

<span class="code-comment">// LLM (relacionamento unidirecional)</span>
"Anitta ‚Üí Miriam Macedo" ‚úì (dire√ß√£o frequente nos dados)
"Miriam Macedo ‚Üí Anitta" ‚úó (dire√ß√£o rara, pode falhar)
                        </div>
                    </div>

                    <div class="block">
                        <h2 class="block-title">Por que Isso Acontece?</h2>
                        <p>Lembre-se: LLMs s√£o m√°quinas de previs√£o de tokens treinadas em texto. Eles aprendem padr√µes <strong>na ordem em que aparecem</strong> nos dados.</p>
                        <div class="table-wrapper">
                            <table>
                                <thead><tr><th>Nos dados de treinamento</th><th>O que o modelo aprende</th></tr></thead>
                                <tbody>
                                    <tr><td>"Anitta, nome art√≠stico de Larissa de Macedo Machado, √© filha de Miriam Macedo..."</td><td>Anitta ‚Üí Miriam Macedo (forte)</td></tr>
                                    <tr><td>Raras men√ß√µes de "Miriam Macedo" como sujeito principal</td><td>Miriam Macedo ‚Üí ? (fraco)</td></tr>
                                </tbody>
                            </table>
                        </div>
                        <p>A dire√ß√£o "pessoa famosa ‚Üí familiar" aparece milhares de vezes. A dire√ß√£o inversa "familiar ‚Üí pessoa famosa" quase nunca aparece como padr√£o nos textos.</p>
                    </div>
                

**Exercise:**
Title: Teste da Revers√£o
Prompt: undefined
Checklist:
- Entendo o conceito de 'Maldi√ß√£o da Revers√£o'
- Percebi que LLMs aprendem associa√ß√µes em uma dire√ß√£o
- Testei a assimetria com o exemplo Anitta/Miriam Macedo
- Compreendo por que a dire√ß√£o da pergunta importa
- Entendo que LLMs n√£o 'entendem' relacionamentos como humanos


**Tip:** Os LLMs <strong>n√£o "entendem"</strong> relacionamentos bidirecionais como os humanos. Eles dependem da <strong>frequ√™ncia estat√≠stica</strong> das associa√ß√µes nos dados de treinamento.

**Warning:** <strong>"Se o modelo sabe que A‚ÜíB, ele sabe que B‚ÜíA."</strong><br>Errado. Esta √© a maldi√ß√£o da revers√£o. LLMs n√£o fazem infer√™ncia l√≥gica autom√°tica ‚Äî eles reproduzem padr√µes estat√≠sticos. Se a revers√£o n√£o est√° nos dados de treinamento, o modelo n√£o a 'deduz' sozinho.

## Output Format

Return ONLY a valid JSON object with this exact structure:
{
  "number": "1.5",
  "module": "[translated module name]",
  "title": "[translated title]",
  "objective": "[translated objective]",
  "content": "[translated HTML content - keep all HTML tags]",
  "exercise": {
    "title": "[translated exercise title]",
    "prompt": "[translated prompt]",
    "checklist": ["[translated item 1]", "[translated item 2]", ...],
    "example": "[translated example if present]"
  },
  "tip": "[translated tip if present]",
  "warning": "[translated warning if present]"
}

---

# Translation Task

## Instructions
Translate from Brazilian Portuguese to English.
Keep technical terms in their original form when commonly used in English (e.g., LLM, API, RAG).
Maintain HTML formatting exactly as provided.
For Brazilian legal references (e.g., "CNJ", "STF"), keep the abbreviation and add a brief explanation in parentheses on first use.
For idiomatic expressions, adapt to natural English equivalents.

## Source Lesson (Portuguese)

**Lesson 1.6: A Janela de Contexto**

**Module:** M√≥dulo 1 ‚Äî Fundamentos

**Objective:** Dominar o conceito de janela de contexto ‚Äî seu superpoder e sua kriptonita no uso de LLMs.

**Content:**

                    <div class="block">
                        <h2 class="block-title">O que √© Janela de Contexto?</h2>
                        <p>A <strong>Janela de Contexto</strong> √© a quantidade de texto que a m√°quina consegue "prestar aten√ß√£o" ao mesmo tempo. O modelo s√≥ enxerga uma janela limitada ‚Äî e esquece o que fica muito para tr√°s.</p>
                        <div class="table-wrapper">
                            <table>
                                <thead><tr><th>Modelo</th><th>Janela Padr√£o</th><th>Janela Estendida (beta)</th></tr></thead>
                                <tbody>
                                    <tr><td>GPT-4o (2024)</td><td>128K tokens</td><td>‚Äî</td></tr>
                                    <tr><td>Claude 3.5 Sonnet</td><td>200K tokens</td><td>‚Äî</td></tr>
                                    <tr><td><strong>Claude Opus 4</strong></td><td>200K tokens</td><td>‚Äî</td></tr>
                                    <tr><td>Gemini 2.0</td><td>1M tokens</td><td>2M tokens</td></tr>
                                </tbody>
                            </table>
                        </div>
                        <p>Na pr√°tica, 200K tokens equivalem a aproximadamente <strong>500 p√°ginas</strong> de texto ou um livro inteiro. Mas aten√ß√£o: isso n√£o significa que voc√™ pode anexar 500 p√°ginas. Dentro da janela de contexto entram tamb√©m o prompt de sistema e as respostas do modelo. Por isso, em um modelo de 200K, a <strong>capacidade real de processamento</strong> fica em torno de <strong>150-200 p√°ginas</strong> ‚Äî e mesmo assim, com degrada√ß√£o de qualidade conforme veremos no fen√¥meno do <em>Context Rot</em> mais adiante.</p>
                    </div>

                    <div class="block">
                        <h2 class="block-title">Duas Met√°foras para Entender</h2>
                        <div class="table-wrapper">
                            <table>
                                <thead><tr><th>Met√°fora</th><th>O que significa</th></tr></thead>
                                <tbody>
                                    <tr><td><strong>üö∂ Trilha na Areia</strong></td><td>Voc√™ caminha deixando pegadas. Conforme avan√ßa, as pegadas antigas v√£o sendo apagadas pelo vento. Novas paisagens surgem, mas o que est√° mais pr√≥ximo √© visto com maior foco ‚Äî o distante j√° se perdeu.</td></tr>
                                    <tr><td><strong>üóÇÔ∏è Mesa de Trabalho</strong></td><td>Voc√™ vai empilhando documentos na mesa. Quando a pilha cresce demais, o que estava embaixo vai sendo soterrado ‚Äî ainda existe, mas fica cada vez mais dif√≠cil de acessar at√© desaparecer.</td></tr>
                                </tbody>
                            </table>
                        </div>
                        <p><strong>Trate a janela de contexto como uma mesa de trabalho.</strong> O que voc√™ coloca primeiro pode ser soterrado pelo que vem depois.</p>
                    </div>

                    <div class="callout callout-warning">
                        <div class="callout-title">‚ö° Superpoder e Kriptonita</div>
                        <p>A janela de contexto ser√° seu <strong>superpoder</strong> e sua <strong>kriptonita</strong>:</p>
                        <ul style="margin: 12px 0; padding-left: 24px;">
                            <li><strong>Pouco contexto</strong> = resposta fraca, gen√©rica (o modelo n√£o tem informa√ß√£o suficiente)</li>
                            <li><strong>Excesso de contexto</strong> = distra√ß√£o, degrada√ß√£o (o modelo se perde no volume)</li>
                        </ul>
                        <p>O segredo √© o <strong>equil√≠brio</strong>: contexto suficiente para a tarefa, n√£o mais.</p>
                    </div>

                    <div class="block">
                        <h2 class="block-title">O Fen√¥meno "Context Rot"</h2>
                        <p>Um <a href="https://research.trychroma.com/context-rot" target="_blank">estudo de 2025 da Chroma Research</a> documentou a <strong>degrada√ß√£o de desempenho</strong> conforme o contexto aumenta ‚Äî o chamado <strong>Context Rot</strong> (apodrecimento do contexto).</p>
                        <p>Contr√°rio √† suposi√ß√£o comum, os LLMs <strong>n√£o processam uniformemente</strong> todos os tokens em contextos longos. A qualidade cai dramaticamente.</p>
                        <img src="assets/img/context-rot-hero.png" alt="Gr√°fico mostrando degrada√ß√£o de desempenho de LLMs conforme contexto aumenta" style="max-width: 100%; margin: 20px 0; border-radius: 8px; border: 1px solid var(--border-light);" loading="lazy">
                        <p><strong>Como ler o gr√°fico:</strong> O eixo X mostra o tamanho do contexto (de 25 a 10.000 palavras). O eixo Y mostra a precis√£o. Observe como <strong>todos os modelos</strong> (Claude, GPT-4, Gemini, Qwen) come√ßam com alta precis√£o em contextos curtos, mas <strong>degradam significativamente</strong> conforme o contexto aumenta.</p>
                    </div>

                    <div class="block">
                        <h2 class="block-title">Mais Contexto ‚â† Melhor Resultado</h2>
                        <p>A promessa de janelas de 1 milh√£o de tokens pode ser uma <strong>armadilha</strong>:</p>
                        <div class="table-wrapper">
                            <table>
                                <thead><tr><th>Efeito</th><th>O que acontece</th></tr></thead>
                                <tbody>
                                    <tr><td><strong>Degrada√ß√£o de qualidade</strong></td><td>Quanto mais texto, menor a precis√£o em encontrar informa√ß√µes espec√≠ficas</td></tr>
                                    <tr><td><strong>Aumento de lat√™ncia</strong></td><td>Mais contexto = mais tempo de processamento (segundos ou minutos)</td></tr>
                                    <tr><td><strong>Custo maior</strong></td><td>Voc√™ paga por token processado ‚Äî contexto grande = conta maior</td></tr>
                                    <tr><td><strong>Dilui√ß√£o de instru√ß√µes</strong></td><td>Suas instru√ß√µes podem se "perder" no meio de documentos longos</td></tr>
                                </tbody>
                            </table>
                        </div>
                        <p><strong>Regra pr√°tica:</strong> N√£o jogue tudo na janela s√≥ porque cabe. Selecione o que √© relevante.</p>
                    </div>

                    <div class="block">
                        <h2 class="block-title">Contextos Isolados</h2>
                        <p>Cada conversa √© uma janela de contexto <strong>independente</strong>:</p>
                        <div class="code-block">
Conversa 1: [Voc√™: "Meu nome √© Jo√£o"] [Claude: "Ol√° Jo√£o!"]
Conversa 2: [Voc√™: "Qual meu nome?"] [Claude: "N√£o sei seu nome."]

‚Üë Janelas completamente isoladas ‚Äî o modelo N√ÉO transfere informa√ß√£o.</div>
                        <p>Quando voc√™ abre uma nova conversa, o modelo come√ßa do <strong>zero</strong> ‚Äî sem mem√≥ria das conversas anteriores.</p>
                    </div>

                    <div class="block">
                        <h2 class="block-title">Simule a Janela de Contexto</h2>
                        <p>Use o simulador abaixo para entender na pr√°tica como a janela de contexto funciona. Adicione elementos e veja o que acontece quando ela enche:</p>
                        <div class="iframe-container">
                            <iframe src="../docs/caderno/demo-contexto.html" style="width:100%;height:600px;border:1px solid var(--border-light);border-radius:12px;margin:20px 0;box-shadow:var(--shadow-md);" loading="lazy" title="Simulador de Janela de Contexto" onerror="this.classList.add('error')"></iframe>
                            <div class="iframe-fallback">
                                <p>Conteudo interativo indisponivel.</p>
                                <p><a href="../docs/caderno/demo-contexto.html" target="_blank">Abrir Simulador de Contexto em nova aba</a></p>
                            </div>
                        </div>
                        <p><strong>Experimente:</strong> Compare os modos <em>Stuffing</em> vs <em>RAG</em> e veja a diferen√ßa entre <em>Colapso</em> e <em>Deslizante</em>.</p>
                    </div>
                

**Exercise:**
Title: Explorando a Janela de Contexto
Prompt: undefined
Checklist:
- Entendo que a janela de contexto √© como uma mesa de trabalho limitada
- Sei os tamanhos aproximados (200K padr√£o, 1M estendido)
- Compreendo o fen√¥meno 'Context Rot' (degrada√ß√£o com contexto longo)
- Entendo que mais contexto n√£o significa melhor resultado
- Sei que cada conversa √© uma janela isolada


**Tip:** Trate a janela de contexto como <strong>mesa de trabalho</strong>: coloque apenas o que √© relevante para a tarefa atual. Posicione informa√ß√µes importantes no <strong>in√≠cio</strong> ou no <strong>final</strong>, nunca no meio.

**Warning:** <strong>"Vou jogar todo o processo na janela porque cabe."</strong><br>P√©ssima ideia. Quanto maior o contexto, maior a lat√™ncia, menor a precis√£o, e maior o custo. Selecione os documentos relevantes em vez de jogar tudo.

## Output Format

Return ONLY a valid JSON object with this exact structure:
{
  "number": "1.6",
  "module": "[translated module name]",
  "title": "[translated title]",
  "objective": "[translated objective]",
  "content": "[translated HTML content - keep all HTML tags]",
  "exercise": {
    "title": "[translated exercise title]",
    "prompt": "[translated prompt]",
    "checklist": ["[translated item 1]", "[translated item 2]", ...],
    "example": "[translated example if present]"
  },
  "tip": "[translated tip if present]",
  "warning": "[translated warning if present]"
}